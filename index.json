[{"content":"Demystifying Long Chain-of-Thought Reasoning in LLMs This study systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories and providing practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs\nhttps://arxiv.org/pdf/2502.03373.pdf\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning This work introduces first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL and achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.\nhttps://arxiv.org/pdf/2501.12948.pdf\nPhysics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model\u0026rsquo;s hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions? Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.\nhttps://arxiv.org/pdf/2407.20311.pdf\nThe Mystery of the Pathological Path-star Task for Language Models The recently introduced path-star task is a minimal task designed to exemplify limitations to the abilities of language models, and a regularization method is introduced using structured samples of the same graph but with differing target nodes, improving results across a variety of model types.\nhttps://www.semanticscholar.org/reader/b3c5da33f73b8d4b77c107134e05957b20d544ba\n* What Algorithms can Transformers Learn? A Study in Length Generalization This work proposes a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task and provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.\nhttps://www.semanticscholar.org/reader/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b\n","permalink":"https://sino-huang.github.io/posts/awesome_llm_reasoning_capability_papers/","summary":"\u003ch3 id=\"demystifying-long-chain-of-thought-reasoning-in-llms\"\u003eDemystifying Long Chain-of-Thought Reasoning in LLMs\u003c/h3\u003e\n\u003cp\u003eThis study systematically investigate the mechanics of long CoT reasoning,  identifying the key factors that enable models to generate long CoT  trajectories and providing practical guidance for optimizing training  strategies to enhance long CoT reasoning in LLMs\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2502.03373.pdf\"\u003ehttps://arxiv.org/pdf/2502.03373.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning\"\u003eDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\u003c/h3\u003e\n\u003cp\u003eThis work introduces first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, which incorporates multi-stage training and cold-start  data before RL and achieves performance comparable to OpenAI-o1-1217 on  reasoning tasks.\u003c/p\u003e","title":"Awesome LLMs Reasoning Abilities Papers"},{"content":"","permalink":"https://sino-huang.github.io/posts/","summary":"","title":"Posts"},{"content":"[TOC]\nTitle: Pallagani Plansformer Generating Plans 2023 Author: Pallagani, Vishal et. al. Publish Year: GenPlan 2023 Workshop Review Date: Tue, Dec 24, 2024 url: https://arxiw.org/pdf/2212.08681 1 2 3 4 5 6 7 8 9 10 11 12 13 # input bibtex here @InProceedings{pallagani2023plansformer, author = {Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Horesh, Lior and Srivastava, Biplav and Fabiano, Francesco and Loreggia, Andrea}, title = {Plansformer: Generating Symbolic Plans using Transformers}, booktitle = {Seventh Workshop on Generalization in Planning (GenPlan 2023)}, year = {2023}, month = {December}, address = {New Orleans, USA}, venue = {Room 238-239, New Orleans Ernest N. Morial Convention Center} } Pallagani, Vishal, et al. \u0026#34;Plansformer: Generating Symbolic Plans using Transformers.\u0026#34; NeurIPS 2023 Workshop on Generalization in Planning. [!Note]\nplease also check MVSplat360 openreview https://openreview.net/forum?id=B0OWOkMwhz\u0026referrer=%5Bthe%20profile%20of%20Bohan%20Zhuang%5D(%2Fprofile%3Fid%3D~Bohan_Zhuang1) in terms of how to do rebuttal.\nAlso check plangpt openreview https://openreview.net/forum?id=yB8oafJ8bu\nSummary of paper The paper introduces Plansformer, a large language model (LLM) fine-tuned on planning problems to generate symbolic plans. The authors leverage the capabilities of the CodeT5 model, which is pre-trained on code generation tasks, and further fine-tune it on a dataset of planning problems and their corresponding valid plans. The goal is to harness the syntactic and structural knowledge learned by LLMs for natural language tasks and apply it to the domain of automated planning.\nThe experimental results indicate that the syntactic/symbolic knowledge learned from different programming languages in the CodeT5 model can be beneficial for the PDDL-based automated planning task. For example, in one configuration of Plansformer tested on the Towers of Hanoi domain, the model was able to generate 97% valid plans, out of which 95% are shortest length plans. The results reveal a promising direction to harness LLMs for symbolic tasks such as planning.\nThe paper evaluates Plansformer using both model testing and planner testing. The model testing uses metrics like ROUGE and BLEU to understand the performance of Plansformer as a language model, while the planner testing uses a plan validation tool to check the validity and optimality of the generated plans. The results show that Plansformer can achieve high plan validity and optimality, reducing the need for extensive knowledge engineering efforts required by traditional planning systems.\nOverall, the paper presents a positive attitude towards the use of LLMs for automated planning, demonstrating the potential of Plansformer to generate valid and optimal plans with reduced knowledge engineering efforts compared to traditional planning systems.\nMotivation The motivation of this work is to explore the use of Large Language Models (LLMs) for automated planning\nContribution The evaluation of Plansformer\u0026rsquo;s competence in generating valid and optimal plans, achieving 97% valid plans with 95% optimality on the Towers of Hanoi domain. Some key terms The dataset may be biased, as there is no evidence that LLMs can learn optimal planning by simply maintaining one plan path without knowing how to do heuristic search and pruning. 6 It is possible that the LLM has already seen similar patterns in the training data and is simply doing pattern matching, rather than reasoning from first principles. Potential future work Researchers, particularly from the ICAPS community, have discovered that Large Language Models (LLMs) cannot effectively solve automated planning problems through simple next-token prediction of planning instances. This revelation has led to a critical reassessment of earlier optimistic claims about LLMs\u0026rsquo; planning capabilities.\nLooking back at the 2022 paper Plansformer, it appears that its claims were overly optimistic. This optimism likely stemmed from flaws in the design of the training and testing datasets.\nFurther investigation is crucial to unravel the enigma surrounding LLMs\u0026rsquo; planning abilities. Some researchers have become overly pessimistic, while others remain too optimistic. Additionally, certain strategies claimed to improve sequential reasoning in LLMs have primarily been tested in mathematical domains, leaving their effectiveness in automated planning contexts uncertain. It is essential to explore whether these strategies can be successfully applied to the automated planning domain, and if they do not directly improve plan validity, we need extra metrics to capture incremental improvements in planning capabilities. By not considering more granular metrics and failing to investigate where strategies fail, researchers miss opportunities to understand how different approaches contribute and which aspects of reasoning need the most attention.\n","permalink":"https://sino-huang.github.io/posts/pallagani-plansformer-generating-plans-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Pallagani Plansformer Generating Plans 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Pallagani, Vishal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: GenPlan 2023 Workshop\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 24, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiw.org/pdf/2212.08681\"\u003ehttps://arxiw.org/pdf/2212.08681\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e 1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 4\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 5\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 6\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 7\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 8\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e 9\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e10\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e11\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e12\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e13\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bibtex\" data-lang=\"bibtex\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003e# input bibtex here\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nc\"\u003e@InProceedings\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"nl\"\u003epallagani2023plansformer\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003eauthor\u003c/span\u003e    \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Horesh, Lior and Srivastava, Biplav and Fabiano, Francesco and Loreggia, Andrea}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003etitle\u003c/span\u003e     \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{Plansformer: Generating Symbolic Plans using Transformers}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003ebooktitle\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{Seventh Workshop on Generalization in Planning (GenPlan 2023)}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003eyear\u003c/span\u003e      \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{2023}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003emonth\u003c/span\u003e     \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{December}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003eaddress\u003c/span\u003e   \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{New Orleans, USA}\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  \u003cspan class=\"na\"\u003evenue\u003c/span\u003e     \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e{Room 238-239, New Orleans Ernest N. Morial Convention Center}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c\"\u003ePallagani, Vishal, et al. \u0026#34;Plansformer: Generating Symbolic Plans using Transformers.\u0026#34; NeurIPS 2023 Workshop on Generalization in Planning.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cblockquote\u003e\n\u003cp\u003e[!Note]\u003c/p\u003e","title":"Pallagani Plansformer Generating Plans 2023"},{"content":"[TOC]\nTitle: DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture of Experts Language Models Author: Damai Dai et. al. Publish Year: 11 Jan 2024 Review Date: Sat, Jun 22, 2024 url: https://arxiv.org/pdf/2401.06066 Summary of paper Motivation conventional MoE architecture like GShard, which avtivate top-k out of N experts, face challenges in ensuring expert specialization, i.e., each expert acquires non-overlapping and focused knowledge, in response, we propose DeepSeekMoE architecture towards ultimate expert specialization Contribution segmenting expert into mN ones and activating mK from them isolating K_s, experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts Some key terms MoE architecture\nref: [1, 2, 3, 4]\n[1] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Computing, 3(1):79–87, 1991. URL https://doi.org/10.1162/neco.1991.3.1. 79.\n[2] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computing, 6(2):181–214, 1994. URL https://doi.org/10.1162/neco.1994.6.2.181.\n[3] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id=B1ckMDqlg.\n[4] Dai, Damai, et al. \u0026ldquo;Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.\u0026rdquo; arXiv preprint arXiv:2401.06066 (2024).\nIssue of existing MoE\nknowledge hybridity limited number of experts (8), thus token will be likely to cover diverse knowledge. the expert will intend to assume vastly different types of knowledge in its parameters, which are hard to utilize simultaneously and also the expert does not focus on a single specialization but instead tries to handle a wide variety of knowledge areas knowledge redundancy tokens assigned to different experts may require common knowledge. As a result, multiple experts may converge in acquiring shared knowledge in their respective parameters. Two innovative methods\n1. splitting the FFN intermediate hidden dimension\n2. shared expert isolation\nwe isolate certain experts to serve as shared experts that are always activated, aiming at capturing and consolidating common knowledge across varying contexts.\nBackground\ng is the gate, TopK denotes the set comprising K highest affinity score among those calculated for the t-th token and $e^l_i$​ is the centroid of the i-th expert in the l-th layer\nWhat is $e^l_i$ In the paper it said $e^l_i$​ is the centroid of the i-th expert in the l-th layer,\nBut when we see the code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class MoEGate(nn.Module): def __init__(self, config): super().__init__() self.config = config self.top_k = config.num_experts_per_tok self.n_routed_experts = config.n_routed_experts self.scoring_func = config.scoring_func self.alpha = config.aux_loss_alpha self.seq_aux = config.seq_aux # topk selection algorithm self.norm_topk_prob = config.norm_topk_prob self.gating_dim = config.hidden_size self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim))) self.reset_parameters() def reset_parameters(self) -\u0026gt; None: import torch.nn.init as init init.kaiming_uniform_(self.weight, a=math.sqrt(5)) def forward(self, hidden_states): bsz, seq_len, h = hidden_states.shape ### compute gating score hidden_states = hidden_states.view(-1, h) logits = F.linear(hidden_states, self.weight, None) if self.scoring_func == \u0026#39;softmax\u0026#39;: scores = logits.softmax(dim=-1) else: raise NotImplementedError(f\u0026#39;insupportable scoring function for MoE gating: {self.scoring_func}\u0026#39;) ### select top-k experts topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False) The score is calculated as\n1 2 3 4 hidden_states = hidden_states.view(-1, h) logits = F.linear(hidden_states, self.weight, None) if self.scoring_func == \u0026#39;softmax\u0026#39;: scores = logits.softmax(dim=-1) 1 2 self.gating_dim = config.hidden_size self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim))) therefore, $e^l_i$ is actually a weight constructed and stored inside the Gate object, it is trainable also.\n","permalink":"https://sino-huang.github.io/posts/damai-dai-deepseekmoe-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture of Experts Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Damai Dai et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 11 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Jun 22, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2401.06066\"\u003ehttps://arxiv.org/pdf/2401.06066\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240622111425099\" loading=\"lazy\" src=\"/posts/damai-dai-deepseekmoe-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003econventional MoE architecture like GShard, which avtivate top-k out of N experts, face challenges in ensuring expert specialization, i.e., each expert acquires non-overlapping and focused knowledge,\u003c/li\u003e\n\u003cli\u003ein response, we propose DeepSeekMoE architecture towards ultimate expert specialization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003esegmenting expert into mN ones and activating mK from them\u003c/li\u003e\n\u003cli\u003eisolating K_s, experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMoE architecture\u003c/strong\u003e\u003c/p\u003e","title":"Damai Dai Deepseekmoe 2024"},{"content":"[TOC]\nTitle: Learning to Model the World With Language 2024 Author: Jessy Lin et. al. Publish Year: ICML 2024 Review Date: Fri, Jun 21, 2024 url: https://arxiv.org/abs/2308.01399 Summary of paper Motivation in this work, we propose that agents can ground diverse kinds of language by using it to predict the future in contrast to directly predicting what to do with a language-conditioned policy, Dynalang decouples learning to model the world with language (supervised learning with prediction objectives) from learning to act given that model (RL with task rewards) Future prediction provides a rich grounding signal for learning what language utterances mean, which in turn equip the agent with a richer understanding of the world to solve complex tasks. Contribution investigate whether learning language-conditioned world models enable agents to scale to more diverse language use, compared to language-conditioned policies. Some key terms related work\nthe author mentioned the term conditioning policies on language\nMuch work has focused on teaching reinforcement learning agents to utilize language to solve tasks by directly conditioning policies on language (Lynch \u0026amp; Sermanet, 2021; Shridhar et al., 2022; Abramson et al., 2020). Recent work proposes text-conditioning a video model trained on expert demonstrations and using the model for planning. However, language in these settings has thus far been limited to short instructions, and only a few works investigate how RL agents can learn from other kinds of language like descriptions of how the world works, in simple settings (Zhong et al., 2020; Hanjie et al., 2021). Basically, the author mentioned the types of language text and how RL agents utilize them\nAfter that, they mentioned supervised approaches rely on expensive human data (often with aligned language annotations and expert demonstrations), and both these approaches have limited ability to improve their behaviors and language understanding online.\nIn the end, the author talked about how this work is different from the related work.\nWorld model learning The world model learns representations of all sensory modalities that the agent receives and then predicts the sequence of these latent representation given actions.\nPredicting future representations not only provides a rich learning signal to ground language in visual experience but also allows planning and policy optimization from imagined sequence. Experiments\nThis hypothesis statement helps the reader to know what\u0026rsquo;s going on, very helpful\n","permalink":"https://sino-huang.github.io/posts/jessy-lin-learning-to-model-the-world-with-language-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Learning to Model the World With Language 2024\u003c/li\u003e\n\u003cli\u003eAuthor: Jessy Lin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: ICML 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Jun 21, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2308.01399\"\u003ehttps://arxiv.org/abs/2308.01399\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240621173424831\" loading=\"lazy\" src=\"/posts/jessy-lin-learning-to-model-the-world-with-language-2024/image-assets/image-20240621173424831.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this work, we propose that agents can ground diverse kinds of language by using it to predict the future\u003c/li\u003e\n\u003cli\u003ein contrast to directly predicting what to do with a language-conditioned policy, Dynalang decouples learning to model the world with language (supervised learning with prediction objectives) from learning to act given that model (RL with task rewards)\u003c/li\u003e\n\u003cli\u003eFuture prediction provides a rich grounding signal for learning what language utterances mean, which in turn equip the agent with a richer understanding of the world to solve complex tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003einvestigate whether learning language-conditioned world models enable agents to scale to more diverse language use, compared to language-conditioned policies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003erelated work\u003c/strong\u003e\u003c/p\u003e","title":"Jessy Lin Learning to Model the World With Language 2024"},{"content":"[TOC]\nReview Date: Thu, Jun 20, 2024 Verification in LLM Topic 2024 Paper 1: Weng, Yixuan, et al. \u0026ldquo;Large language models are better reasoners with self-verification.\u0026rdquo; arXiv preprint arXiv:2212.09561 (2022).\nthe better reasoning with CoT is carried out in the following two steps, Forward Reasoning ad Backward Verification. Specifically, in Forward Reasoning, LLM reasoners generate candidate answers using CoT, and the question and candidate answers form different conclusions to be verified. And in Backward Verification, We mask the original condition and predict its result using another CoT. We rank candidate conclusions based on a verification score, which is calculated by assessing the consistency between the predicted and original condition values ","permalink":"https://sino-huang.github.io/posts/verification-in-llm-topic-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eReview Date: Thu, Jun 20, 2024\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"verification-in-llm-topic-2024\"\u003eVerification in LLM Topic 2024\u003c/h2\u003e\n\u003ch2 id=\"paper-1\"\u003ePaper 1:\u003c/h2\u003e\n\u003cp\u003eWeng, Yixuan, et al. \u0026ldquo;Large language models are better reasoners with self-verification.\u0026rdquo; \u003cem\u003earXiv preprint arXiv:2212.09561\u003c/em\u003e (2022).\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240620202053164\" loading=\"lazy\" src=\"/posts/verification-in-llm-topic-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe better reasoning with CoT is carried out in the following two steps, Forward Reasoning ad Backward Verification.\u003c/li\u003e\n\u003cli\u003eSpecifically, in Forward Reasoning, LLM reasoners generate candidate answers using CoT, and the question and candidate answers form different conclusions to be verified. And in Backward Verification, We mask the original condition and predict its result using another CoT. We rank candidate conclusions based on a verification score, which is calculated by assessing the consistency between\nthe predicted and original condition values\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240620202519073\" loading=\"lazy\" src=\"/posts/verification-in-llm-topic-2024/image-assets/image-20240620202519073.png\"\u003e\u003c/p\u003e","title":"Verification in Llm Topic 2024"},{"content":"[TOC]\nTitle: Reward Engineering for Generating Semi-Structured Explanation Author: Jiuzhou Han et. al. Publish Year: EACL2024 Review Date: Thu, Jun 20, 2024 url: https://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG Summary of paper Motivation Contribution the objective is to equip moderately-sized LMs with the ability to not only provide answers but also generate structured explanations Some key terms Intro\nthe author talked about some background on Cui et al. incorporate a generative pre-training mechanism over synthetic graphs by aligning inputs pairs of text-graph to improve the model\u0026rsquo;s capability in generating semi-structured explanation.\nthe author first stated that they utilise SFT as teh de-facto solution. We then turn our focus to RLHF as a mechanism to further align the explanations with ground-truth on top of SFT. Related work\nthe author mention in details about the benchmark they use \u0026ndash; ExplaGraph and COPA-SSE\nResults Experiments\nthe author first talked about the dataset size and said that \u0026ldquo;The detailed descriptions of all evaluation metrics are provided in Appendix A.\u0026rdquo;\nHuman evaluation\n","permalink":"https://sino-huang.github.io/posts/jiuzhou-reward-engineering-for-generating-semi-structured-explan-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Engineering for Generating Semi-Structured Explanation\u003c/li\u003e\n\u003cli\u003eAuthor: Jiuzhou Han et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: EACL2024\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jun 20, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG\"\u003ehttps://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe objective is to equip moderately-sized LMs with the ability to not only provide answers but also generate structured explanations\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eIntro\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ethe author talked about some background on Cui et al. incorporate a generative pre-training mechanism over synthetic graphs by aligning inputs pairs of text-graph to improve the model\u0026rsquo;s capability in generating semi-structured explanation.\u003c/p\u003e","title":"Jiuzhou Reward Engineering for Generating Semi Structured Explan 2023"},{"content":"[TOC]\nTitle: Towards Uncertainty Aware Language Agent Author: Jiuzhou Han et. al. Publish Year: 30 May 2024 Review Date: Thu, Jun 20, 2024 url: arXiv:2401.14016v3 Summary of paper Motivation The existing approaches neglect the notion of uncertainty during these interactions Contribution Some key terms Related work 1: lang agent\nthe author define what is language agent and discuss it \u0026ndash; the prominent work of ReAct propose a general language agent framework to combine reasoning and acting with LLMs for solving diverse language reasoning tasks. continue the track of ReAct \u0026ndash; introducing Reflexion, use the history failure trials as input to ask for reflection and can gain better results FireAct \u0026ndash; add more diverse fine-tuning data to improve the performance Later the author mention Toolformer, Gorilla and other lang agent that is not start from ReAct\nRelated work 2: uncertainty in generation with LLMs\nauthor mentioned some common practice in leveraging uncertainty during lang generation \u0026ndash; via sampling or decoding which do not measure uncertainty directly, but rather they rely on the stochasticity over prediction space along with a form of aggregation approach such as majority voting. then the author talked about Consistency \u0026ndash; a sampling method which takes majority voting over multiple sampling outputs. then they refer readers to a paper For a comprehensive review of sampling and decoding methods in NLG. the author stated that \u0026ldquo;We focus on leveraging uncertainty estimation in free-form QA with short answers. Uncertainty estimation in free-form NLG remains a challenge for LLMs due to the diversity of the outputs.\u0026rdquo; then the author give the category of how to do uncertainty estimation in NLG: logits or entropy based methods prompt based methods all the sections are provided with detailed info about previous work on these methods of uncertainty estimation Results ","permalink":"https://sino-huang.github.io/posts/jiuzhou-towards-uncertaintty-aware-lang-agent-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Towards Uncertainty Aware Language Agent\u003c/li\u003e\n\u003cli\u003eAuthor: Jiuzhou Han et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 30 May 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jun 20, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.14016v3\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240620111719451\" loading=\"lazy\" src=\"/posts/jiuzhou-towards-uncertaintty-aware-lang-agent-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe existing approaches neglect the notion of uncertainty during these interactions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRelated work 1: lang agent\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe author define what is language agent and discuss it \u0026ndash; the prominent work of ReAct propose a general language agent framework to combine reasoning and acting with LLMs for solving diverse language reasoning tasks.\u003c/li\u003e\n\u003cli\u003econtinue the track of ReAct \u0026ndash; introducing Reflexion, use the history failure trials as input to ask for reflection and can gain better results\u003c/li\u003e\n\u003cli\u003eFireAct \u0026ndash; add more diverse fine-tuning data to improve the performance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLater the author mention Toolformer, Gorilla and other lang agent that is not start from ReAct\u003c/p\u003e","title":"Jiuzhou Towards Uncertainty Aware Lang Agent 2024"},{"content":"[TOC]\nTitle: Failure Modes of Learning Reward Models for LLMs and other Sequence Models Author: Silviu Pitis Publish Year: ICML workshop 2023 Review Date: Fri, May 10, 2024 url: https://openreview.net/forum?id=NjOoxFRZA4\u0026noteId=niZsZfTPPt Summary of paper C3. Preference cannot represented as numbers M1. rationality level of human preference 3.2, if the condition/context changes, the preference may change rapidly, and this cannot reflect on the reward machine A2. Preference should be expressed with respect to state-policy pairs, rather than just outcomes A state-policy pair includes both the current state of the system and the strategy (policy) being employed. This approach avoids the complication of unresolved stochasticity (randomness that hasn\u0026rsquo;t yet been resolved), focusing instead on scenarios where the outcomes of policies are already known. Example with Texas Hold’em: The author uses an example from poker to illustrate these concepts. In the example, a player holding a weaker hand (72o) wins against a stronger hand (AA) after both commit to large bets pre-flop. Traditional reward modeling would prefer the successful trajectory of the weaker hand due to the positive outcome. However, a rational analysis (ignoring stochastic outcomes) would prefer the decision-making associated with the stronger hand (AA), even though it lost, as it\u0026rsquo;s typically the better strategy.\nPreference Ambiguity in LLM Tool Usage: When applying these concepts to large language models (LLMs), similar ambiguities arise. Should a model prefer trajectories where a risky action led to a correct outcome, or should it also consider what might have gone wrong (counterfactuals)?\n3.3. Reward misgeneralization we may face cases where different reward function can explain the agent behaviour in IRL, and we cannot tell which reward function is the true one the limitation of training data will cause the reward model to perform poorly when exposed to examples that are out-of-distribution (OOD) again, it is a issue on how to use exact number to represent the strength of preference ","permalink":"https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Failure Modes of Learning Reward Models for LLMs and other Sequence Models\u003c/li\u003e\n\u003cli\u003eAuthor: Silviu Pitis\u003c/li\u003e\n\u003cli\u003ePublish Year: ICML workshop 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, May 10, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://openreview.net/forum?id=NjOoxFRZA4\u0026amp;noteId=niZsZfTPPt\"\u003ehttps://openreview.net/forum?id=NjOoxFRZA4\u0026noteId=niZsZfTPPt\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510222642292\" loading=\"lazy\" src=\"/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"c3-preference-cannot-represented-as-numbers\"\u003eC3. Preference cannot represented as numbers\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510222758050\" loading=\"lazy\" src=\"/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510222758050.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"m1-rationality-level-of-human-preference\"\u003eM1. rationality level of human preference\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510223017797\" loading=\"lazy\" src=\"/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510223017797.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"32-if-the-conditioncontext-changes-the-preference-may-change-rapidly-and-this-cannot-reflect-on-the-reward-machine\"\u003e3.2, if the condition/context changes, the preference may change rapidly, and this cannot reflect on the reward machine\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510224002585\" loading=\"lazy\" src=\"/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510224002585.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"a2-preference-should-be-expressed-with-respect-to-state-policy-pairs-rather-than-just-outcomes\"\u003eA2. Preference should be expressed with respect to state-policy pairs, rather than just outcomes\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eA state-policy pair includes both the current state of the system and the strategy (policy) being employed. This approach avoids the complication of unresolved stochasticity (randomness that hasn\u0026rsquo;t yet been resolved), focusing instead on scenarios where the outcomes of policies are already known.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExample with Texas Hold’em\u003c/strong\u003e: The author uses an example  from poker to illustrate these concepts. In the example, a player  holding a weaker hand (72o) wins against a stronger hand (AA) after both commit to large bets pre-flop. Traditional reward modeling would prefer the successful trajectory of the weaker hand due to the positive  outcome. However, a rational analysis (ignoring stochastic outcomes)  would prefer the decision-making associated with the stronger hand (AA), even though it lost, as it\u0026rsquo;s typically the better strategy.\u003c/p\u003e","title":"Silviu Pitis Failure Modes of Learning Reward Models for Sequence Model 2023"},{"content":"[TOC]\nTitle: The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types Author: Gaurav R. Ghosal et. al. Publish Year: 9 Mar 2023 AAAI 2023 Review Date: Fri, May 10, 2024 url: arXiv:2208.10687v2 Summary of paper Contribution We find that overestimating human rationality can have dire effects on reward learning accuracy and regret We also find that fitting the rationality coefficient to human data enables better reward learning, even when the human deviates significantly from the noisy-rational choice model due to systematic biases Some key terms What is Boltzmann Rationality coefficient $\\beta$\nApply this Boltzmann rationality coefficient into PPO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import torch import torch.nn.functional as F class PolicyNetwork(torch.nn.Module): def __init__(self, ...): super().__init__() # define network layers def forward(self, state): # compute logits return logits def select_action(logits, tau): probabilities = F.softmax(logits / tau, dim=-1) action = torch.distributions.Categorical(probabilities).sample() return action # During training, adjust tau and use it in loss calculation tau = initial_tau # This could be a fixed value or decay over episodes logits = policy_network(state) action = select_action(logits, tau) # in real codebase def forward(self, x: th.Tensor, beta: th.Tensor=None) -\u0026gt; th.Tensor: x = self.linear(x) if beta is not None: x = x * beta # ! for Human Rationality Level Beta parameter, 0 meaning the reward signal is random, 1 meaning the reward signal is perfect logits = F.log_softmax(x, dim=-1) return logits essentially when the beta is low, the policy will have more exploration Results 1. remark: underestimating $\\beta$ is better than over estimating it\nthe author provided proof (proposition 1 and proposition 3)\nPotential future work What it did is straightforward, if you do not trust the reward signal, you let the policy to explore more.\n","permalink":"https://sino-huang.github.io/posts/gaurav-ghosal-the-effect-of-modeling-human-rationality-level-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types\u003c/li\u003e\n\u003cli\u003eAuthor: Gaurav R. Ghosal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 9 Mar 2023 AAAI 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, May 10, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2208.10687v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510211346583\" loading=\"lazy\" src=\"/posts/gaurav-ghosal-the-effect-of-modeling-human-rationality-level-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe find that overestimating human rationality can have dire effects on reward learning accuracy and regret\u003c/li\u003e\n\u003cli\u003eWe also find that fitting the rationality coefficient to human data enables better reward learning, even when the human deviates significantly from the noisy-rational choice model due to systematic biases\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is Boltzmann Rationality coefficient $\\beta$\u003c/strong\u003e\u003c/p\u003e","title":"Gaurav Ghosal the Effect of Modeling Human Rationality Level 2023"},{"content":"[TOC]\nTitle: Policy Optimization in Noisy Neighborhood Author: Nate Rahn et. al. Publish Year: NeruIPS 2023 Review Date: Fri, May 10, 2024 url: https://arxiv.org/abs/2309.14597 Summary of paper Contribution in this paper, we demonstrate that high-frequency discontinuities in the mapping from policy parameters $\\theta$ to return $R(\\theta)$​ are an important cause of return variation. As a consequence of these discontinuities, a single gradient step or perturbation to the policy parameters often causes important changes in the return, even in settings where both the policy and the dynamics are deterministic. unstable learning in some sense based on this observation, we demonstrate the usefulness of studying the landscape through the distribution of returns obtained from small perturbation of $\\theta$ Some key terms Evidence that noisy reward signal leads to substantial variance in performance\nIt is well-documented that agents trained with deep reinforcement learning can exhibit substantial variations in performance – as measured by their episodic return. The problem is particularly acute in continuous control, where these variations make it difficult to compare the end product of different algorithms or implementations of the same algorithm [ 11 , 20 ] or even reliably measure an agent’s progress from episode to episode [9].\n[9] Stephanie CY Chan, Samuel Fishman, Anoop Korattikara, John Canny, and Sergio Guadarrama. Measuring the Reliability of Reinforcement Learning Algorithms. In International Conference on Learning Representations, 2019.\n[11] Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. A hitchhiker’s guide to statistical comparisons of reinforcement learning algorithms. In Reproducibility in Machine Learning, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019. OpenReview.net, 2019.\n[12] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In International conference on machine learning, pages 1309–1318. PMLR, 2018\nResults Observation on the performance\nWe find qualitatively that the policy in the noisy neighborhood exhibits a curved gait which is sometimes faster, but unstable, whereas the policy in the smooth neighborhood produces an upright gait which can be slower, yet is very stable Unstability\nTaken together, these results demonstrate that some policies exist on the edge of failure, where a slight update can trigger the policy to take actions which push it out of its stable gait and into catastrophe interpolation policy in the same run\nDefinition of same run: starting from the same initialization and history of batches, that is one $\\theta$ is the old intermediate version of the latest $\\theta$\nBy contrast, when interpolating between policies from the same run, the transition from a noisy to a smooth landscape happens without encountering any valley of low return – even when these policies are separated by hundreds of thousands of gradient steps in training. This is particularly surprising given that θ is a high-dimensional vector containing all of the weights of the neural network, and there is no a priori reason to believe that interpolated parameters should result in policies that are at all sensible.\nSummary This work focus on how to stabilize the policy training.\nSadly it did not consider the case where the reward signal is noisy\n","permalink":"https://sino-huang.github.io/posts/nate-rahn-policy-optimization-in-noisy-neighbourhood-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Policy Optimization in Noisy Neighborhood\u003c/li\u003e\n\u003cli\u003eAuthor: Nate Rahn et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: NeruIPS 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, May 10, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2309.14597\"\u003ehttps://arxiv.org/abs/2309.14597\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240510141939305\" loading=\"lazy\" src=\"/posts/nate-rahn-policy-optimization-in-noisy-neighbourhood-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, we demonstrate that high-frequency discontinuities in the mapping from policy parameters $\\theta$ to return $R(\\theta)$​ are an important cause of return variation.\u003c/li\u003e\n\u003cli\u003eAs a consequence of these discontinuities, a single gradient step or perturbation to the policy parameters often causes important changes in the return, even in settings where both the policy and the dynamics are deterministic.\n\u003cul\u003e\n\u003cli\u003eunstable learning in some sense\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ebased on this observation, we demonstrate the usefulness of studying the landscape through the distribution of returns obtained from small perturbation of $\\theta$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEvidence that noisy reward signal leads to substantial variance in performance\u003c/strong\u003e\u003c/p\u003e","title":"Nate Rahn Policy Optimization in Noisy Neighbourhood 2023"},{"content":"[TOC]\nTitle: Language Reward Modulation for Pretraining Reinforcement Learning Author: Ademi Adeniji et. al. Publish Year: ICLR 2023 reject Review Date: Thu, May 9, 2024 url: https://openreview.net/forum?id=SWRFC2EupO Summary of paper Motivation Learned reward function (LRF) are notorious for noise and reward misspecification errors\nwhich can render them highly unreliable for learning robust policies with RL due to issues of reward exploitation and noisy models that these LRF’s are ill-suited for directly learning downstream tasks. Generalization ability issue of multi-modal vision and language model (VLM)\nnah, the author did not mention it Contribution Using LAMP directly as a task reward would not enable the RL agents to solve any of the tasks. As we evidence in Figure 3, and motivate in the introduction, rewards from current pretrained VLMs are far too noisy and susceptible to reward exploitation to be usable with online RL. In fact, this is central to our message - current VLMs are ill-suited for this purpose in that they lack the required precision to supervise challenging robot manipulation tasks, however, they do possess certain properties that render them very useful for pretraining RL agents with limited supervision. Results Summary Regrettably, the author did not provide compelling evidence to support the claim that the VLM reward signal is ineffective for training RL agents. Reviewers have requested further explanation on why VLM reward signals may fall short in training complex RL tasks.\nCritics often counter with remarks such as, \u0026ldquo;If this is indeed a problem, why then do many studies successfully employ visual-text alignment based reward models?\u0026rdquo; So, it is very challenging to convince people that certain widely accepted methods may be flawed or inadequate. It\u0026rsquo;s a tough journey.\nAdditionally, the pretraining evaluation section in the paper does not substantiate the author\u0026rsquo;s assertion that the VLM reward signal is noisy and therefore unsuitable for training RL tasks. From the presented pretraining performance, it is not apparent whether the VLM pretraining results are suboptimal.\nPotential future work it is equivalent to use Language based signals and then abandon it.\n","permalink":"https://sino-huang.github.io/posts/ademi-adeniji-language-reward-modulation-for-pretraining-rl-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language Reward Modulation for Pretraining Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Ademi Adeniji et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: ICLR 2023 reject\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, May 9, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://openreview.net/forum?id=SWRFC2EupO\"\u003ehttps://openreview.net/forum?id=SWRFC2EupO\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240509213653815\" loading=\"lazy\" src=\"/posts/ademi-adeniji-language-reward-modulation-for-pretraining-rl-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLearned reward function (LRF) are notorious for noise and reward misspecification errors\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewhich can render them highly unreliable for learning robust policies with RL\u003c/li\u003e\n\u003cli\u003edue to issues of reward exploitation and noisy models that these LRF’s are ill-suited for directly learning downstream tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGeneralization ability issue of multi-modal vision and language model (VLM)\u003c/strong\u003e\u003c/p\u003e","title":"Ademi Adeniji Language Reward Modulation for Pretraining Rl 2023"},{"content":"[TOC]\nTitle: Reward Model Ensembles Help Mitigate Overoptimization Author: Thomas Coste et. al. Publish Year: 10 Mar 2024 Review Date: Thu, May 9, 2024 url: arXiv:2310.02743v2 Summary of paper Motivation however, as imperfect representation of the \u0026ldquo;true\u0026rdquo; reward, these learned reward models are susceptible to over-optimization. Contribution the author conducted a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specially worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization the author additionally extend the setup to include 25% label noise to better mirror real-world conditions For PPO, ensemble-based conservative optimization always reduce overoptimization and outperforms single reward model optimization Some key terms Overoptimization\na phenomenon in which policy optimization appears to be making progress according to the learned reward model, but in reality begins to regress with respect to the true reward function Label noises\nIn the real-world RLHF setup, in which agreement rates among human annotators are typically between 60% - 75% (Ziegler et al., 2019; Stiennon et al., 2020; Dubois et al., 2023). Best of N Sampling\nMethod: reward model ensemble Results for PPO, with a small KL penalty coefficient of 0.01 ($\\beta$), WCO and UWO both successfully prevent overoptimization.\nSummary Use ensemble of reward models\n","permalink":"https://sino-huang.github.io/posts/thomas-coste-reward-model-ensembles-help-mitigate-overoptimization-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Model Ensembles Help Mitigate Overoptimization\u003c/li\u003e\n\u003cli\u003eAuthor: Thomas Coste et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Mar 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, May 9, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2310.02743v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240509140808445\" loading=\"lazy\" src=\"/posts/thomas-coste-reward-model-ensembles-help-mitigate-overoptimization-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ehowever, as imperfect representation of the \u0026ldquo;true\u0026rdquo; reward, these learned reward models are susceptible to over-optimization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author conducted a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specially worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization\u003c/li\u003e\n\u003cli\u003ethe author additionally extend the setup to include 25% label noise to better mirror real-world conditions\u003c/li\u003e\n\u003cli\u003eFor PPO, ensemble-based conservative optimization always reduce overoptimization and outperforms single reward model optimization\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eOveroptimization\u003c/strong\u003e\u003c/p\u003e","title":"Thomas Coste Reward Model Ensembles Help Mitigate Overoptimization 2024"},{"content":"[TOC]\nTitle: Internally Rewarded Reinforcement Learning Author: Mengdi Li et. al. Publish Year: 2023 PMLR Review Date: Wed, May 8, 2024 url: https://proceedings.mlr.press/v202/li23ax.html Summary of paper Motivation the author studied a class o RL problem where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy (parallel training on both the policy and the reward model) this leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning , and conversely, an under-optimized policy impedes discriminator learning we call this learning setting Internally Rewarded RL (IRRL) as the reward is not provided directly by the environment but internally by the discriminator. Contribution proposed the clipped linear reward function. Results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance. we formulate a class of RL problems as IRRL, and formulate the inherent issues of noisy rewards that leads to an unstable training loop in IRRL we empirically characterize the noise in the discriminator and derive the effect of the reward function in reducing the bias of the estimated reward and the variance of reward noise from an underdeveloped discriminator Comment: the author tried to express the bias and variance of reward noises in Taylor approximation propose clipped linear reward function Some key terms Simultaneous optimization causes suboptimal training\nAIM\nin this work, we seek to solve this issue by reducing the impact of reward noise, which is challenging due to the unavailability of an oracle discriminator whose posterior probability can reflect the information sufficiency for discrimination. Define discriminator\n$\\tau \\in (\\mathcal S \\times \\mathcal A)^n$ ($n \\in \\mathbb N$ is the trajectory length)\nthe discriminator $q_\\phi(y \\mid \\tau)$ computes the probability of label $y$ being the cause of trajectory $\\tau$.\nit is because $y$ is not accessible to the agent during policy training, thus we measure the alignment the goal $y$ with the collected $\\tau$ . Hard Attention example is one instance of IRRL\nI think Actor Critic is somehow similar to this scenario but the author did not mention it\u0026hellip;. Mutual Information maximization\nfrom deir paper:\nfrom this paper:\nin this equation, $p(y\\mid \\tau)$ is the oracle posterior probability that reflects the information sufficiency of observation ($\\tau$) for discrimination. It can be interpreted as being generated by an oracle discriminator, a conceptual term utilized for the theoretical formulation.\nExtend this mutual information to reward design and policy training\ndiscriminator training\nThe standard cross-entropy loss for training a classifier would be: $$ \\mathbb{E}{\\tau \\sim \\pi{\\theta}, y \\sim p(y)} \\left[ p(y \\mid \\tau) \\log q_{\\phi}(y \\mid \\tau) \\right], $$ but we drop $ p(y \\mid \\tau) $ by assuming it to be 1\nwhy:\nThe simplification to drop $ p(y \\mid \\tau) $ implicitly assumes that for every trajectory $ \\tau $, there is a direct, deterministic relationship to a label $ y $. This is equivalent to assuming $ p(y \\mid \\tau) = 1 $ for the sampled $ y $ and $ \\tau $. In practice, this means assuming that the trajectory $ \\tau $ contains all necessary information to unequivocally determine $ y $. then it means that we do not care about uncertainty Reward hacking = current LRM setting where the language reward model is trained beforehand\nsee the paper for more details\nGeneralized Reward and increasing function\nINTUITION: if we make g as linear, we have these nth derivative in Taylor approximation of the reward noise becomes 0,\nCLIPPED\nResults Summary think about reward noise, think about Taylor approximation of the reward noise representation, and then make it Linear to reduce the noise! Potential future work go check our theory to see if we can make it linear?\nnot really applicable to rewards signals that do not consider \u0026ldquo;log\u0026rdquo; but pure $p(y\\mid \\tau)$\nbut it contains a further $p(y)$, so maybe we can use this clipped reward signal $max(p(y \\mid \\tau) - p(y), 0)$ to compare with the pure $p(y \\mid r)$\n","permalink":"https://sino-huang.github.io/posts/mengdi-li-internally-rewarded-rl-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Internally Rewarded Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Mengdi Li et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023 PMLR\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, May 8, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://proceedings.mlr.press/v202/li23ax.html\"\u003ehttps://proceedings.mlr.press/v202/li23ax.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cimg src=\"image-assets/cover.png\" alt=\"image-20240508150740997\" style=\"zoom: 50%;\" /\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author studied a class o RL problem where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy (parallel training on both the policy and the reward model)\u003c/li\u003e\n\u003cli\u003ethis leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning , and conversely, an under-optimized policy impedes discriminator learning\u003c/li\u003e\n\u003cli\u003ewe call this learning setting Internally Rewarded RL (IRRL) as the reward is not provided directly by the environment but internally by the discriminator.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eproposed the clipped linear reward function. Results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance.\u003c/li\u003e\n\u003cli\u003ewe formulate a class of RL problems as IRRL, and formulate the inherent issues of noisy rewards that leads to an unstable training loop in IRRL\u003c/li\u003e\n\u003cli\u003ewe empirically characterize the noise in the discriminator and derive \u003cem\u003ethe\u003c/em\u003e \u003cem\u003eeffect of the reward function in reducing the bias of the estimated reward and the variance of reward noise from an underdeveloped discriminator\u003c/em\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eComment\u003c/strong\u003e: the author tried to express the bias and variance of reward noises in Taylor approximation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003epropose clipped linear reward function\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSimultaneous optimization causes suboptimal training\u003c/strong\u003e\u003c/p\u003e","title":"Mengdi Li Internally Rewarded Rl 2023"},{"content":"[TOC]\nTitle: On the Integration of Self-Attention and Convolution Author: Xuran Pan et. al. Publish Year: 2022 IEEE Review Date: Thu, Apr 25, 2024 url: https://arxiv.org/abs/2111.14556 Summary of paper Motivation there exists a strong underlying relation between convolution and self-attention. Related work Convolution NN\nit uses convolution kernels to extract local features, have become the most powerful and conventional technique for various vision tasks Self-attention only\nRecently, vision transformer shows that given enough data, we can treat an image as a sequence of 256 tokens and leverage Transformer models to achieve competitive results in image recognition. Attention enhanced convolution\nMultiple previously proposed attention mechanisms over images suggest it can overcome the limitation of locality for convolutional networks. Convolution enhanced Attention\nAmong which exist researchers focusing on complementing transformer models with convolution operations to introduce additional inductive biases. Add convolutions at the early stage to achieve stabler training. ","permalink":"https://sino-huang.github.io/posts/xuran-pan-on-the-integration-of-self-attention-and-convolution-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: On the Integration of Self-Attention and Convolution\u003c/li\u003e\n\u003cli\u003eAuthor: Xuran Pan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022 IEEE\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Apr 25, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2111.14556\"\u003ehttps://arxiv.org/abs/2111.14556\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethere exists a strong underlying relation between convolution and self-attention.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240425231916932\" loading=\"lazy\" src=\"/posts/xuran-pan-on-the-integration-of-self-attention-and-convolution-2022/image-assets/image-20240425231916932.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"related-work\"\u003eRelated work\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eConvolution NN\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit uses convolution kernels to extract local features, have become the most powerful and conventional technique for various vision tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSelf-attention only\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRecently, vision transformer shows that given enough data, we can treat an image as a sequence of 256 tokens and leverage Transformer models to achieve competitive results in image recognition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAttention enhanced convolution\u003c/strong\u003e\u003c/p\u003e","title":"Xuran Pan on the Integration of Self Attention and Convolution 2022"},{"content":"[TOC]\nTitle: Recent Language Model Technique 2024 Review Date: Thu, Apr 25, 2024 url: https://www.youtube.com/watch?v=kzB23CoZG30 url2: https://www.youtube.com/watch?v=iH-wmtxHunk url3: https://www.youtube.com/watch?v=o68RRGxAtDo LLama 3 key modification: grouped query attention (GQA)\nkey instruction-tuning process:\nTheir approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO). The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an outsized influence on the performance of aligned models. fine-tuning tool: torchtune\nLlama architecture RMSNorm pre-normalization: This is a normalization technique used in the model SwiGLU activation function: This is the activation function used in the model] Rotary Position Embedding (RoPE): Llama2 uses a new kind of positional embedding mechanism called Rotary Position Embedding (RoPE) Extra: Sampling and Rejection Sampling ref: https://www.youtube.com/watch?v=9ixzzPQWuAY (Inverse Transform Sampling)\nref: https://www.youtube.com/watch?v=OXDqjdVVePY (Accept-Reject Sampling)\nGrouped Query Attention Purpose: save computational cost Predecessor architecture: Multi-Query Attention Result\nNorNet: Efficient High Order Spatial Interaction with Recursive Gated Convolution ref: https://arxiv.org/abs/2207.14284\nCheck definition of depthwise convolution: https://www.youtube.com/watch?v=vVaRhZXovbw\nCapability Forgetting (from GLM-4) as a post-training step subsequent to SFT, the author also observed unexpected behaviour in the policy after the RLHF stage. The model shows a reduced capability in handling specific scenarios.\nreason this behavior could be attributed to the problem of difference among data distributions or the inability of the reward model in such nuanced details. solution to overcome, the author incorporate an extra supervised next token prediction loss as an additional regularization besides the KL divergence. (in PPO) this is intended to preserve the pre-existing abilities of the SFT model, by encouraging the model\u0026rsquo;s outputs to align with human preferences through RLHF and leveraging next-token prediction to capture more granular signals. the next token prediction with a small amount of human-annotated (prompt, response) pairs $\\mathcal{D}_S$, which are specific to particular tasks and serve as supervised regularization (trying to shifting the data distribution back) Direct Preference Optimization can we just use Cross entropy instead of PPO?\n","permalink":"https://sino-huang.github.io/posts/recent-language-model-technique-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Recent Language Model Technique 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Apr 25, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.youtube.com/watch?v=kzB23CoZG30\"\u003ehttps://www.youtube.com/watch?v=kzB23CoZG30\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eurl2: \u003ca href=\"https://www.youtube.com/watch?v=iH-wmtxHunk\"\u003ehttps://www.youtube.com/watch?v=iH-wmtxHunk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eurl3: \u003ca href=\"https://www.youtube.com/watch?v=o68RRGxAtDo\"\u003ehttps://www.youtube.com/watch?v=o68RRGxAtDo\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"llama-3\"\u003eLLama 3\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240425125031837\" loading=\"lazy\" src=\"/posts/recent-language-model-technique-2024/image-assets/image-20240425125031837.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ekey modification: grouped query attention (GQA)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ekey instruction-tuning process:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTheir approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO).\u003c/li\u003e\n\u003cli\u003eThe quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an \u003cstrong\u003e\u003cu\u003e\u003cem\u003eoutsized influence\u003c/em\u003e\u003c/u\u003e\u003c/strong\u003e on the performance of aligned models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efine-tuning tool: \u003ca href=\"https://github.com/pytorch/torchtune\"\u003etorchtune\u003c/a\u003e\u003c/p\u003e","title":"Recent Language Model Technique 2024"},{"content":"[TOC]\nTitle: Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning Author: Thomas Carta el. al. Publish Year: 6 Sep 2023 Review Date: Tue, Apr 23, 2024 url: arXiv:2302.02662v3 Summary of paper Summary The author considered an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online reinforcement learning to improve its performance to solve goals (under the RL paradigm environment (MDP))\nThe author studied several questions\nSample efficiency How fast can an LLM adapt and learn to solve various spatial and navigation problems specified in natural language? How does the use of pre-trained knowledge from LLM boosts sample efficiency? Generalization to new objects: Once functionally grounded, how can an LLM generalize to various kinds of changes about objects, yet staying in trained tasks? Generalization to new tasks: How can such an interactively trained LLM perform zero-shot generalization to new tasks? How does generalization depend on the kind of new tasks? Impact of online interventions: What is the empirical impact of grounding using online RL with incremental interactions in comparison with offline Behavioral Cloning from a dataset of expert trajectories? env: https://minigrid.farama.org/environments/minigrid/CrossingEnv/\nSample efficiency Generalisation we see that d (testing a new composition of tasks) becomes difficult for LLM policy agent (though others also failed)\nthe author claimed that it is because none of the agents managed to master the Pick up object A then go to object B task. ","permalink":"https://sino-huang.github.io/posts/thomas-carta-grounding-llms-in-rl-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Thomas Carta el. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 6 Sep 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Apr 23, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2302.02662v3\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240423132057110\" loading=\"lazy\" src=\"/posts/thomas-carta-grounding-llms-in-rl-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThe author considered an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online reinforcement learning to improve its performance to solve goals (under the RL paradigm environment (MDP))\u003c/p\u003e","title":"Thomas Carta Grounding Llms in Rl 2023"},{"content":"[TOC]\nTitle: Hierarchies of Reward Machines Author: Daniel Furelos-Blanco et. al. Publish Year: 4 Jun 2023 Review Date: Fri, Apr 12, 2024 url: https://arxiv.org/abs/2205.15752 Summary of paper Motivation Finite state machine are a simple yet powerful formalism for abstractly representing temporal tasks in a structured manner. Contribution The work introduces Hierarchies of Reinforcement Models (HRMs) to enhance the abstraction power of existing models. Key contributions include:\nHRM Abstraction Power: HRMs allow for the creation of hierarchies of Reinforcement Models (RMs), enabling constituent RMs to call other RMs. It\u0026rsquo;s proven that any HRM can be converted into an equivalent flat HRM with identical behavior. However, the equivalent flat HRM can have significantly more states and edges, especially under specific conditions.\nHierarchical Reinforcement Learning (HRL) Algorithm: A novel HRL algorithm is proposed to leverage HRMs by treating each call as a subtask. Learning policies in HRMs align with desired criteria, offering flexibility across multiple time scales and a richer range of abstract events and durations. Hierarchies also promote modularity and reusability of RMs and policies. Empirical results demonstrate that utilizing a handcrafted HRM leads to faster convergence compared to an equivalent flat HRM.\nCurriculum-Based Learning for HRMs: A curriculum-based method is introduced to learn HRMs from traces given a set of composable tasks. This approach is essential due to the complexity of learning flat HRMs from scratch. By decomposing an RM into several simpler ones, learning becomes feasible, and previously learned RMs can efficiently explore the environment for traces in more complex tasks.\nSome key terms Reward machine\nspecifically, each edge is labeled with (i) a formula over a set of high-level events that capture a task\u0026rsquo; subgoal, and (ii) a reward for satisfying the formula I think each node will be the key state\u0026hellip; RM fulfill the need for structuring events and durations, and keep track of the achieved and pending subgoals. The use of HRL\nthe subtask decomposition powered by HRL enables learning at multiple scales simultaneously, and ease the handling of long-horizon and sparse reward tasks. Limitation\nA common problem among methods learning minimal RMs is that they scale poorly as the number of states grows. $\\rightarrow$ learning the RM automatically is an open research question\nProblem Definition label trace\n$\\lambda_t = \\langle l(s_0), \u0026hellip;,l(s_t)\\rangle \\in (2^P)^+$ assigns labels to all states in history $\\langle s_0,a_0,\u0026hellip;, s_t\\rangle$\nassumption: $\\lambda_t, s_t$ captures all relevant information about history $h_t$ (i.e., the label trace is a summary of history state trajectory $h_t$​)\nwhere $\\tau$ is the termination function which will indicate whether a trajectory reach a terminal state or a goal state\nterminal state and goal state are different So, we can have different types of traces\nA trace $\\lambda_t$ is a goal trace if $(s^T_t, s^G_t) = (\\top, \\top)$, a dead-end trace if $(s^T_t, s^G_t) = (\\top, \\bot)$, an incomplete trace if $s^T_t = \\bot$. reward construction\nWe assume that the reward is $r(\\lambda_{t+1},s_{t+1})=\\mathbb{1}[\\tau(\\lambda_{t+1},s_{t+1})=(\\top,\\top)]$, i.e.~$1$ for goal traces and $0$​ otherwise. reward machine mapping to trace\nIdeally, RM states should capture traces, such that (i) pairs (u, s) of an RM state and an MDP state make termination and reward Markovian, (ii) the reward r(u, u′) matches the underlying MDP’s reward, and (iii) goal traces end in an accepting state, rejecting traces end in a rejecting state, and incomplete traces do not end in accepting or rejecting states.\nDNF definition\nit is just the propositional logic formula\nWhat is Disjunctive Normal Form (DNF)?\nDisjunctive Normal Form (DNF) is a standard logical format used in boolean algebra where a formula is expressed as a disjunction (OR operation) of several conjunctions (AND operations) of literals. A literal is either a proposition or its negation. This form makes it straightforward to evaluate whether the formula is true based on the truth values of its constituent propositions.\nStructure of a DNF Formula\nA DNF formula can be structured as follows: $$ \\text{DNF} = (l_{11} \\land l_{12} \\land \\ldots \\land l_{1n}) \\lor (l_{21} \\land l_{22} \\land \\ldots \\land l_{2n}) \\lor \\ldots \\lor (l_{m1} \\land l_{m2} \\land \\ldots \\land l_{mn}) $$ Where:\n$ l_{ij} $ is a literal, which can be either a proposition (e.g., $ p $) or its negation (e.g., $ \\neg p $). Each group of literals connected by AND (e.g., $ l_{11} \\land l_{12} \\land \\ldots \\land l_{1n} $) forms a clause. The entire formula is a disjunction (OR) of one or more such clauses. Examples of DNF Formulas\nHere are a few examples to illustrate DNF formulas, considering a set of propositions $ P = {a, b, c} $:\nSimple DNF Formula: $$ a \\land b $$ This formula has only one clause consisting of two literals without negation.\nSingle Clause with Negations: $$ a \\land \\neg b $$ This formula is also a single clause but includes a negation.\nMultiple Clauses: $$ (a \\land b) \\lor (\\neg a \\land c) $$ This DNF formula consists of two clauses. The first clause asserts that both $ a $ and $ b $ are true, while the second clause is true if $ a $ is false and $ c $ is true.\nMore Complex DNF: $$ (a \\land b \\land \\neg c) \\lor (b \\land c) \\lor (\\neg a \\land \\neg b) $$ Here, the formula has three clauses with a mixture of negated and non-negated literals. It is satisfied if either both $ a $ and $ b $ are true and $ c $ is false, or $ b $ and $ c $ are both true, or both $ a $ and $ b $ are false.\nDeterministic property\nthe trace or we say the trajectory can only match only one transition formula for example, if the trace obtained both rabbit leather and paper, the reward machine will only transit from u0 to u1 ","permalink":"https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Hierarchies of Reward Machines\u003c/li\u003e\n\u003cli\u003eAuthor: Daniel Furelos-Blanco et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 4 Jun 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Apr 12, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2205.15752\"\u003ehttps://arxiv.org/abs/2205.15752\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240412151420609\" loading=\"lazy\" src=\"/posts/daniel-hierarchies-of-reward-machines-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFinite state machine are a simple yet powerful formalism for abstractly representing temporal tasks in a structured manner.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe work introduces Hierarchies of Reinforcement Models (HRMs) to enhance the abstraction power of existing models. Key contributions include:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHRM Abstraction Power\u003c/strong\u003e: HRMs allow for the creation of hierarchies of Reinforcement Models (RMs), enabling constituent RMs to call other RMs. It\u0026rsquo;s proven that any HRM can be converted into an equivalent flat HRM with identical behavior. However, the equivalent flat HRM can have significantly more states and edges, especially under specific conditions.\u003c/p\u003e","title":"Daniel Hierarchies of Reward Machines 2023"},{"content":"[TOC]\nTitle: DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards Author: Shanchuan Wan et. al. Publish Year: 18 May 2023 Review Date: Fri, Apr 12, 2024 url: https://arxiv.org/abs/2304.10770 Summary of paper Motivation Recent studies have shown the effectiveness of encouraging exploration with intrinsic rewards estimated from novelties in observations However, there is a gap between the novelty of an observation and an exploration, as both the stochasticity in the environment and agent\u0026rsquo;s behaviour may affect the observation. Contribution we propose DEIR, a novel method in which we theoretically derive an intrinsic reward with a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and then implement the reward with a discriminative forward model. want to design a novel intrinsic reward design that considers not only the observed novelty but also the effective contribution brought by the agent. Some key terms internal rewards\nintrinsic rewards is devised to encourage visiting states that are likely to be more novel, where novelty is defined as either the distance between the current and the past observations or the difference between model predictions and realities. however, the relationship between the observed novelty and the agent\u0026rsquo;s actions has not yet been explicitly decoupled. In other words, effectively handling trivial novelties remains unaddressed, as they are rooted in the stochasticity in the environment’s dynamics and have little to do with the agent’s exploration capabilities (e.g., the “noisy TV” problem [Pathak et al., 2017]) I think the author wants to consider the action sequence to determine the novelty to prevent the noisy TV problem. exploration, what is it?\nthe method effectively decouples the stochasticity in the environment and the novelty gained by the agent\u0026rsquo;s exploration. the author wants to say that the exploration shall be come from the agent\u0026rsquo;s policy contribution rather than coming from the stochastic state transitions mutual information term\npurpose: designed to distinguish between the contributions to novelty caused by the state transitions and by the agent\u0026rsquo;s policy. the method incorporates the advantages of the two categories: while we explicitly encourage our agent to seek novel observations, we also rely on a discriminative model to construct a conditional mutual inforamation term that scales novelty in the observation space, incorporating the model-based prediction task from prediction error-driven methods discriminative forward model\ngenerative model focus on modeling the distribution discriminative model focus on the decision boundary The term \u0026ldquo;discriminative\u0026rdquo; refers to the fact that this model directly predicts the output (next state or observation) without explicitly modeling the underlying probability distribution of the data. RND\nRND defines the distance as the difference between the outputs of a parameter-fixed target neural network and a randomly initialized neural network. In this approach, the parameter-fixed network is used to be distilled into the learning network, effectively \u0026ldquo;evolving\u0026rdquo; a distance metric that adjusts dynamically the the agent\u0026rsquo;s experience. In probability notation, the symbols \u0026ldquo;;\u0026rdquo; and \u0026ldquo;,\u0026rdquo; are used to distinguish between different types of conditioning in a probabilistic model, specifically in the context of conditional probability distributions. Here\u0026rsquo;s what each symbol typically represents:\n\u0026quot;,\u0026quot; (comma): The comma is used to denote joint probabilities or to list multiple conditions in a conditional probability. It essentially represents a logical AND. For example, ( P(A, B) ) refers to the probability of both ( A ) and ( B ) occurring. In a conditional setting, ( P(X | Y, Z) ) indicates the probability of ( X ) given that both ( Y ) and ( Z ) are true.\n\u0026quot;;\u0026quot; (semicolon): The semicolon is used less frequently and serves to separate variables that are part of the conditioning environment from those that we condition upon. It\u0026rsquo;s particularly useful in more complex scenarios where clarity is needed about the nature of the conditioning. For example, ( P(X ; Y | Z) ) can be interpreted as the probability of ( X ), considering ( Y ) as a parameter or a fixed condition, given ( Z ). This notation isn\u0026rsquo;t standard in all statistical texts but can be seen in fields like information theory or in specific research papers where distinctions between types of conditions are crucial.\nP(X;Y|Z) vs P(X,Y|Z)\nComma Usage: ( P(X | Y, Z) ) is read as \u0026ldquo;the probability of ( X ) given ( Y ) and ( Z )\u0026rdquo;. Here, ( Y ) and ( Z ) are conditions that both need to be true for the probability of ( X ) to be considered.\nSemicolon Usage: ( P(X ; Y | Z) ) might be used (in certain contexts) to suggest a different nuance, such as \u0026ldquo;the probability of ( X ) under a model parameterized by ( Y ), given ( Z )\u0026rdquo;. This notation can help separate model parameters or fixed properties from the stochastic variables being conditioned on. This kind of notation might be found in contexts where parameters or regimes are considered alongside random variables, though, as noted, it is less standard and can vary in interpretation based on the field and context.\nIn general, the use of \u0026ldquo;;\u0026rdquo; in probability notation is much rarer and less standardized than \u0026ldquo;,\u0026rdquo;, and its meaning can depend heavily on the specific context or the author\u0026rsquo;s definitions. If you encounter this in a text or a paper, it\u0026rsquo;s advisable to look for an explanation or definition provided in that document to understand exactly how the author intends it to be interpreted.\nConditional Mutual Information\nhttps://arxiv.org/abs/1906.01824\nEpisodic Intrinsic rewards scaling the novelty\nKL divergence \u0026ndash; the higher means the two distribution are more dissimilar Mutual Information Formula:\nThis tells us how much the action (a_t) impacts the change in observation, beyond what could be expected just from knowing the current and previous states alone.\nBretagnolle–Huber Inequality: $$ \\mathrm{D_{KL}}(P | Q) \\geq - \\mathrm{log} (1 - \\mathrm{d^2_{TV}}(P, Q) ), $$\nA mathematical tool used here to simplify and lower-bound the KL divergence, which is a measure of how one probability distribution diverges from a second, expected probability distribution.\nSimplified Surrogate Function: $$ \\mathrm{D_{KL}}(P | Q) \\geq \\mathrm{log}{\\left( \\mathrm{dist}(s_t, s_i) \\right)} + \\frac{\\mathrm{dist}(o_{t+1}, o_i)}{\\mathrm{dist}(s_t, s_i)} + \\mathrm{const}. $$\nThis simplifies the KL divergence using the total variation distance and distances between states and observations, providing a way to estimate the mutual information in terms of observable quantities.\nFinal Formulation: $$ J \\ge \\min_{i}{\\frac{\\mathrm{dist}^2(o_{t+1}, o_i)}{\\mathrm{dist}(s_t, s_i)}}, $$\nThis provides a lower bound for the original objective, focusing on the squared ratio of differences between new and old observations to the differences in the corresponding states. This formulation simplifies the calculation while maintaining effectiveness.\nAssumption\nDistribution $P (x) = p(x|s_t, s_i, a_t)$, and assume given $s_t$, $s_i$, $a_t$ in deterministic environments (including POMDPs), $s_{t+1}$, $o_{t+1}$ and $D_{t+1, i}$, are uniquely determined. P is thus a unit impulse function which has the only non-zero value at $D_{t+1,i}$:\nconvert $-\\log(Q(D_{t+1,i}))$, where $Q(D_{t+1,i})$ = $p(dist(o_{t+1}, o_i) | s_t, s_i, a_t)$\nthe use of the exponential distribution Modeling Assumption: The exponential distribution is often used in scenarios where we are modeling the time or distance until an event occurs, especially when events happen independently with a constant average rate. In this context, the \u0026rsquo;event\u0026rsquo; is the observation at a certain distance from a previous observation, under the assumption that smaller distances are more likely than larger ones due to the continuity in state and observation spaces. so we assume $p(dist(o_{t+1}, o_i) | s_t, s_i, a_t)$ = $\\lambda\\exp(-\\lambda dist(o_{t+1}, o_i))$ where $\\lambda = \\frac{1}{dist(s_t, s_i)}$ is the rate parameter, usually it means that the expected value of dist(o) is dist(s), Thus, the average expected observation distance aligns with the underlying state distance, providing a direct and meaningful link between the state space and observation space. The logarithmic transformation of the probability density function (PDF) of the exponential distribution is used for simplification and to derive further insights. Here\u0026rsquo;s how it works:\nOriginal PDF: The PDF of the exponential distribution for $D_{t+1,i}$ is given by: $$ Q(D_{t+1,i}) = \\lambda e^{-\\lambda D_{t+1,i}} $$ Substituting $\\lambda = \\frac{1}{\\mathrm{dist}(s_t, s_i)}$​, the PDF becomes:\n$$ Q(D_{t+1,i}) = \\frac{1}{\\mathrm{dist}(s_t, s_i)} \\exp\\left(-\\frac{\\mathrm{dist}(o_{t+1}, o_i)}{\\mathrm{dist}(s_t, s_i)}\\right) $$\nLogarithm of the PDF: Applying the logarithm to the PDF, we get:\nThis simplifies using the properties of logarithms ($-\\log(ab) = -\\log a - \\log b$​) to:\nThe term $\\log(\\mathrm{dist}(s_t, s_i))$ reflects the natural logarithm of the distance between states, while the other term linearly scales the observation distance by the inverse of the state distance, effectively normalizing it.\nNow the issue is how to represent state Episodic Reward: The intrinsic reward $ r_t^{{\\tiny I}} $ is calculated and applied within the confines of a single episode. This means the reward is based solely on the agent\u0026rsquo;s experiences and actions from the start of the episode to the current timestep $ t $.\nObservations and Trajectories:\nEmbeddings of Observations ($e_{obs_t}$): These are feature representations derived from the observations $ o_t $. Embeddings reduce complex observations into a form (like vectors) that captures essential information in a more manageable size. Embeddings of Trajectories ($e_{traj_t}$): These are similar feature representations but for trajectories. A trajectory at time $ t $, $ \\tau_t $, is likely a sequence or aggregation of states or observations leading up to $ t $. The embedding $ t_{traj_t} $ serves as a practical approximation of the state $ s_t $, especially useful in scenarios where $ s_t $ cannot be directly observed (common in Partially Observable Markov Decision Processes or POMDPs). Euclidean Distance: The reward formula uses the Euclidean distance between these embeddings to quantify differences between past and current observations and trajectories.\nIntrinsic Reward Formula The intrinsic reward at time $ t $, denoted as $ r_t^{{\\tiny I}} $, is defined by:\n$$ r_t^{\\tiny I} = \\min_{\\forall i \\in \\left[0, t\\right)} \\left\\lbrace \\frac{ \\mathrm{dist}^2 (e_{obs_i}, e_{obs_{t+1}}) }{ \\mathrm{dist}(e_{traj_i}, e_{traj_t}) + \\epsilon} \\right\\rbrace $$\nComponents of the Formula Numerator ($ \\mathrm{dist}^2 (e_{obs_i}, e_{obs_{t+1}}) $): This represents the squared Euclidean distance between the current observation\u0026rsquo;s embedding and the embeddings of all previous observations. Squaring the distance emphasizes larger differences, making the reward more sensitive to novel observations.\nDenominator ($ \\mathrm{dist}(t_{traj_i}, t_{traj_t}) + \\epsilon $): This includes the Euclidean distance between the current trajectory embedding and all previous ones, adjusted by a small constant $ \\epsilon $ for numerical stability (to avoid division by zero). This denominator essentially normalizes the reward by how much the trajectory has changed, preventing the reward from becoming disproportionately large simply due to minor observation changes in similar state contexts.\nMinimization: The intrinsic reward takes the minimum value across all previous time steps $ i $​. This ensures that the reward is focused on the most novel (or least similar) observation relative to the past, which strongly encourages exploration of new states or actions that lead to new observations.\nLearning a discriminative model Results The result is pretty good\nhttps://github.com/swan-utokyo/deir\n","permalink":"https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards\u003c/li\u003e\n\u003cli\u003eAuthor: Shanchuan Wan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 18 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Apr 12, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2304.10770\"\u003ehttps://arxiv.org/abs/2304.10770\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240412151513920\" loading=\"lazy\" src=\"/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRecent studies have shown the effectiveness of encouraging exploration with intrinsic rewards estimated from novelties in observations\u003c/li\u003e\n\u003cli\u003eHowever, there is a gap between the novelty of an observation and an exploration, as both the stochasticity in the environment and agent\u0026rsquo;s behaviour may affect the observation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose DEIR, a novel method in which we theoretically derive an intrinsic reward with a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and then implement the reward\nwith a discriminative forward model.\u003c/li\u003e\n\u003cli\u003ewant to design a novel intrinsic reward design that considers not only the observed novelty but also the effective contribution brought by the agent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003einternal rewards\u003c/strong\u003e\u003c/p\u003e","title":"Shanchuan Efficient N Robust Exploration Through Discriminative Ir 2023"},{"content":"[TOC]\nTitle: Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning Author: Seungyong Moon et. al. Publish Year: 2 Nov 2023 Review Date: Tue, Apr 2, 2024 url: https://arxiv.org/abs/2307.03486 Summary of paper Contribution PPO agents demonstrate some ability to predict future achievements. Leveraging this observation, a novel contrastive learning method called achievement distillation is introduced, enhancing the agent\u0026rsquo;s predictive abilities. This approach excels at discovering hierarchical achievements, Some key terms Model based and explicit module in previous studies are not that good\nMany prior methods have been built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be advantageous for learning hierarchical dependencies. However, these methods demand an excessive number of environment interactions or large model sizes, limiting their practicality. requirements for modern agent\nTo successfully deploy RL agents in real-world scenarios, which are constantly changing and open-ended, they should generalize well to new unseen situations and acquire reusable skills for solving increasingly complex tasks via long-term reasoning. model-based methods\nthey predicts future states and rewards for learning long-term dependencies by employing a latent-world model (e.g., . DreamerV3) these methods have shown effectiveness in discovering hierarchical achievements, particularly in procedurally generated environments however, they are constructed with large model size, and often require substantial exploratory data, which limits their practicality hierarchical methods\nrely on prior knowledge, hard-code engineering they necessitate a significant number of offline expert data to reconstruct the graph **Big picture of the methodology **\nthe method periodically distills relevant information on achievements from episodes to the encoder via contrastive learning. maximize the similarity using optimal transport in the latent space between achievements from two different episodes, matching them using optimal transport, so that they will have the same semantic, Problem Setting and Assumptions\nassumptions they assume that the agent has no prior knowledge of the achievement graph, including the number of achievements and their dependencies the agent has no direct access to information about which achievements have been unlocked. instead, the agent must infer this information indirectly from the reward signal it receives bootstrapped value function\nthe bootstrapped value function is a technique used to estimate the value function target $\\hat{V}_t$​. \u0026ldquo;bootstrapping\u0026rdquo; means self-guiding In statistics and RL, bootstrapping is a sampling method that uses individual observations to estimate the statistics of the population. Bootstrapping: When you estimate something based on another estimation. Key observation relying solely on policy and value optimization to train the encoder can lead to suboptimal state representations, particularly in procedurally generated environments (i.e., cartoon images) PPO is good enough if we add size, add layer normalization and value normalization analyzing learned latent representations of the encoder\nwhat they did train a linear classifier using the latent representation as input to predict the very next achievement unlocked in the episode containing the current state, i.e., predict the intention using the latent vector this shows that PPO may struggle to generate optimal action sequence towards the current specific subgoal. issue\nwe cannot really label the achievement because we assume that the agent has no access to this kind of information, thus, an alternative way is to clearly distinguish different achievements FiLM layer\nthis is used to get the latent vector for the transition $$ \\mathrm{FiLM}\\theta(\\phi\\theta(s_t), a_t) = (1 + \\eta_\\theta(a_t)) \\phi_\\theta(s_t) + \\delta_\\theta(a_t), $$\nwhere $\\eta_\\theta$ and $\\delta_\\theta$ are two-layer MLPs, each with a hidden size of 1024\nContrastive learning for achievement distillation Intra-trajectory achievement prediction: Within an episode, this maximizes the similarity in the latent space between a state-action pair and its corresponding next achievement. rationale: this will group the sequence of achieving the next subgoal together Cross-trajectory achievement matching: Between episodes, this maximizes the similarity in the latent space for matched achievements. rationale: Since Crafter environments are procedurally generated, the achievement representations learned solely from intra-trajectory information may include environment-specific features that limit generalization Entropic Regularization ($\\alpha$ term):\nThe term $\\alpha \\sum_{i=1}^m \\sum_{j=1}^n T_{ij} \\log T_{ij}$ introduces entropic regularization, which encourages the transport plan to be smoother and more spread out. This discourages the solution from being too \u0026ldquo;spiky\u0026rdquo; (i.e., putting all probability mass into a single match), which can happen in cases of high dissimilarity or ambiguity. Entropic regularization makes the optimization problem easier to solve computationally and encourages solutions that are more robust to small variations in the data. Constraints:\nThe constraints ensure that the transport plan is valid. $T \\mathbf{1} \\leq \\mathbf{1}$ and $T^\\top \\mathbf{1} \\leq \\mathbf{1}$ ensure that no more mass is transported from an achievement than is available, and no more mass is received by an achievement than is possible. The equality constraint $\\mathbf{1}^\\top T^\\top \\mathbf{1} = \\min { m, n }$ ensures that the total transported mass equals the minimum sequence length, acknowledging that not all achievements can or should be matched. 1 2 3 4 5 6 7 # Match source and target goals a = np.ones(len(states_s)) b = np.ones(len(states_t)) M = 1 - th.einsum(\u0026#34;ik,jk-\u0026gt;ij\u0026#34;, states_s, states_t).cpu().numpy() # this calculates the cosine distance T = entropic_partial_wasserstein(a, b, M, reg=0.05, numItermax=100) T = th.from_numpy(T).float() row_inds, col_inds = th.where(T \u0026gt; 0.5) It seems that the entropic_partial_wasserstein fully represents the soft matching equation, thus we need to check what is entropic_partial_wasserstein\nusing memory\nwe concatenate the latent state representation with the previous achievement representation and the resulting vector is then fed into the policy to predict next action this allows the agent to know what is the previous achievement next achievement prediction through vector alignment\nAlgorithm Results For instance, our method collects iron with a probability over 3%, which is 20 times higher than DreamerV3. This achievement is extremely challenging due to its scarcity on the map and the need for wood and stone tools.\nother results\nvalue based RL (DQN) model just cannot play well due to complex environment. limitation Limitation - Lack of Evaluation on Transferability: The text identifies a critical limitation in the work, highlighting that the transferability of the method to an unsupervised setting has not been evaluated. Specifically, it\u0026rsquo;s unclear how the approach would perform in scenarios where an agent operates without any predefined rewards. In traditional RL, rewards guide learning by providing feedback on the desirability of actions taken in different states. The concern here is whether the method would still be effective if an agent had to learn without such guidance.\n","permalink":"https://sino-huang.github.io/posts/seungyong-discover-hierarchical-achieve-in-rl-via-cl-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Seungyong Moon et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year:  2 Nov 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Apr 2, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2307.03486\"\u003ehttps://arxiv.org/abs/2307.03486\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240402210833949\" loading=\"lazy\" src=\"/posts/seungyong-discover-hierarchical-achieve-in-rl-via-cl-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePPO agents demonstrate some ability to predict future achievements. Leveraging this observation, a novel contrastive learning method called achievement distillation is introduced, enhancing the agent\u0026rsquo;s predictive abilities. This approach excels at discovering hierarchical achievements,\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eModel based and explicit module in previous studies are not that good\u003c/strong\u003e\u003c/p\u003e","title":"Discover Hierarchical Achieve in Rl via Cl 2023"},{"content":"[TOC]\nTitle: Structured Chaint of Thought Prompting for Code Generation 2023 Author: Jia Li et. al. Publish Year: 7 Sep 2023 Review Date: Wed, Feb 28, 2024 url: https://arxiv.org/pdf/2305.06599.pdf Summary of paper Contribution The paper introduces Structured CoTs (SCoTs) and a novel prompting technique called SCoT prompting for improving code generation with Large Language Models (LLMs) like ChatGPT and Codex. Unlike the previous Chain-of-Thought (CoT) prompting, which focuses on natural language reasoning steps, SCoT prompting leverages the structural information inherent in source code. By incorporating program structures (sequence, branch, and loop structures) into intermediate reasoning steps (SCoTs), LLMs are guided to generate more structured and accurate code. Evaluation on three benchmarks demonstrates that SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1, is preferred by human developers in terms of program quality, and exhibits robustness to various examples, leading to substantial improvements in code generation performance.\nSome key terms code generation\ncode generation aims to automatically a program tat satisfies a given natural language requirement.\ndifferent from natural languages, source code contains rich structural information. sequence branch loop intuitively, intermediate reasoning steps leading to the structured code should also be structured. standard CoT ignores the program structures and has low accuracy in code generation. Structured CoT\nhow: it asks LLMs first to generate a SCoT using programs structures and then implement the code. SCoT prompting explicitly introduces program structures into intermediate reasoning steps and constraints LLMs to think about how to solve requirements form the view of programming language. Evaluation\nThree representative benchmarks (i.e., HumanEval [ 7], MBPP [ 2], and MBCPP [1]). We use unit tests to measure the correctness of generated programs and report the Pass@𝑘 (𝑘 ∈ [1, 3, 5]) [7 ].\nPass@k\nSpecifically, given a requirement, a code generation model is allowed to generate 𝑘 programs. The requirement is solved if any generated programs pass all test cases. We compute the percentage of solved requirements in total requirements as Pass@𝑘. For Pass@𝑘, a higher value is better.\nResults Summary A CoT is several intermediate natural language reasoning steps. This paper propose Structured CoT to allow program structures (i.e., sequence, branch, and loop structures) to explicitly reveal the program structure, which is helpful for code generation.\nin one sentence: having more informative CoT intermediate information helps ","permalink":"https://sino-huang.github.io/posts/jia-li-structured-cot-prompting-for-code-generation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Structured Chaint of Thought Prompting for Code Generation 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Jia Li et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 7 Sep 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.06599.pdf\"\u003ehttps://arxiv.org/pdf/2305.06599.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240229114924548\" loading=\"lazy\" src=\"/posts/jia-li-structured-cot-prompting-for-code-generation-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe paper introduces Structured CoTs (SCoTs) and a novel prompting  technique called SCoT prompting for improving code generation with Large Language Models (LLMs) like ChatGPT and Codex. Unlike the previous  Chain-of-Thought (CoT) prompting, which focuses on natural language  reasoning steps, SCoT prompting leverages the structural information  inherent in source code. By incorporating program structures (sequence,  branch, and loop structures) into intermediate reasoning steps (SCoTs),  LLMs are guided to generate more structured and accurate code.  Evaluation on three benchmarks demonstrates that SCoT prompting  outperforms CoT prompting by up to 13.79% in Pass@1, is preferred by  human developers in terms of program quality, and exhibits robustness to various examples, leading to substantial improvements in code  generation performance.\u003c/p\u003e","title":"Jia Li Structured Cot Prompting for Code Generation 2023"},{"content":"[TOC]\nTitle: Teaching Models to Express Their Uncertainty in Words Author: Stephanie Lin et. al. Publish Year: 13 Jun 2022 Review Date: Wed, Feb 28, 2024 url: https://arxiv.org/pdf/2205.14334.pdf Summary of paper Motivation The study demonstrates that a GPT-3 model can articulate uncertainty about its answers in natural language without relying on model logits. It generates both an answer and a confidence level (e.g., \u0026ldquo;90% confidence\u0026rdquo; or \u0026ldquo;high confidence\u0026rdquo;), which map to well-calibrated probabilities. The model maintains moderate calibration even under distribution shift and shows sensitivity to uncertainty in its answers rather than mimicking human examples.\nContribution Introduction of CalibratedMath, a test suite comprising elementary mathematics problems where models must provide both numerical answers and confidence levels in their answers. This allows testing of calibration under distribution shifts and presents a challenging assessment due to varying question types and difficulties for GPT-3. Demonstration that GPT-3 can learn to express calibrated uncertainty using verbalized probabilities, achieved through fine-tuning. The model exhibits reasonable calibration both in- and out-of-distribution, surpassing a strong baseline. Evidence that the calibration performance of verbalized probability is not solely dependent on learning to output logits, as GPT-3 does not merely replicate uncertainty information from logits. Superficial heuristics, like integer size in arithmetic questions, cannot fully explain the performance of verbalized probability. Comparison between verbalized probability and finetuning the model logits, showing that both methods generalize calibration under distribution shift. Finetuning The supervised fine-tuning dataset consists of approximately 10k examples,\nResults Verbalized probability generalizes well to both eval sets After finetuning on the Add-subtract training set, verbalized probabilities generalize reason- ably well to both the Multiply-divide and Multi-answer evaluation sets. So the model remains moderately calibrated under a substantial distribution shift.\nwe have already seen that verbalized probability generalizes better than the answer logit on the Multi-answer evaluation.\nNotably, the 50-shot approach achieves calibration levels nearly comparable to the finetuned setup.\nPotential future work Notably, the 50-shot approach achieves calibration levels nearly comparable to the finetuned setup.\n","permalink":"https://sino-huang.github.io/posts/stephanie-teaching-models-to-express-their-uncertainty-in-words-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Teaching Models to Express Their Uncertainty in Words\u003c/li\u003e\n\u003cli\u003eAuthor: Stephanie Lin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 13 Jun 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2205.14334.pdf\"\u003ehttps://arxiv.org/pdf/2205.14334.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240228162359685\" loading=\"lazy\" src=\"/posts/stephanie-teaching-models-to-express-their-uncertainty-in-words-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eThe study demonstrates that a GPT-3 model can articulate uncertainty  about its answers in natural language without relying on model logits.  It generates both an answer and a confidence level (e.g., \u0026ldquo;90%  confidence\u0026rdquo; or \u0026ldquo;high confidence\u0026rdquo;), which map to well-calibrated  probabilities. The model maintains moderate calibration even under  distribution shift and shows sensitivity to uncertainty in its answers  rather than mimicking human examples.\u003c/p\u003e","title":"Stephanie Teaching Models to Express Their Uncertainty in Words 2022"},{"content":"[TOC]\nTitle: Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement Author: Gwenyth Portillo Wightman et. al. Publish Year: TrustNLP 2023 Review Date: Tue, Feb 27, 2024 url: https://aclanthology.org/2023.trustnlp-1.28.pdf Summary of paper Motivation while traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. the authors proposed a method that involves comparing generated outputs across diverse prompts to create confidence score. By utilizing multiple prompts, they aim to obtain more precise confidence estimates, using response diversity as a measure of confidence. Contribution The results show that this method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt, which could be valuable for users relying on prediction confidence in larger systems or decision-making processes. in one sentence: try multiple times, get the mean, mean is more robust and consistent. Some key terms calibrated confidence score\na model is considered well calibrated if its prediction probabilities are aligned with the actual probability of its predictions being correct. if a model says an answer has 90% confidence, then we should expect it to be correct 90% of the time. summarized method\nconsider two approaches\nmeasure the log probability of the response across multiple prompts that agree on the same answer. measure the diversity in answer across different prompts in the model output, concluding that answers which appear in more responses have relatively higher confidence. the diversity measures the confidence, for example, suppose that for a given question queried across ten prompts, the model always replies eggplant. For a second question queried with the same prompts, the model answers potato (5 times) and eggplant, cucumber, squash, carrot and kale. We would say the model is more confident in its answer to the first question. directly ask GPT about the confidence\nref: Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words.\nit suggests that model have some notion of confidence in MCQ tasks\nResults the confidence estimate based on multiple prompts more accurately reflects the chance that a model is correct as compared to log probabilities from a single prompt. Summary Our experiments with T0++, FLAN-T5-XXL, and GPT-3 suggest that prompt agreement provides a more calibrated confidence estimate than the typical approach of log probability from a single prompt\n","permalink":"https://sino-huang.github.io/posts/gwenyth-estimating-confidence-of-llm-by-prompt-agreement-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement\u003c/li\u003e\n\u003cli\u003eAuthor: Gwenyth Portillo Wightman et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: TrustNLP 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Feb 27, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://aclanthology.org/2023.trustnlp-1.28.pdf\"\u003ehttps://aclanthology.org/2023.trustnlp-1.28.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240227154510805\" loading=\"lazy\" src=\"/posts/gwenyth-estimating-confidence-of-llm-by-prompt-agreement-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewhile traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated.\u003c/li\u003e\n\u003cli\u003ethe authors proposed a method that involves comparing generated outputs across diverse prompts to create confidence score. By utilizing multiple prompts, they aim to obtain more precise  confidence estimates, using response diversity as a measure of  confidence.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe results show that this method produces more calibrated confidence  estimates compared to the log probability of the answer to a single  prompt, which could be valuable for users relying on prediction  confidence in larger systems or decision-making processes.\u003c/li\u003e\n\u003cli\u003ein one sentence: try multiple times, get the mean, mean is more robust and consistent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ecalibrated confidence score\u003c/strong\u003e\u003c/p\u003e","title":"Gwenyth Estimating Confidence of Llm by Prompt Agreement 2023"},{"content":"[TOC]\nTitle: TIC: Translate-Infer-Compile for accurate \u0026ldquo;text to plan\u0026rdquo; using LLMs and logical intermediate representations Author: Sudhir Agarwal et. al. Publish Year: Jan 2024 Review Date: Sat, Feb 17, 2024 url: https://arxiv.org/pdf/2402.06608.pdf Summary of paper Motivation using an LLM to generate the task PDDL from a natural language planning task descriptions is challenging. One of the primary reasons for failure is that the LLM often make errors generating information that must abide by the constraints specified in the domain knowledge or the task descriptions\nIt could be that the crucial domain knowledge information is often implicitly embedded or hidden within the descriptions, making it difficult for the LLM to accurately generate the task PDDL.\nIn fact, in this paper, the issue that LLM faced is that LLMs cannot handle the \u0026ldquo;fact enumeration and object enumeration\u0026rdquo; well. e.g., the description \u0026ldquo;3 dispensers for 3 ingredients.\u0026rdquo; is more directly represented as the shortcut map(dispenser, dispenses, ingredient) rather than explicitly enumerating all the facts.\nContribution The approach described focuses on bridging the gap between natural language understanding and classical planning. It combines the strengths of large language models (LLMs) for natural language processing and classical planning tools for task planning. Unlike previous methods that directly use LLMs for generating Planning Domain Definition Language (PDDL) representations, this approach involves three steps:\nTranslate: LLMs are used to generate a logically interpretable intermediate representation of natural language task descriptions. Infer: Additional logically dependent information is derived from the intermediate representation using a logic reasoner, such as an Answer Set Programming solver. Compile: The target task PDDL is generated from the base and inferred information. By only using LLMs to output the intermediate representation, errors are significantly reduced. This approach, known as the TIC approach, achieves high accuracy in generating task PDDLs for all evaluated domains, leveraging the strengths of both LLMs and classical planning tools.\nSome key terms Intermediate Representation\nthe core idea of the TIC approach is the introduction of the intermediate representation. unlike the in-context example of direct PDDL generation approaches, an intermediate representation does not contain information that is not present in the task descriptions but required in the task PDDL, it means that the LLM does not need to derive dispenser 1, dispenser 2, dispenser 3 from the sentence \u0026ldquo;3 dispensers\u0026rdquo;. some are higher level rules and the LLM should not explicitly enumerating all the facts for example, a barman task description may not contain information corresponding to the PDDL facts (next 10 11), (next 11 12), because the knowledge that the shaker levels have an order is domain knowledge that is common for all barman queries. comments from sukai\nPDDL snippets sometimes is very concrete (e.g., you need to state all the fact facts (next n n+1), and we should prevent LLM from doing it because LLM is not good at stating low level things. Results Summary The core idea of the paper is that directly outputting PDDL from LLMs is challenging due to their inability to handle very concrete information, such as enumerating every facts to every objects, leading to error-prone translations. Instead, the paper suggests having LLMs translate to intermediate semi-formal representations that can represent higher level rules. This approach addresses the issue of LLMs having to explicitly enumerate all objects to state facts by allowing them to output more abstract representations (high level declaration and high level rules) and then they manually implemented ASP rules and ASP solvers to infer materialised representation. ASP refers to answer set programming.\nPotential future work Can we just replace the whole ASP section to asking LLMs to ask python code to handle the \u0026ldquo;object and fact enumeration\u0026rdquo; things. In this case, we do not need to manually writing ASP rules which were stated in Section 3.3. Translating to both PDDL and Python code\u0026hellip; Well, it leads to the hybrid modelling topic.\n","permalink":"https://sino-huang.github.io/posts/sudhir-agarwal-translate-infer-compile-for-accurate-text-to-plan-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  TIC: Translate-Infer-Compile for accurate \u0026ldquo;text to plan\u0026rdquo; using LLMs and logical intermediate representations\u003c/li\u003e\n\u003cli\u003eAuthor: Sudhir Agarwal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Feb 17, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2402.06608.pdf\"\u003ehttps://arxiv.org/pdf/2402.06608.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240217125703052\" loading=\"lazy\" src=\"/posts/sudhir-agarwal-translate-infer-compile-for-accurate-text-to-plan-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eusing an LLM to generate the task PDDL from a natural language planning task descriptions is challenging. One of the primary reasons for failure is that the LLM often make errors generating information that must abide by the constraints specified in the domain knowledge or the task descriptions\u003c/p\u003e","title":"Sudhir Agarwal Translate Infer Compile for Accurate Text to Plan 2024"},{"content":"[TOC]\nTitle: Intention Is Choice With Commitment Author: Philip Cohen et. al. Publish Year: 1990 Review Date: Tue, Jan 30, 2024 url: https://www.sciencedirect.com/science/article/pii/0004370290900555 Summary of paper Contribution This paper delves into the principles governing the rational balance between an agent\u0026rsquo;s beliefs, goals, actions, and intentions, offering valuable insights for both artificial agents and a theory of human action. It focuses on clarifying when an agent can abandon their goals and how strongly they are committed to these goals. The formalism used in the paper captures several crucial aspects of intentions, including an analysis of Bratman\u0026rsquo;s three characteristic functional roles of intentions and how agents can avoid intending all the unintended consequences of their actions. Furthermore, the paper discusses how intentions can be shaped based on an agent\u0026rsquo;s relevant beliefs and other intentions or goals. It also introduces a preliminary concept of interpersonal commitments by relating one agent\u0026rsquo;s intentions to their beliefs about another agent\u0026rsquo;s intentions or beliefs.\nSome key terms lack of commitment vs overcommitment in AI agent\nrational balance this paper tried to explore the relationship that intention plays in maintaining this balance to understand what it means for an agent to possess an intention, the paper emphasise the importance of describing how that intention influence the agent\u0026rsquo;s beliefs, commitments to future actions, and other interconnected intentions Logic for affect mental state of users as well as agents\nthe author proposed a logic suitable both for describing and reasoning about agent\u0026rsquo;s mental states as well as agents\u0026rsquo; abilities to affect the mental states of others AGENTS needs to reason about the beliefs, intentions and commitments of other agents. Intention in planning agent\nIf asked, the designer of a planning system may say that the notion of intention is defined operationally: A planning system\u0026rsquo;s intentions are no more than the contents of its plans. As such, intentions are representations of possible actions the system may take to achieve its goal(s).\nhowever, agents may form plans they never adopt, thus the notion of a plan lacks he characteristic commitment to action inherent in our commonsense understanding of intention Intention in philosophical theory\nthere can be two type of intentions For example, one\u0026rsquo;s future-directed intentions may include cooking dinner tomorrow, and one\u0026rsquo;s present-directed intentions may include moving an arm now this paper concentrate primarily on future-directed intentions intention vs belief\nbelief is often represented as proposition, intention is often represented as action intention is intimately connected with other attitude. Bratman\u0026rsquo;s three functional roles for intention\nIntentions normally pose problems for the agent; the agent needs to determine a way to achieve them. Intentions serve as a \u0026ldquo;screen of admissibility\u0026rdquo; for the adoption of other intentions. Unlike desires, which can be inconsistent, agents typically do not adopt intentions that they believe conflict with their existing present- and future-directed intentions. For instance, if an agent intends to hardboil an egg and is aware that they have only one egg and cannot obtain more in time, it would be irrational for them to simultaneously intend to make an omelette, as these two intentions are in conflict with each other. Agents \u0026ldquo;track\u0026rdquo; the success of their attempts to achieve their intentions. Not only do agents care whether their attempts succeed, but they are disposed to replan to achieve the intended effects if earlier attempts fail. not giving up too soon\nif an agent has an intention, he also needs to know what is the appropriate time to stop it the consistency maintainer needs to know whether the plan is carried out. McDermott attributes the problem to inability of various planning systems to express \u0026ldquo;Nell is going to be mashed unless I save her\u0026rdquo; Haas blames the problem on a failure to distinguish between the actual and possible events intention as a composite concept\nIntention will be modelled as a composite concept specifying what the agent has chosen, and how the agent is committed to that choice. Persistent goal\noccurs when an agent maintain a goal as long as specific conditions are met. Persistent goals reflect an agent\u0026rsquo;s commitment to a particular course of events over time. Intentions are modeled as a type of persistent goal, capturing both the agent\u0026rsquo;s choice and commitment. agents need not intend the expected side-effects of their intentions, as they may not be committed to those consequences due to changing beliefs. idealisation setting\nthe agent is going to adopt every persistent goal eventually Summary the work developed a logic for intention communication and understanding intention is the key for so called \u0026ldquo;Agent\u0026rdquo; or assistant ","permalink":"https://sino-huang.github.io/posts/philip-cohen-intention-is-choice-with-commitment-1990/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Intention Is Choice With Commitment\u003c/li\u003e\n\u003cli\u003eAuthor: Philip Cohen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 1990\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Jan 30, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.sciencedirect.com/science/article/pii/0004370290900555\"\u003ehttps://www.sciencedirect.com/science/article/pii/0004370290900555\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThis paper delves into the principles governing the rational balance  between an agent\u0026rsquo;s beliefs, goals, actions, and intentions, offering  valuable insights for both artificial agents and a theory of human  action. It focuses on clarifying when an agent can abandon their goals  and how strongly they are committed to these goals. The formalism used  in the paper captures several crucial aspects of intentions, including  an analysis of Bratman\u0026rsquo;s three characteristic functional roles of  intentions and how agents can avoid intending all the unintended  consequences of their actions. Furthermore, the paper discusses how  intentions can be shaped based on an agent\u0026rsquo;s relevant beliefs and other  intentions or goals. It also introduces a preliminary concept of  interpersonal commitments by relating one agent\u0026rsquo;s intentions to their  beliefs about another agent\u0026rsquo;s intentions or beliefs.\u003c/p\u003e","title":"Philip Cohen Intention Is Choice With Commitment 1990"},{"content":"[TOC]\nTitle: Christian Muise Planning for Goal Oriented Dialgue Systems 2019 Author: Publish Year: Review Date: Tue, Jan 30, 2024 url: arXiv:1910.08137v1 Summary of paper Motivation there is increasing demand for dialogue agents capable of handling specific tasks and interactions in a business context Contribution the author propose a new approach that eliminates the need for manual specification of dialogue trees, a common practice in existing systems. they suggest using a declarative representation of the dialogue agent, which can be processed by advanced planning tech (tree -\u0026gt; planning) The paper introduces a paradigm shift in specifying complex dialogue agents by recognizing that many aspects of these agents share similarities or identical underlying processes. Instead of manually creating and maintaining entire dialogue graphs, the authors propose a declarative approach where behavior is specified compactly, and the complete implicit graphs are generated from this specification. Some key terms limitation of end to end trained machine learning architectures\nparticularly in understanding the goal-oriented nature of dialogues. Initial attempts using recurrent neural networks were more suitable for casual chit-chat applications. Actions types\nthe interface (Program UI) allows three types of actions that perform different roles in the course of an interaction between the bot and the end user. Dialogue action are performed directly with the user via conversation. The designer out- lines how the bot expresses itself (such as in asking for credit card details) and possible user utterances in response for each possible outcome. System or “logic” actions are internal to the bot and are used to make internal inferences about the world to maintain and update state information (for example, in setting credit card details to known once the bot determines membership in loyalty program). Web actions or “cloudfunctions” allow the bot to respond to actionable items during an in- teraction (for example, placing an order with the acquired card details). This is crucial to the development of goal-oriented dialogue agents in domains such as customer support where the agent needs to perform actual tasks beyond chitchat, for example, to pull data from a user’s account, set information such as usernames, passwords, etc. issue tickets, book orders, and so on. Evaluation they demonstrate scale up of the size of declarative specification by having more variables and actions and see the generation time\nsize of specification number of variables number of actions needed size of graph number of node and eges time to generate Potential future work I cannot find a good metric here to evaluate such AI assistant\n","permalink":"https://sino-huang.github.io/posts/christian-muise-planning-for-goal-oriented-dialgue-systems-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Christian Muise Planning for Goal Oriented Dialgue Systems 2019\u003c/li\u003e\n\u003cli\u003eAuthor:\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Jan 30, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:1910.08137v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethere is increasing demand for dialogue agents capable of handling specific tasks and interactions in a business context\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author propose a new approach that eliminates the need for manual specification of dialogue trees, a common practice in existing systems.\u003c/li\u003e\n\u003cli\u003ethey suggest using a declarative representation of the dialogue agent, which can be processed by advanced planning tech (tree -\u0026gt; planning)\u003c/li\u003e\n\u003cli\u003eThe paper introduces a paradigm shift in specifying complex dialogue  agents by recognizing that many aspects of these agents share  similarities or identical underlying processes. Instead of manually  creating and maintaining entire dialogue graphs, the authors propose a  declarative approach where behavior is specified compactly, and the  complete implicit graphs are generated from this specification.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of end to end trained machine learning architectures\u003c/strong\u003e\u003c/p\u003e","title":"Christian Muise Planning for Goal Oriented Dialgue Systems 2019"},{"content":"[TOC]\nTitle: \u0026ldquo;On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS).\u0026rdquo; Author: Pallagani, Vishal, et al. Publish Year: arXiv preprint arXiv:2401.02500 (2024). Review Date: Mon, Jan 29, 2024 url: Summary of paper Contribution The paper provides a comprehensive review of 126 papers focusing on the integration of Large Language Models (LLMs) within Automated Planning and Scheduling, a growing area in Artificial Intelligence (AI). It identifies eight categories where LLMs are applied in addressing various aspects of planning problems:\nLanguage Translation Plan Generation Model Construction Multi-Agent Planning Interactive Planning Heuristics Optimization Tool Integration Brain-Inspired Planning For each category, the paper discusses the issues addressed and identifies existing gaps in research. It emphasizes that the true potential of LLMs emerges when they are integrated with traditional symbolic planners, advocating for a neuro-symbolic approach. This approach combines the generative capabilities of LLMs with the precision of classical planning methods, addressing complex planning challenges more effectively. The paper aims to encourage the ICAPS (International Conference on Automated Planning and Scheduling) community to recognize the complementary strengths of LLMs and symbolic planners, advocating for a direction in automated planning that leverages these synergistic capabilities to develop more advanced and intelligent planning systems.\nSome key terms position statement\nIntegrating LLMs into APS marks a pivotal advance- ment, bridging the gap between the advanced reason- ing of traditional APS and the nuanced language un- derstanding of LLMs. Traditional APS systems excel in structured, logical planning but often lack flexibility and contextual adaptability, a gap readily filled by LLMs. Conversely, while LLMs offer unparalleled nat- ural language processing and a vast knowledge base, they fail to generate precise, actionable plans where APS systems thrive. This integration surpasses the limitations of each standalone method, offering a dynamic and context-aware planning approach, while also scaling up the traditional use of data and past experiences in the planning process.\nCausal Language Modeling (CLMs):\nCLMs, such as GPT- 4, are designed for tasks where text generation is sequen- tial and dependent on the preceding context. Definition of classical planning problem\nIn-context learning\nIn-context learning refers to a machine learning concept where a model learns from the context provided in the input data, without the need for explicit external labels or fine-tuning.\nLanguage translation\nLT in the context of LLMs and planning involves converting natural language instructions into structured planning language like PDDL, using in-context learning techniques. This capability effectively bridges the gap between human linguistic expression and machine understandable format.\nThe LLM+P framework exemplifies this capability by translating natural language descriptions of planning problems into PDDL using GPT-4, solving them with classical planners, and translating solutions back into natural language, especially for robot planning scenarios. (Liu et al. 2023) Despite these advancements, a critical research gap emerges in the autonomous translation capabilities of LLMs, particularly in converting natural language to PDDL without external expert intervention.\nWhile LLMs effectively translate PDDL to natural language,\na notable gap is evident in their limited understanding of real-world objects and the problem of grounding affor- dances, mainly when translating natural language to structured languages like PDDL Model Construction\ninvolves the use of LLMs to create or refine world and domain models necessary for precise planning.\nGragera and Pozanco explore LLMs\u0026rsquo; capability in completing ill-defined PDDL domains. ","permalink":"https://sino-huang.github.io/posts/vishal-pallagani-llm-n-planning-survey-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  \u0026ldquo;On the Prospects of Incorporating Large  Language Models (LLMs) in Automated Planning and Scheduling (APS).\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eAuthor: Pallagani, Vishal, et al.\u003c/li\u003e\n\u003cli\u003ePublish Year:  \u003cem\u003earXiv preprint arXiv:2401.02500\u003c/em\u003e (2024).\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 29, 2024\u003c/li\u003e\n\u003cli\u003eurl:\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe paper provides a comprehensive review of 126 papers focusing on the integration of Large Language Models (LLMs) within Automated Planning and Scheduling, a growing area in Artificial Intelligence (AI). It identifies eight categories where LLMs are applied in addressing various aspects of planning problems:\u003c/p\u003e","title":"Vishal Pallagani Llm N Planning Survey 2024"},{"content":"[TOC]\nTitle: ProgPrompt: program generation for situated robot task planning using large language models Author: Ishika Singh et. al. Publish Year: 28 August 2023 Review Date: Mon, Jan 29, 2024 url: https://progprompt.github.io/ Summary of paper Motivation Classical Task planning\nrequires myriad domain knowledge large serach space, hard toscale domain specific require concrete goal specification Planning with LLMs\nLLM is not situated in the scene Plan steps using unavailable actions and objects Text-to-robot action mapping may not be trivial combinatorial admissible action space. Contribution present a programmatic LLM prompt structure that enables plan generation function across situated environments, robot capabilities and tasks ","permalink":"https://sino-huang.github.io/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  ProgPrompt: program generation for situated robot task planning using large language models\u003c/li\u003e\n\u003cli\u003eAuthor: Ishika Singh et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 28 August 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 29, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://progprompt.github.io/\"\u003ehttps://progprompt.github.io/\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eClassical Task planning\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erequires myriad domain knowledge\u003c/li\u003e\n\u003cli\u003elarge serach space, hard toscale\u003c/li\u003e\n\u003cli\u003edomain specific\u003c/li\u003e\n\u003cli\u003erequire concrete goal specification\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePlanning with LLMs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240129205334711\" loading=\"lazy\" src=\"/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/image-assets/image-20240129205334711.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLLM is not situated in the scene\u003c/li\u003e\n\u003cli\u003ePlan steps using unavailable actions and objects\u003c/li\u003e\n\u003cli\u003eText-to-robot action mapping may not be trivial\u003c/li\u003e\n\u003cli\u003ecombinatorial admissible action space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003epresent a programmatic LLM prompt structure that enables plan generation function across situated environments, robot capabilities and tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240129205546614\" loading=\"lazy\" src=\"/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e","title":"Ishika Singh Progprompt Program Generation for Robot Task Planning 2023"},{"content":"[TOC]\nTitle: Understanding Natural Language in Context Author: Avichai Levy et. al. Publish Year: ICAPS 2023 Review Date: Mon, Jan 29, 2024 url: https://ojs.aaai.org/index.php/ICAPS/article/view/27248 Summary of paper Contribution The paper discusses the increasing prevalence of applications with natural language interfaces, such as chatbots and personal assistants like Alexa, Google Assistant, Siri, and Cortana. While current dialogue systems mainly involve static robots, the challenge intensifies with cognitive robots capable of movement and object manipulation in home environments. The focus is on cognitive robots equipped with knowledge-based models of the world, enabling reasoning and planning. The paper proposes an approach to translate natural language directives into the robot\u0026rsquo;s formalism, leveraging state-of-the-art large language models, planning tools, and the robot\u0026rsquo;s knowledge of the world and its own model. This approach enhances the interpretation of directives in natural language, facilitating the completion of complex household tasks.\nSome key terms limitation of previous work\nHowever, these assistants are very limited in both their understanding of the world around them, and in the actions they can perform. Impor- tantly, these assistants are static – they do not move around the house, and they can not physically manipulate the world around them.\nAssumption\nWe now describe our system, which is illustrated in Figure 1, in more detail. First, we assume that we already have a cog- nitive robot, with a domain theory that describes the types of objects in the world, the possible relations between them, and the possible actions that the robot can execute – that is, a PDDL domain. We also assume that our robot is clairvoyant, and has complete and perfect information about the state of the world, including which objects exists in the world and the relations between them.\nTranslation system\nThe translation system receives input from two sources:\nDirective: Provided by a human in natural language, this instructs the robot to perform a specific task. Directives can range from high-level task descriptions (e.g., \u0026ldquo;put a slice of tomato in the fridge\u0026rdquo;) to more detailed instructions (e.g., specifying actions like going to the kitchen, picking up a knife, slicing a tomato, etc.).\nState: The robot possesses knowledge of the world\u0026rsquo;s current state, including existing objects and their relationships. This information is integral for understanding the context in which the directive is given.\nBy combining these modalities, the system enables the robot to interpret natural language directives and execute tasks accurately within its environment.\nResults The system integrates state-of-the-art pretrained large language models and achieves notable performance metrics on the ALFRED dataset:\nTask Translation: The system achieves an 85% accuracy rate in translating tasks expressed in natural language into specific goals for the robot to accomplish.\nPlan Execution: It attains a 57% accuracy rate in following plans expressed in natural language on the ALFRED dataset.\nThese results demonstrate the system\u0026rsquo;s effectiveness in understanding and executing tasks based on natural language instructions.\n","permalink":"https://sino-huang.github.io/posts/avichai-levy-understanding-natural-language-in-context-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Understanding Natural Language in Context\u003c/li\u003e\n\u003cli\u003eAuthor: Avichai Levy et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: ICAPS 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 29, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://ojs.aaai.org/index.php/ICAPS/article/view/27248\"\u003ehttps://ojs.aaai.org/index.php/ICAPS/article/view/27248\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240129203153924\" loading=\"lazy\" src=\"/posts/avichai-levy-understanding-natural-language-in-context-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe paper discusses the increasing prevalence of applications with natural language interfaces, such as chatbots and personal assistants like Alexa, Google Assistant, Siri, and Cortana. While current dialogue systems mainly involve static robots, the challenge intensifies with cognitive robots capable of movement and object manipulation in home environments. The focus is on cognitive robots equipped with knowledge-based models of the world, enabling reasoning and planning. The paper proposes an approach to translate natural language directives into the robot\u0026rsquo;s formalism, leveraging state-of-the-art large language models, planning tools, and the robot\u0026rsquo;s knowledge of the world and its own model. This approach enhances the interpretation of directives in natural language, facilitating the completion of complex household tasks.\u003c/p\u003e","title":"Avichai Levy Understanding Natural Language in Context 2023"},{"content":"[TOC]\nTitle: The Impact of Reasoning Steps Length on Large Language Models Author: Mingyu Jin et. al. Publish Year: 20 Jan 2024 Review Date: Mon, Jan 29, 2024 url: arXiv:2401.04925v3 Summary of paper Contribution The study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:\nEffect of Reasoning Step Length:\nLengthening reasoning steps in prompts, even without introducing new information, notably improves LLMs\u0026rsquo; reasoning abilities across various datasets. Conversely, shortening reasoning steps, while preserving key information, notably diminishes LLMs\u0026rsquo; reasoning abilities. This suggests the critical role of reasoning step length in CoT prompts and offers practical insights for leveraging LLMs in complex problem-solving scenarios. Impact of Rationales:\nSurprisingly, even incorrect rationales can lead to favorable outcomes if they maintain the necessary length of inference. This finding suggests that the length of reasoning steps may compensate for inaccuracies in rationales, emphasizing the importance of sequence length in CoT. Task-Dependent Nature:\nThe advantages of increasing reasoning steps vary depending on the complexity of tasks: Simpler tasks require fewer steps. Complex tasks benefit significantly from longer inference sequences. The study underscores the significance of reasoning step length in CoT prompts for enhancing LLMs\u0026rsquo; reasoning abilities and provides practical guidance for optimizing their performance in diverse problem-solving contexts.\nSome key terms incorrect but coherent rationales can improve reasoning performance\nInterestingly, Wang et al. found that even incorrect but coherent rationales can improve reasoning performance, highlighting the value of logical continuity (Wang et al., 2023). Strategies\nFew-shot setting\nthink about the word. This process does not introduce new information. Read the question again: Read the questions repeatedly to reduce the interference of other texts on the chain of thought. Repeat State: we include a small summary of the current state after a long chain of reasoning Self-Verification: before the model gets the answer, we add a self-verification process to judge whether the answer is reasonable based on some basic information. Zero-shot setting\naltered the initial prompt from “Let’s think step by step\u0026quot; to “Let’s think step by step, you must think more steps\u0026quot; Results The study emphasizes the significance of the length of the thinking chain rather than its accuracy in improving Chain of Thought (CoT) performance. Here are the key findings:\nLinear Correlation between Step Count and Accuracy:\nIn few-shot CoT scenarios, there exists a direct linear correlation between the number of reasoning steps and accuracy. Lengthening reasoning steps notably enhances Large Language Models\u0026rsquo; (LLMs) reasoning abilities across multiple datasets. Conversely, shortening reasoning steps significantly diminishes model performance, even when key information is preserved. Role of Incorrect Rationales:\nEven incorrect rationales can produce favorable outcomes if they maintain the necessary length of inference. Errors in intermediate numbers, particularly in process-oriented tasks like mathematical problems, have a minor impact on overall performance. Task-Dependent Nature:\nThe benefits of increasing reasoning steps depend on the complexity of tasks: Simpler tasks require fewer steps. More complex tasks benefit significantly from longer inference sequences. Enhancement in Zero-Shot CoT:\nIncreasing reasoning steps in zero-shot CoT notably improves LLM accuracy. Altering the initial prompt to explicitly encourage more reasoning steps led to noticeable enhancements, particularly in datasets involving mathematical problems. Overall, the findings suggest that optimizing CoT prompting involves prioritizing the length of the reasoning chain, which significantly impacts LLMs\u0026rsquo; reasoning abilities across various tasks and scenarios.\n","permalink":"https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: The Impact of Reasoning Steps Length on Large Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Mingyu Jin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 20 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 29, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.04925v3\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEffect of Reasoning Step Length\u003c/strong\u003e:\u003c/p\u003e","title":"Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024"},{"content":"[TOC]\nTitle: Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision Author: Collin Burns et. al. Publish Year: 14 Dec 2023 Review Date: Mon, Jan 29, 2024 url: arXiv:2312.09390v1 Summary of paper Motivation Superalignment: OPENAI believe that RLHF is essentially use human to supervise the model (RM is trained by human annotation). One day when superhuman models come out, human are no longer to annotate the good / bad of the model\u0026rsquo;s output. e.g., superhuman model generate a 1M lines complex code and human cannot review it. How to do the alignment in for this case? thus the research question is can we use a weak teacher model to improve strong student model Contribution they used weak model to generate annotations and fine tune the strong model, they empirically did a lot of experiments note: although they use the term teacher and student, the alignment task is not about \u0026ldquo;teaching\u0026rdquo;, alignment is to elicit learnt stuffs from strong foundation model (something like finetuning), rather than asking strong model to follow weak teacher model. Some key terms Bootstrapping\nBootstrapping refers to a process where an initial starting point, often basic or minimal, is used to gradually build up more complex or sophisticated systems or solutions. In various contexts, bootstrapping involves self-starting, self-sustaining processes that rely on minimal external resources to initiate growth or development. approach\ncreate the weak supervisor train a strong student model with weak supervision compare train a strong model with ground truth labels as ceiling limitation of approach\nimitation saliency: today\u0026rsquo;s mistake made by weak model may not. be the same mistake type when human try to supervise superhuman model pretraining leakage: the pretraining dataset contains human supervision implicitly. The superhuman model can learn a lot from self-learning. (e.g., AlphaGo) Performance Gap Recovered Metric\nResults NLP task PGR increases as Student Model\u0026rsquo;s ability increases Weak Student model is able to enhance weak-to-strong supervision even when student model is very weak Chess puzzle when using weakest student model, PGR get\u0026rsquo;s to 0 when Strong model gets stronger, the performance gap recovered gets smaller. ChatGPT reward modelling Very bad generalisation, PGR cannot pass 20% Summary Direct learning in Strong Student model is the best Weak to strong alignment is feasible Alignment task is elicitation rather than learning from scratch ","permalink":"https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision\u003c/li\u003e\n\u003cli\u003eAuthor: Collin Burns et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 14 Dec 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 29, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2312.09390v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240129161843295\" loading=\"lazy\" src=\"/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSuperalignment: OPENAI believe that RLHF is essentially use human to supervise the model (RM is trained by human annotation). One day when superhuman models come out, human are no longer to annotate the good / bad of the model\u0026rsquo;s output. e.g., superhuman model generate a 1M lines complex code and human cannot review it.\u003c/li\u003e\n\u003cli\u003eHow to do the alignment in for this case?\u003c/li\u003e\n\u003cli\u003ethus the research question is can we use a weak teacher model to improve strong student model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethey used weak model to generate annotations and fine tune the strong model, they empirically did a lot of experiments\u003c/li\u003e\n\u003cli\u003enote: although they use the term teacher and student, the alignment task is not about \u0026ldquo;teaching\u0026rdquo;, alignment is to elicit learnt stuffs from strong foundation model (something like finetuning), rather than asking strong model to follow weak teacher model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eBootstrapping\u003c/strong\u003e\u003c/p\u003e","title":"Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision"},{"content":"[TOC]\nTitle: Hallucination Is Inevitable an Innate Limitation Llm 2024 Author: Ziwei Xu et. al. Publish Year: 22 Jan 2024 Review Date: Sun, Jan 28, 2024 url: arXiv:2401.11817v1 Summary of paper Contribution The paper formalizes the issue of hallucination in large language models (LLMs) and argues that it is impossible to completely eliminate hallucination. It defines hallucination as inconsistencies between a computable LLM and a computable ground truth function. By drawing from learning theory, the paper demonstrates that LLMs cannot learn all computable functions, thus always prone to hallucination. The formal world is deemed a simplified representation of the real world, implying that hallucination is inevitable for real-world LLMs. Additionally, for real-world LLMs with provable time complexity constraints, the paper identifies tasks prone to hallucination and provides empirical validation. Finally, the paper evaluates existing hallucination mitigators using the formal world framework and discusses practical implications for the safe deployment of LLMs.\nSome key terms hallucination\nthe model generate plausible but factually incorrect or nonsensical information Up to now, research on LLM hallucination remains largely empirical. Useful as they are, empirical studies cannot answer the fundamental question: can hallucination be completely eliminated? The answer to this question is fundamental as it indicates a possible upper limit of LLMs’ abilities. Formal definition of hallucination is difficult\nIn the real world, formally defining hallucination, a factual or logical error of LLM, turns out to be extremely difficult. This is because a formal definition of semantics in the real world is still an open problem [12, 58]. To address this, the paper establishes a formal world of computable functions where precise discussions on hallucination become feasible. Hallucination is defined as the failure of an LLM to reproduce the output of a computable function exactly. Results In defence of LLMs and Hallucination\nLLMs are continuously evolving, with advancements in model architecture and error correction strategies expected to mitigate the severity of hallucinations over time. While complete elimination is improbable, researchers aim to better understand and control hallucination for various applications.\nMoreover, hallucination is not entirely negative. In creative fields like art and literature, the unintended outputs from LLMs can inspire human creators, offering unique perspectives and fostering innovation. Thus, the hallucinatory aspect of LLMs can be viewed positively as a source of creativity and inspiration.\nPractical implications\nGuardrails and Fences are Essential: Without proper guardrails and fences, LLMs cannot be relied upon for critical decision-making. These mechanisms are designed to ensure that LLMs operate within expected boundaries and do not deviate into unethical, disturbing, or destructive content. Given the inevitability of hallucination, guardrails and fences are deemed essential safeguards.\nSummary Possible hallucination mitigators\nlarger models and more training data Prompting LLMs with Chain of Thoughts/Reflections/Verification Prompting is effective in mitigating hallucination by guiding them towards solutions more preferred by humans, which are possibly the ones with lower complexities, and within LLMs’ capabilities. However, this approach will only work for specific tasks. Ensemble of LLMs This approach uses multiple instances of LLMs to solve a single problem. The solution is usually produced through majority votes Guardrails and Fences The guardrails are principles that align LLMs’ output with human values, ethics, and legal requirements. The fences is a list of critical tasks that should never be fully automated using LLMs ","permalink":"https://sino-huang.github.io/posts/ziwei-xu-hallucination-is-inevitable-an-innate-limitation-llm-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Hallucination Is Inevitable an Innate Limitation Llm 2024\u003c/li\u003e\n\u003cli\u003eAuthor: Ziwei Xu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.11817v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eThe paper formalizes the issue of hallucination in large language models (LLMs) and argues that it is impossible to completely eliminate hallucination. It defines hallucination as inconsistencies between a computable LLM and a computable ground truth function. By drawing from learning theory, the paper demonstrates that LLMs cannot learn all computable functions, thus always prone to hallucination. The formal world is deemed a simplified representation of the real world, implying that hallucination is inevitable for real-world LLMs. Additionally, for real-world LLMs with provable time complexity constraints, the paper identifies tasks prone to hallucination and provides empirical validation. Finally, the paper evaluates existing hallucination mitigators using the formal world framework and discusses practical implications for the safe deployment of LLMs.\u003c/p\u003e","title":"Ziwei Xu Hallucination Is Inevitable an Innate Limitation Llm 2024"},{"content":"[TOC]\nTitle: Improving Machine Translation Use Quality Estimation as a Reward Model 2024 Author: Zhiwei He et. al. Publish Year: 23 Jan 2024 Review Date: Sun, Jan 28, 2024 url: arXiv:2401.12873v1 Summary of paper Contribution In this research, the authors explore using Quality Estimation (QE) models as a basis for reward systems in translation quality improvement through human feedback. They note that while QE has shown promise aligning with human evaluations, there\u0026rsquo;s a risk of overoptimization where translations receive high rewards despite declining quality. The study addresses this by introducing heuristic rules to identify and penalize incorrect translations, resulting in improved training outcomes. Experimental results demonstrate consistent enhancements across various setups, validated by human preference studies. Additionally, the approach proves highly data-efficient, outperforming systems relying on larger parallel corpora with only a small amount of monolingual data.\nSome key terms Preliminary experiment result\nOveroptimisation\nThe researchers identified an overoptimization problem in their translation quality improvement system, where increasing rewards led to declining translation performance. This issue, observed in preliminary experiments, was termed \u0026ldquo;overoptimization\u0026rdquo; and attributed to the imperfect alignment between the reward model and human preferences, reminiscent of Goodhart\u0026rsquo;s Law.\nThey found that the reward model sometimes gave high scores to erroneous translations, particularly those involving common machine translation errors like length-ratio discrepancies, off-target errors, and hallucinations. These errors, if rewarded highly, could propagate and disrupt the training process significantly.\nTo address overoptimization, the researchers proposed monitoring length-ratio and off-target errors during training and penalizing them with negative rewards. They introduced a criterion C(x, y) to identify unacceptable length ratios and off-target translations, assigning punitive rewards accordingly. This solution aimed to mitigate overoptimization and improve translation quality during training.\nSummary Very simple and straightforward solution\n","permalink":"https://sino-huang.github.io/posts/zhiwei-he-improving-machine-translation-use-quality-estimation-as-a-reward-model-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Improving Machine Translation Use Quality Estimation as a Reward Model 2024\u003c/li\u003e\n\u003cli\u003eAuthor: Zhiwei He et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 23 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.12873v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eIn this research, the authors explore using Quality Estimation (QE) models as a basis for reward systems in translation quality improvement through human feedback. They note that while QE has shown promise aligning with human evaluations, there\u0026rsquo;s a risk of overoptimization where translations receive high rewards despite declining quality. The study addresses this by introducing heuristic rules to identify and penalize incorrect translations, resulting in improved training outcomes. Experimental results demonstrate consistent enhancements across various setups, validated by human preference studies. Additionally, the approach proves highly data-efficient, outperforming systems relying on larger parallel corpora with only a small amount of monolingual data.\u003c/p\u003e","title":"Zhiwei He Improving Machine Translation Use Quality Estimation as a Reward Model 2024"},{"content":"[TOC]\nTitle: SayPlan: Grounding Large Language Models using 3D Scene for for Scalable Task Planning Author: Krishan Rana Publish Year: CoRL 2023 Review Date: Sun, Jan 28, 2024 url: https://arxiv.org/abs/2307.06135 Summary of paper Motivation this is a pipeline introduction paper Contribution Hierarchical Exploration: SayPlan leverages the hierarchical structure of 3DSGs to enable LLMs to conduct semantic searches for task-relevant subgraphs from a condensed representation of the full graph. Path Planning Integration: It integrates a classical path planner to reduce the planning horizon for the LLM, thus improving efficiency. Iterative Replanning Pipeline: An iterative replanning pipeline refines initial plans by incorporating feedback from a scene graph simulator, correcting infeasible actions and preventing planning failures. Some key terms Semantic search stage\nThe Semantic Search stage in SayPlan addresses the challenges of planning over 3D scene graphs (3DSGs) using Large Language Models (LLMs) by considering two key observations:\nPractical Representation Limitations: Due to token limits and the potential infinite growth of a large-scale environment\u0026rsquo;s 3DSG, it\u0026rsquo;s impractical to pass the full representation to an LLM. Task-Specific Subset Identification: Only a subset of the full 3DSG, denoted as G\u0026rsquo;, is necessary to solve a given task, as irrelevant details can be disregarded. To identify the task-specific subgraph G\u0026rsquo; from the full 3DSG, SayPlan leverages:\nSemantic Hierarchy: SayPlan exploits the semantic hierarchy within 3DSGs. Reasoning Capabilities of LLMs: The LLM is guided to manipulate the collapsed graph via expand and contract API calls, reducing the token representation by approximately 80%. The process involves:\nManipulation of Collapsed Graph: The LLM manipulates the collapsed graph through expand and contract API calls based on the given task instruction I, utilizing in-context learning and chain-of-thought prompting. Scene Graph Simulator Interaction: API calls and node manipulations are executed within the scene graph simulator. Maintenance of Task-Specific Subgraph: If expanded nodes contain irrelevant entities, the LLM contracts them to manage token limitations and maintain the task-specific subgraph. Memory Input: A list of previously expanded nodes is maintained and passed as additional memory input to facilitate decision-making. Autonomous Planning Phase: The LLM proceeds to the planning phase once all necessary assets and objects are identified in the current subgraph G'. An example of the LLM-scene graph interaction during Semantic Search is provided, demonstrating the systematic approach adopted by SayPlan to efficiently identify task-specific subsets within large-scale 3DSGs for effective planning.\n","permalink":"https://sino-huang.github.io/posts/krishan-rana-sayplan-grounding-llm-for-scalable-task-planning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: SayPlan: Grounding Large Language Models using 3D Scene for for Scalable Task Planning\u003c/li\u003e\n\u003cli\u003eAuthor: Krishan Rana\u003c/li\u003e\n\u003cli\u003ePublish Year: CoRL 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2307.06135\"\u003ehttps://arxiv.org/abs/2307.06135\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis is a pipeline introduction paper\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHierarchical Exploration\u003c/strong\u003e: SayPlan leverages the hierarchical structure of 3DSGs to enable LLMs to conduct semantic searches for task-relevant subgraphs from a condensed representation of the full graph.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePath Planning Integration\u003c/strong\u003e: It integrates a classical path planner to reduce the planning horizon for the LLM, thus improving efficiency.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIterative Replanning Pipeline\u003c/strong\u003e: An iterative replanning pipeline refines initial plans by incorporating feedback from a scene graph simulator, correcting infeasible actions and preventing planning failures.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240128220929621\" loading=\"lazy\" src=\"/posts/krishan-rana-sayplan-grounding-llm-for-scalable-task-planning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e","title":"Krishan Rana Sayplan Grounding Llm for Scalable Task Planning 2023"},{"content":"[TOC]\nTitle: Planning With Qualitative Constraints Pddl3 2022 Author: Luigi Bonassi et. al. Publish Year: Review Date: Sun, Jan 28, 2024 url: https://www.ijcai.org/proceedings/2022/0639.pdf Summary of paper The paper introduces a formalism to express trajectory constraints over actions in plans, complementing the state-trajectory constraints of PDDL3. This new formalism retains PDDL3\u0026rsquo;s temporal modal operators and adds two modalities. The authors then explore compilation-based methods for dealing with action-trajectory constraints in propositional planning, proposing a new, simple, and effective method. Experimental results demonstrate the utility of action-trajectory constraints for expressing control knowledge, showing significant performance improvements in classical planners when leveraging knowledge expressed through action constraints. Conversely, the same knowledge specified as state constraints and handled by two state-of-the-art systems yields less beneficial results.\nKeyword: action constraints\n","permalink":"https://sino-huang.github.io/posts/luigi-bonassi-planning-with-qualitative-constraints-pddl3-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Planning With Qualitative Constraints Pddl3 2022\u003c/li\u003e\n\u003cli\u003eAuthor: Luigi Bonassi et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 28, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.ijcai.org/proceedings/2022/0639.pdf\"\u003ehttps://www.ijcai.org/proceedings/2022/0639.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe paper introduces a formalism to express trajectory constraints over actions in plans, complementing the state-trajectory constraints of PDDL3. This new formalism retains PDDL3\u0026rsquo;s temporal modal operators and adds two modalities. The authors then explore compilation-based methods for dealing with action-trajectory constraints in propositional planning, proposing a new, simple, and effective method. Experimental results demonstrate the utility of action-trajectory constraints for expressing control knowledge, showing significant performance improvements in classical planners when leveraging knowledge expressed through action constraints. Conversely, the same knowledge specified as state constraints and handled by two state-of-the-art systems yields less beneficial results.\u003c/p\u003e","title":"Luigi Bonassi Planning With Qualitative Constraints Pddl3 2022"},{"content":"[TOC]\nTitle: Zero Shot Reward Specification via Grounded Natural Language Author: Parsa Mahnoudieh et. al. Publish Year: PMLR 2022 Review Date: Sun, Jan 28, 2024 url: Summary of paper Motivation reward signals in RL are expensive to design and often require access to the true state. common alternatives are usually demonstrations or goal images which can be label intensive on the other hand, text descriptions provide a general low-effect way of communicating. previous work rely on true state or labelled expert demonstration match, this work directly use CLIP to convert the observation to semantic embeddings Contribution Some key terms Difference\nimage based goal specification they are typicaly limited to a particular scene where semantic goal comprise multiple possible scene configuration NOTE: DIFFERENT MOTIVATION -\u0026gt; the generalisation ability for multitask reward functions or policies that take natural language as for goal descriptions they however relay on reward signals that have access to state of the system Scope of the study\nWe wish to have an agent that can learn purely from pixels, with no access to the underlying state of the environment at any point during learning or task execu- tion. Achieving this goal without access to an instrumented reward function has been exceedingly challenging. Limitation of image based reward\nTo be sure that the true goal is properly specified irrespective of the invariances of the model’s underlying perceptual representation, a user may have to provide a set of goal image examples that cover the variation of the target concept, potentially a very expensive undertaking to collect or generate. Large scale joint embedding models\nCLIP and XCLIP Method\nOne way to communicate text-based goal to a robot is by simply offering a description of the goal configuration in natural language and using the CLIP embedding dot product with an observed image to evaluate proximity to goal state. direct cosine similarity is not working for complex environment. a little object extraction and spatial relation text generator was created in order to create the (observation, text) pair Generic task described by text\ntext description is more generic / ambiguous compared to image-based reward Very clear and good problem setting using natural language\nIn our work we assume an agent only has access to a text description of the goal desired by the user and image observations from the environment. At no point during training or testing does the agent have access to demonstrations, goal images, or reward from the environment. The agent only has access to a reward model that takes images and goal text as input to provide progress towards goal text description with a reward score output. The reward is then used to teach the agent how to achieve the goal described by the text with online reinforcement learning. Given these assumptions we will now describe how we provide a zero-shot reward model by leveraging CLIP and how we learn a text conditioned policy with this model. Results Claim\nWe argue that existing (e.g., CLIP-like) models can be used to ground the what aspects of a goal quite effectively, including appropriate attribute and concept-level generalization, while a separate where/how module can ground spatial relation- ship aspects of goal configuration. Summary CLIP cosine similarity for cur state and goal description\n","permalink":"https://sino-huang.github.io/posts/parsa-mahmoudieh-zero-shot-reward-specification-via-grounded-natural-language-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Zero Shot Reward Specification via Grounded Natural Language\u003c/li\u003e\n\u003cli\u003eAuthor: Parsa Mahnoudieh et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: PMLR 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 28, 2024\u003c/li\u003e\n\u003cli\u003eurl:\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240128113716178\" loading=\"lazy\" src=\"/posts/parsa-mahmoudieh-zero-shot-reward-specification-via-grounded-natural-language-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ereward signals in RL are expensive to design and often require access to the true state.\u003c/li\u003e\n\u003cli\u003ecommon alternatives are usually demonstrations or goal images which can be label intensive\u003c/li\u003e\n\u003cli\u003eon the other hand, text descriptions provide a general low-effect way of communicating.\u003c/li\u003e\n\u003cli\u003eprevious work rely on true state or labelled expert demonstration match,\n\u003cul\u003e\n\u003cli\u003ethis work directly use CLIP to convert the observation to semantic embeddings\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDifference\u003c/strong\u003e\u003c/p\u003e","title":"Parsa Mahmoudieh Zero Shot Reward Specification via Grounded Natural Language 2022"},{"content":"[TOC]\nTitle: Robots That Ask for Help: Uncertainty Alignment for Large Language Model Planners Author: Allen Z. Ren et. al. Publish Year: 4 Sep 2023 Review Date: Fri, Jan 26, 2024 url: arXiv:2307.01928v2 Summary of paper Motivation LLMs have various capabilities but often make overly confident yet incorrect predictions. KNOWNO aims to measure and align this uncertainty, enabling LLM-based planners to recognize their limitations and request assistance when necessary. Contribution built on theory of conformal prediction Some key terms Ambiguity in NL\nMoreover, natural language instructions in real- world environments often contain a high degree of ambiguity inherently or unintentionally from humans, and confidently following an incorrectly constructed plan could lead to undesirable or even unsafe actions. e.g., ambiguous instructions matches multiple trajectories Previous works does not seek clarification does so via extensive prompting The issue of extensive prompting\ncareful prompt engineering Formalize the challenge\n(i) calibrated confidence: the robot should seek sufficient help to ensure a statistically guaranteed level of task success specified by the user, and\n(ii) minimal help: the robot should minimize the overall amount of help it seeks by narrowing down possible ambiguities in a task.\nCore method\nCP: conformal prediction\nuse CP to select options, which allows the robot to decide an action to execute or ask for help otherwise environment setting\nvarious types of potential ambiguities (e.g., based on spatial locations, numerical values, attributes of objects, and Winograd schemas). Winograd schemas solved by transformer already Calibrated confidence\nCalibrated confidence implies that these confidence scores accurately reflect the likelihood that a particular prediction is correct. robots that ask for help\nprediction set generation: use CP to choose a subset of candidate plans using LLM\u0026rsquo;s confidence in each prediction given the context. Goal uncertainty alignment\nin real world, language can be ambiguous so, balancing the confidence among multiple choices and ask for help if not confident is the key in this work Conformal Prediction\nwe have a set of plans (predefined) labeled as $y \\in Y$ . the conformal prediction will offer a subset of $Y$ such that they are confident enough that at least one of them is the correct plan based on the input $x$. since we are having a subset of plans, we need human experts to help to pick the right one. and the method in this work try to minimize the size of subset. $\\hat f (x)_{y}$ is the LLM confidence in each prediction $y$ given the context $x$, i.e., the LLM\u0026rsquo;s prior confidence. Multi-step uncertainty alignments\ni.i.d assumption for coverage guarantee is no longer valid Experiments introduce three settings based on different types of ambiguities in the user instruction:\n(1) Attribute (e.g., referring to the bowl with the word “receptacle”), (2) Numeric (e.g., under-specifying the number of blocks to be moved by saying “a few blocks”), and (3) Spatial (e.g., “put the yellow block next to the green bowl”, but the human has a preference over placing it at the front/back/left/right). Results Potential future work ask should predicate be in what kind of relationship in the action -\u0026gt; solve ambiguity\n","permalink":"https://sino-huang.github.io/posts/allen-z-ren-robots-that-ask-for-help-uncertainty-alignment-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Robots That Ask for Help:  Uncertainty Alignment for Large Language Model Planners\u003c/li\u003e\n\u003cli\u003eAuthor: Allen Z. Ren et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 4 Sep 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Jan 26, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2307.01928v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240127222901220\" loading=\"lazy\" src=\"/posts/allen-z-ren-robots-that-ask-for-help-uncertainty-alignment-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLLMs have various capabilities but often make overly confident yet  incorrect predictions. KNOWNO aims to measure and align this  uncertainty, enabling LLM-based planners to recognize their limitations  and request assistance when necessary.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ebuilt on theory of conformal prediction\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAmbiguity in NL\u003c/strong\u003e\u003c/p\u003e","title":"Allen Z Ren Robots That Ask for Help Uncertainty Alignment 2023"},{"content":"[TOC]\nTitle: RePlan: Robotic Replanning with Perception and Language Models Author: Marta Skreta et. al. Publish Year: 8 Jan 2024 Review Date: Thu, Jan 25, 2024 url: arXiv:2401.04157v1 Summary of paper Motivation However, the challenge remains that even with syntac- tically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects. Contribution Robotic Replanning with Perception and Language Models that enables real-time replanning capabilities for long-horizon tasks. Some key terms Address the challenge of multi-stage long-horizon tasks\ninheriting key ideas from recent progress in foundation models introduce REPLAN, an innovative zero-shot approach that harnesses LLMs at multiple levels by iterative re-prompting to serve as a reward generator for robot MPC. Structure\nPerceiver Models for High-Level Replanning: We utilize perceiver models to facilitate high-level replanning, enabling the system to adapt and adjust strategies as needed during task execution. Creation of Hierarchical Plans with Language Models: We employ language models to generate hierarchical plans, allowing for structured and organized task execution strategies. Verification of Outputs from Language Models: We implement mechanisms to verify the outputs generated by language models, ensuring the reliability and accuracy of the planned actions. Robot Behavior through Reward Generation: We influence robot behavior through reward generation mechanisms, incentivizing desirable actions and outcomes during task execution. Results similar to Eureka, the low level LLM planner will generate python reward function to give reward to the motion controller ","permalink":"https://sino-huang.github.io/posts/marta-skreta-replan-robotic-replanning-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: RePlan: Robotic Replanning with Perception and Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Marta Skreta et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 8 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jan 25, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.04157v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240126170742457\" loading=\"lazy\" src=\"/posts/marta-skreta-replan-robotic-replanning-2024/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHowever, the challenge remains that even with syntac- tically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRobotic Replanning with Perception and Language Models that enables \u003cstrong\u003ereal-time replanning\u003c/strong\u003e capabilities for long-horizon tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eAddress the challenge of multi-stage long-horizon tasks\u003c/strong\u003e\u003c/p\u003e","title":"Marta Skreta Replan Robotic Replanning 2024"},{"content":"[TOC]\nTitle: Secrets of RLHF in Large Language Models Part II: Reward Modelling Author: Binghai Wang et. al. Publish Year: 12 Jan 2024 Review Date: Wed, Jan 24, 2024 url: arXiv:2401.06080v2 Summary of paper Motivation a crucial technology for aligning language models with human values. Two main issues are tackled: (1) Incorrect and ambiguous preference pairs in the dataset hindering reward model accuracy, and (2) Difficulty in generalization for reward models trained on specific distributions. a method measuring preference strength within the data is proposed, utilizing a voting mechanism of multiple reward models. Novel techniques are introduced to mitigate the impact of incorrect preferences and leverage high-quality preference data. For the second issue, contrastive learning is introduced to enhance the reward models\u0026rsquo; ability to distinguish between chosen and rejected responses, improving generalization. Some key terms noisy data\npreference strength\nflip、margin、soft label for contrastive learning\n","permalink":"https://sino-huang.github.io/posts/binghai-wang-secrets-of-rlhf-reward-modelling-2024/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Secrets of RLHF in Large Language Models Part II: Reward Modelling\u003c/li\u003e\n\u003cli\u003eAuthor: Binghai Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 12 Jan 2024\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Jan 24, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2401.06080v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ea crucial technology for aligning language models with human values. Two main issues are tackled: (1) Incorrect and ambiguous preference pairs  in the dataset hindering reward model accuracy, and (2) Difficulty in  generalization for reward models trained on specific distributions.\u003c/li\u003e\n\u003cli\u003ea method measuring preference strength within the data is proposed,  utilizing a voting mechanism of multiple reward models. Novel techniques are introduced to mitigate the impact of incorrect preferences and  leverage high-quality preference data. For the second issue, contrastive learning is introduced to enhance the reward models\u0026rsquo; ability to  distinguish between chosen and rejected responses, improving  generalization.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003enoisy data\u003c/strong\u003e\u003c/p\u003e","title":"Binghai Wang Secrets of Rlhf Reward Modelling 2024"},{"content":"[TOC]\nTitle: Secrets of RLHF in Large Language Models Part1: PPO Author: Rui Zheng et. al. Publish Year: 18 Jul 2023 Review Date: Mon, Jan 22, 2024 url: arXiv:2307.04964v2 Summary of paper Motivation Current approaches involve creating reward models to measure human preferences, using Proximal Policy Optimization (PPO) to improve policy models, and enhancing step-by-step reasoning through process supervision. However, challenges in reward design, interaction with the environment, and agent training, along with the high trial and error costs of LLMs, make it difficult for researchers to develop technically aligned and safe LLMs. Contribution finding that LLMs trained using their algorithm can better understand query meanings and provide responses that resonate with people. A new PPO algorithm called PPO-max is introduced, which incorporates effective implementations and addresses stability issues. Some key terms RLHF limitation\nReinforcement Learning with Human Feedback (RLHF) has been identified as a valid approach, but it is challenging to train LLMs effectively with it due to issues like reward model quality and inefficient exploration in word space. RLHF in one paragraph\nReinforcement Learning (RL) is a promising solution, where agents learn human preferences through a reward model and undergo numerous trials under RLHF (Reinforcement Learning from Human Feedback). Several recent attempts have been made in this direction. The training process of an AI assistant involves three primary stages: supervised fine-tuning (SFT), reward model (RM) training, and proximal policy optimization (PPO). In the SFT phase, the model learns to engage in human-like dialogues by imitating examples provided in human-annotated dialogue data. The RM training phase focuses on training the reward model. In this stage, the model learns to evaluate and compare the preference of different responses based on human feedback. This feedback is crucial for guiding the model towards producing more desirable and aligned responses. The final phase is PPO, where the model is updated based on the feedback obtained from the trained reward model. PPO aims to discover an optimized policy by balancing exploration and exploitation, ensuring that the model\u0026rsquo;s responses align with human preferences. Helpfulness\nHelpfulness means the model should follow instructions; it must not only follow instructions but also deduce the intent from a few-shot prompt or another interpretable pattern. However, the intention behind a given prompt can often be unclear or ambiguous, which is why we depend on our annotators’ judgment, and their preference ratings constitute our primary metric. Alignment metrics\nAlignment is a vague and confusing topic that is intractable to evaluate. In the context of our paper, we endeavor to align models with human intentions. To be more specific, we define models to act as being helpful and harmless similar to [27] Results there will be decline in language understanding capabilities caused by PPO. Summary the paper summarized a bunch of implementation details for the PPO training\n","permalink":"https://sino-huang.github.io/posts/rui-zheng-secrets-of-rlhf-in-llm-part-ppo-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Secrets of RLHF in Large Language Models Part1: PPO\u003c/li\u003e\n\u003cli\u003eAuthor: Rui Zheng et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 18 Jul 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 22, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2307.04964v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCurrent approaches involve creating reward models to measure human  preferences, using Proximal Policy Optimization (PPO) to improve policy  models, and enhancing step-by-step reasoning through process  supervision. However, challenges in reward design, interaction with the  environment, and agent training, along with the high trial and error  costs of LLMs, make it difficult for researchers to develop technically  aligned and safe LLMs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003efinding that LLMs trained using their algorithm can better understand  query meanings and provide responses that resonate with people.\u003c/li\u003e\n\u003cli\u003eA new PPO algorithm called PPO-max is introduced, which incorporates effective implementations and addresses stability issues.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRLHF limitation\u003c/strong\u003e\u003c/p\u003e","title":"Rui Zheng Secrets of Rlhf in Llm Part Ppo 2023"},{"content":"[TOC]\nTitle: Zhiting Hu Language Agent and World Models 2023 Author: Publish Year: Review Date: Mon, Jan 22, 2024 url: arXiv:2312.05230v1 Summary of paper Motivation LAW proposes that world and agent models, which encompass beliefs about the world, anticipation of consequences, goals/rewards, and strategic planning, provide a better abstraction of reasoning. In this framework, language models play a crucial role as a backend Some key terms Limitation of Language\nAmbiguity and Imprecision: LLMs struggle with natural language\u0026rsquo;s ambiguity and imprecision because they lack the rich context that humans use when producing text. This context includes perceptual, social, and mental factors, as well as world commonsense. LLMs simulate surface text without understanding underlying context, leading to limitations in grounding on physical, social, and mental experiences. Inefficiency of Language: LLMs face challenges when using language as the primary medium for reasoning, especially in situations requiring nuanced descriptions. For instance, describing subtle differences between two objects might require lengthy text, while generating an image or using other sensory modalities can be more efficient for certain tasks, such as predicting fluid flow based on physical properties. Failure case\nSystem-II reasoning \u0026ndash; construct a mental model of the world\nfor robust reasoning during complex tasks (Tolman, 1948; Briscoe, 2011;) The paper outlines the background of these three models (language, agent, world models), then introduces the LAW (Language, Agent, World) framework for reasoning. It reviews recent studies related to each element in the framework and discusses the roadmap for addressing the challenges and advancing machine reasoning and planning. Two levels of agent model\nThere are two levels of agent models:\nLevel-0 Agent Models: These models represent how an embodied agent optimizes actions to maximize accumulated rewards based on belief and the physical constraints defined in its world model. They are used in embodied tasks, such as a robot searching for a cup. Level-1 Agent Models: These models are used in social reasoning tasks and involve reasoning about the behaviors of other agents. They encompass Theory of Mind, which means forming mental models of other agents and conducting causal reasoning to interpret their behaviors based on their mental states like goals and beliefs. LAW framework structure\nThe paper reviews recent works relevant to the LAW framework, highlighting several approaches:\nLMs as Both World and Agent Models (Reasoning-via-Planning, or RAP): LMs are repurposed to serve as world models by predicting future states in reasoning and as agent models by generating actions. This approach allows for reasoning traces that consist of interleaved states and reasoning steps, improving inference coherence. RAP incorporates Monte Carlo Tree Search (MCTS) for strategic exploration in reasoning. Probabilistic Programs: Probabilistic programs are used to construct world and agent models for physical and social reasoning. LMs are employed to translate natural language descriptions into probabilistic programs, serving as an interface between language and thought. LMs as the Planner in Agent Models: LMs are used to generate plans based on prompts specifying the state, task, and memory. Interactive planning paradigms provide feedback and reflection on past actions to adjust future plans. LMs can also simulate social behaviors in abstract environments, enhancing social reasoning. LMs as the Goal/Reward in Agent Models: LMs are considered for generating goals or rewards in agent models. They can translate language descriptions of intended tasks into goal and reward specifications, simplifying the process. LMs as the Belief in Agent Models: Although less explored, there is potential for using LMs to explicitly model belief representations in agent models, similar to their role as planners, goals, or rewards. Results However, the authors acknowledge certain limitations of the LAW framework:\nSymbolic Representations: The language model backend relies on symbolic representations in a discrete space. While there\u0026rsquo;s potential to augment this space with continuous latent spaces from other modalities, it remains unclear whether a single continuous latent space can achieve similar capacity as symbolic representations. Incomplete Modeling: The current world and agent modeling may not capture all knowledge about the world and agents. For example, it assumes that agent behaviors are primarily driven by goals or rewards, overlooking other potential factors like social norms. Transformer Architecture Limits: The paper does not delve into the inherent limits of Transformer architectures, which are foundational to many language models. Further research into understanding the learning mechanisms of Transformers may complement the development of machine reasoning. Overall, while the LAW framework presents a promising direction for advancing machine reasoning, it is essential to address these limitations and continue exploring ways to enhance its capabilities.\nSummary This is a discussion paper\n","permalink":"https://sino-huang.github.io/posts/zhiting-hu-language-agent-and-world-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Zhiting Hu Language Agent and World Models 2023\u003c/li\u003e\n\u003cli\u003eAuthor:\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 22, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2312.05230v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240122201639803\" loading=\"lazy\" src=\"/posts/zhiting-hu-language-agent-and-world-models-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLAW proposes that world and agent models, which encompass beliefs about  the world, anticipation of consequences, goals/rewards, and strategic  planning, provide a better abstraction of reasoning. In this framework,  language models play a crucial role as a backend\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation of Language\u003c/strong\u003e\u003c/p\u003e","title":"Zhiting Hu Language Agent and World Models 2023"},{"content":"[TOC]\nTitle: Dynamic Planning With a LLM Author: Gautier Dagan et. al. Publish Year: 11 Aug 2023 Review Date: Sun, Jan 21, 2024 url: arXiv:2308.06391v1 Summary of paper Motivation Traditional symbolic planners can find optimal solutions quickly but need complete and accurate problem representations. In contrast, LLMs can handle noisy data and uncertainty but struggle with planning tasks. The LLM-DP framework combines LLMs and traditional planners to solve embodied tasks efficiently. Traditional Planner need maximal information Some key terms Hallucination\nHowever, employing LLMs in em- bodied agents, which interact with dynamic envi- ronments, presents substantial challenges. LLMs tend to generate incorrect or spurious information, a phenomenon known as hallucination, and their performance is brittle to the phrasing of prompts (Ji et al., 2022). Convert to PDDL\nPrevious work by (Liu et al., 2023) has shown that LLMs can generate valid problem files in the Planning Domain Definition Language (PDDL ) for many simple examples. LLM Dynamic Planner Overview\na neuro-symbolic framework that integrates an LLM with a symbolic planner to solve embodied tasks.\nwe use a set of three in-context examples that are fixed for the entire evaluation duration. We use the OpenAI gpt-3.5-turbo-0613 LLM model with a temperature of 0 in all our LLM-DP experiments.\nProblem PDDL generation\nLLM-DP uses stored observations W, beliefs B and an LLM to construct different planning problem files in PDDL Finally, we convert each likely world state to lists of predicates to interface with the PDDL planner. The agent uses a Plan Generator (PG) to solve each problem and obtain a plan. Example prompt\nResults We contrast the LLM-DP approach with ReAct (LLM-only baseline) from the original implemen- tation by Yao et al. (2023). Since we use a differ- ent backbone LLM model (gpt-3.5-turbo rather than text-davinci-002) than the ReAct base- line for cost purposes, we also reproduce their results using gpt-3.5-turbo and adapt the ReAct prompts to a chat format.\nAs shown in Table 1, LLM-DP solves Alfworld almost perfectly (96%) compared to our baseline reproduction of ReAct (53%). The LLM-DP can translate the task description into an executable PDDL goal 97% of the time, but sampling reduces the accuracy further when it fails to select a valid set of possible world states – for instance, by sam- pling states where the goal is already satisfied.\n","permalink":"https://sino-huang.github.io/posts/gautier-dagan-dynamic-planning-with-a-llm-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Dynamic Planning With a LLM\u003c/li\u003e\n\u003cli\u003eAuthor: Gautier Dagan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 11 Aug 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 21, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2308.06391v1\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240122133428294\" loading=\"lazy\" src=\"/posts/gautier-dagan-dynamic-planning-with-a-llm-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTraditional symbolic planners can find optimal solutions quickly but  need complete and accurate problem representations. In contrast, LLMs  can handle noisy data and uncertainty but struggle with planning tasks.  The LLM-DP framework combines LLMs and traditional planners to solve  embodied tasks efficiently.\u003c/li\u003e\n\u003cli\u003eTraditional Planner need maximal information\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHallucination\u003c/strong\u003e\u003c/p\u003e","title":"Gautier Dagan Dynamic Planning With a Llm 2023"},{"content":"[TOC]\nTitle: Conformal Temporal Logic Planning Using Llm 2023 Author: Jun Wang et. al. Publish Year: 19 Dec, 2023 Review Date: Sun, Jan 21, 2024 url: arXiv:2309.10092v2 Summary of paper Motivation Unlike previous methods that focus on low-level system configurations, this approach focuses on NL-based atomic propositions. now the LTL tasks are defined over NL-based atomic propositions Robots are required to perform high-level sub tasks specified in natural language. To formally define the overarching mission, they leverage LTL defined over atomic predicates modelling these NL-based sub-tasks. Contribution To address the challenge of ensuring the correctness of robot plans with respect to these LTL-encoded tasks, the authors propose HERACLEs, a hierarchical conformal natural language planner. HERACLEs employs automata theory to determine the next NL-specified sub-tasks for mission progress, employs Large Language Models to design robot plans to fulfill these sub-tasks, and uses conformal prediction to assess the probabilistic correctness of the plans, deciding whether external assistance is needed. The paper provides theoretical probabilistic guarantees for mission satisfaction and presents extensive comparative experiments on mobile manipulation tasks. Some key terms Limitation for previous work\nmotion planning can generate paths satisfying LTL tasks\nbut LTL formula require experts to create Natural Language (NL) has also been explored as a more user-friendly means to specify robot missions; however, NL-specified tasks may be characterized by ambiguity.\nalgorithms for NL based task cannot guarantee correctness Specifying robot tasks that combines both LTL and NL\ntasked with performing multiple high-level semantic sub-tasks (e.g., ’deliver a bottle of water to the table’) within their environments in a temporal and logical order. now the atomic predicate bind with natural language described task, which are longer and more vague. Structure\nLTL planner decide which sub tasks to do next LLM planner determine how to transit from sub task t to sub task t+1 For instance, an LLM response to the task ‘bring me a bottle of water’ may be “I need to go to the store and purchase a bottle of water’. Summarize\nthey use LLM as sub task planner Potential future work we may want to use their environment as their environment contains complex temporal logic. ","permalink":"https://sino-huang.github.io/posts/jun-wang-conformal-temporal-logic-planning-using-llm-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Conformal Temporal Logic Planning Using Llm 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Jun Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 19 Dec, 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jan 21, 2024\u003c/li\u003e\n\u003cli\u003eurl: arXiv:2309.10092v2\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240121004104715\" loading=\"lazy\" src=\"/posts/jun-wang-conformal-temporal-logic-planning-using-llm-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUnlike previous methods that focus on low-level system configurations, this approach focuses on NL-based atomic propositions.\n\u003cul\u003e\n\u003cli\u003enow the LTL tasks are defined over NL-based atomic propositions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRobots are required to perform high-level sub tasks specified in natural language.\n\u003cul\u003e\n\u003cli\u003eTo formally define the overarching mission, they leverage LTL defined over atomic predicates modelling these NL-based sub-tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTo address the challenge of ensuring the correctness of robot plans with respect to these LTL-encoded tasks, the authors propose HERACLEs, a  hierarchical conformal natural language planner. HERACLEs employs  automata theory to determine the next NL-specified sub-tasks for mission progress, employs Large Language Models to design robot plans to  fulfill these sub-tasks, and uses conformal prediction to assess the  probabilistic correctness of the plans, deciding whether external  assistance is needed. The paper provides theoretical probabilistic  guarantees for mission satisfaction and presents extensive comparative  experiments on mobile manipulation tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation for previous work\u003c/strong\u003e\u003c/p\u003e","title":"Jun Wang Conformal Temporal Logic Planning Using Llm 2023"},{"content":"[TOC]\nTitle: Gerevini Plan Constraints and Preferences in PDDL3 Author: Alfonso Gerevini, Derek Long Publish Year: 2005 Review Date: Thu, Jan 11, 2024 url: http://www.cs.yale.edu/~dvm/papers/pddl-ipc5.pdf Summary of paper Motivation the notion of plan quality in automated planning is a practically very important issue. it is important to generate plans of good or optimal quality and we need to express the plan quality the proposed extended language allows us to express strong and soft constraints on plan trajectories i.e., constraints over possible actions in the plan and intermediate states reached by the plan as well as strong and soft problem goals. Some key terms some scenarios\nwhen we have soft constraints and goals, it can be useful to give different priorities to them. ","permalink":"https://sino-huang.github.io/posts/gerevini-plan-constraints-and-preferences-in-pddl3-2005/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Gerevini Plan Constraints and Preferences in PDDL3\u003c/li\u003e\n\u003cli\u003eAuthor: Alfonso Gerevini, Derek Long\u003c/li\u003e\n\u003cli\u003ePublish Year: 2005\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jan 11, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"http://www.cs.yale.edu/~dvm/papers/pddl-ipc5.pdf\"\u003ehttp://www.cs.yale.edu/~dvm/papers/pddl-ipc5.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240111195650738\" loading=\"lazy\" src=\"/posts/gerevini-plan-constraints-and-preferences-in-pddl3-2005/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe notion of plan quality in automated planning is a practically very important issue.\u003c/li\u003e\n\u003cli\u003eit is important to generate plans of good or optimal quality and we need to express the plan quality\u003c/li\u003e\n\u003cli\u003ethe proposed extended language allows us to express strong and soft constraints on plan trajectories\n\u003cul\u003e\n\u003cli\u003ei.e., constraints over possible actions in the plan and intermediate states reached by the plan\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eas well as strong and soft problem goals.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003esome scenarios\u003c/strong\u003e\u003c/p\u003e","title":"Gerevini Plan Constraints and Preferences in Pddl3 2005"},{"content":"[TOC]\nTitle: Planning With Perspectives \u0026ndash; Using Decomposing Epistemic Planning using Functional STRIPS Author: Guang Hu, Nir Lipovetzky Publish Year: 2022 Review Date: Thu, Jan 11, 2024 url: https://nirlipo.github.io/publication/hu-2022-planning/ Summary of paper Motivation we present a novel approach to epistemic planning called planning with perspectives (PWP) that is both more expressive and computationally more efficient than existing state of the art epistemic planning tools. Contribution in this paper, we decompose epistemic planning by delegating reasoning about epistemic formulae to an external solver, i.e., Functional STRIPS F-STRIPS supports the user of external, black-box functions within action models. Building on recent work that demonstrates the relationship between what an agent \u0026lsquo;sees\u0026rsquo; and what it knows, we define the perspective of each agent using an external function, and build a solver for epistemic logic around this. Some key terms external functions (black-box)\nthey are arbitrary functions implemented in a programming language that can be called during planning. epistemic planning\nin many scenarios, autonomous agents need to plan about the knowledge or beliefs of other agents in the environment. epistemic planning is concerned about action theories may allow modellers reasoning not only about variables representing the state of the world, but also the beliefs and knowledge that other agents have about those variables. F-STRIPS external functions representation\nthey start with the @ character. @validMove is an external function that checks whether the move is valid, encoding Pacman\u0026rsquo;s Maze implicitly. @move_ghost encodes deterministic movement strategy followed by all ghosts, a strategy that depends on Pacman\u0026rsquo;s Manhattan distance. it looks like the effect of the external function will not be recorded as fluents. ","permalink":"https://sino-huang.github.io/posts/nir-lipo-planning-with-perspectives-using-functional-strips-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Planning With Perspectives \u0026ndash; Using Decomposing Epistemic Planning using Functional STRIPS\u003c/li\u003e\n\u003cli\u003eAuthor: Guang Hu, Nir Lipovetzky\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jan 11, 2024\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://nirlipo.github.io/publication/hu-2022-planning/\"\u003ehttps://nirlipo.github.io/publication/hu-2022-planning/\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20240111194239165\" loading=\"lazy\" src=\"/posts/nir-lipo-planning-with-perspectives-using-functional-strips-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe present a novel approach to epistemic planning called planning with perspectives (PWP) that is both more expressive and computationally more efficient than existing state of the art epistemic planning tools.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, we decompose epistemic planning by delegating reasoning about epistemic formulae to an external solver, i.e., Functional STRIPS\u003c/li\u003e\n\u003cli\u003eF-STRIPS supports the user of external, black-box functions within action models.\u003c/li\u003e\n\u003cli\u003eBuilding on recent work that demonstrates the relationship between what an agent \u0026lsquo;sees\u0026rsquo; and what it knows, we define the perspective of each agent using an external function, and build a solver for epistemic logic around this.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eexternal functions (black-box)\u003c/strong\u003e\u003c/p\u003e","title":"Nir Lipo Planning With Perspectives Using Functional Strips 2022"},{"content":"[TOC]\nTitle: Theory Alignment via a Classical Encoding of Regular Bismulation 2022 Author: Alex Coulter et. al. Publish Year: KEPS 2022 Review Date: Wed, Nov 29, 2023 url: https://icaps22.icaps-conference.org/workshops/KEPS/KEPS-22_paper_7781.pdf Summary of paper Motivation the main question we seek to answer is how we can test if two models align (where the fluents and action implementations may differ), and if not, where that misalignment occurs. Contribution the work is built on a foundation of regular bisimulation found that the proposed alignment was not only viable, with many submissions having \u0026ldquo;solutions\u0026rdquo; to the merged model showing where a modelling error occurs, but several cases demonstrated errors with the submitted domains that were subtle and detected only by this added approach. Some key terms Bisimulation\nloosely defined, a bisimulation is a correspondence between two transition system such taht labeled transitions between two systems coincide: if two nodes (one from each system) coincide, then the reachable nodes in each system also coincide (following the labeled transitions) regular bisimulation the labels of the transitions are presumed to be the same, as well as the reachable state space (predicates?) Merge\nwe merge them. This merged domain retains the original types, action names/parameters, objects and constants (assumed to be the equivalent in both of the original models) fluents and initial states are merged finally, failure actions are introduced that represent a potential misalignment between the two models. as long as one of these failure action can be executed, the two theories do not align (measuring the \u0026lsquo;false negative\u0026rsquo;) Alignment specification Assumptions\ntypes: we assume that the typing for each of the two models are precisely the same. This is required in order to adequately handle the action parameter space. objects / constants: Similarly, we assume that the objects and constants, including the types they adopt, are precisely the same between the two models. Moreover, we assume that both of the models use these objects in the same way. Action Names \u0026amp; Parameters: the space of full ground action specification must coincide. Thus, making sure the same action names and :paramaters definition is used in both of the models under consideration. Initial States We assume that each model captures the same initial state semantically. meaning, same task Fluents: We do not assume that the fluents are aligned. meaning: we can have different predicate setting and thus different action precon and effect details Action Precondition and Effect: we do not assume there to be any syntactic equivalence between the action precondition / effect of the two models. this allows us to compare widely different encodings of the same real world system. given that the fluents are not assumed to be coincide, the effective application of the action will be analyzed through the alignment process Encoding\nevery shared action has a pair of \u0026ldquo;failure actions\u0026rdquo; introduced that correspond to the situation where the action can be executed in the one model, but not the other. during the process, rename all of the predicates/fluents throughout both model so that they are unique: we prepend a prefix to every occurrence of a fluent (e.g., adding domain1_ or domain2_ to the start of every fluent name) failure action can only be executed in one model Failed Goal\nif solution exists, it means that we can at least achieve some states in the precondition of some actions the only way to achieve the goal is for one of the fail actions to be executed, if this happens, it means there will be an action that give different semantic effects and eventually leads to contradiction in precondition section. Error Example\nthis is an erroneous action definition as it did not set not (off ?l). (forgetting to delete a fluent.) The found plan for the merged domain is then (turnon light2) and then (fail_turnon2 light2). this means the action was allowed to execute in the second domain, but not the first it is only through the analysis we present here that the misalignment between the two models is detected. Diagnose a misalignment between models\nFailed action precondition. The most common source of error found is an error in precondition of the failed action. Thus, the first step in analyzing a plan is to contrast the precondition of act in both of the models, and see if there is mismatch in what is implemented Previous Action Effect: If the two models capture act similarly, then the next most common issue leading to misalignment is a previous action in the plan. Typically, candidate solutions that lead to a failure are short, and so the space of actions that must be considered is limited. it would be an action that directly influences the failed action act (through its preconditions) Initial State: the most rare source of errors is a misalignment in the initial state implementation. this source of error can be seen as analogous to the \u0026ldquo;Previous Action Effect\u0026rdquo; Errors, when one views the initial state as the effects of a single action at the start of a plan. Precondition Satisfaction\nApplicability, by its very definition, is concerned with precondition satisfaction How planner is involved in this work\nto decide if they are indeed regular bisimilar if they are not, a candidate explanation as to why in the form of a sequence of actions if (fail_turnon2 light2) is in the plan, then check the correctness of turnon2 Evaluation there is an assignment task where the implementation involved devising (1) the fluents for the domain; (2) the preconditions and effects for 4 actions (move, pickup, drop, unlock; (3) the initial and goal states for 3 described problems Other grading indicators\nother planning oriented functionality included running student domain/problem files to generate plans validating student-found plans with the reference model validating reference plans with the student model align to a model which include the most common error in the end, we can find the following errors plan-based errors validation errors (the plan from one model should be executable and valid in the other model) alignment error (the aligning method in this paper) Good things about the paper (one paragraph) There is a rich theory behind what is being analyzed \u0026ndash; plans in the merged model effectively provide us with a proof of contradiction to the two models being a regular bismulation ([Milner 1990](Milner, R. 1990. Operational and Algebraic Semantics of Concurrent Processes. In van Leeuwen, J., ed., Handbook of Theoretical Computer Science, Volume B: Formal Models and Semantics, 1201–1242. Elsevier and MIT Press)) Major comments They discuss the (mainly qualitative) results in Section 4. ","permalink":"https://sino-huang.github.io/posts/alex_coulter-theory-alignment-via-a-classical-encoding-of-regular-bismulation-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Theory Alignment via a Classical Encoding of Regular Bismulation 2022\u003c/li\u003e\n\u003cli\u003eAuthor: Alex Coulter et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: KEPS 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Nov 29, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://icaps22.icaps-conference.org/workshops/KEPS/KEPS-22_paper_7781.pdf\"\u003ehttps://icaps22.icaps-conference.org/workshops/KEPS/KEPS-22_paper_7781.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20231129172526275\" loading=\"lazy\" src=\"/posts/alex_coulter-theory-alignment-via-a-classical-encoding-of-regular-bismulation-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe main question we seek to answer is how we can test if two models align (where the fluents and action implementations may differ), and if not, where that misalignment occurs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe work is built on a foundation of regular bisimulation\u003c/li\u003e\n\u003cli\u003efound that the proposed alignment was not only viable, with many submissions having \u0026ldquo;solutions\u0026rdquo; to the merged model showing where a modelling error occurs, but several cases demonstrated errors with the submitted domains that were subtle and detected only by this added approach.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eBisimulation\u003c/strong\u003e\u003c/p\u003e","title":"Alex_coulter Theory Alignment via a Classical Encoding of Regular Bismulation 2022"},{"content":"[TOC]\nTitle: Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains Author: Pascal Bercher et. al. Publish Year: 2023 Review Date: Mon, Nov 13, 2023 url: https://bercher.net/publications/2023/Sleath2023PossibleModelingErrors.pdf Summary of paper Contribution the author provided a compilation of potential modelling errors the author supply a public repository of 56 (flawed) benchmark domains conducted an evaluation of well-known AI planning tools for their ability to diagnose those errors, showing that not a single tool is able to spot all errors, with no tool being strictly stronger than another. Some key terms list of errors\nsyntax errors: these are actual errors, but often not spotted by parsers and semantic errors: these would be warnings as they indicate a potential modelling error, to be checked by the domain modeller. Syntax Errors\ninconsistent parameter use the modeller attempts to use a predicate with either a parameter of an incompatible type or a different number of parameters than it was defined with. undefined entities the modeller attempts to use an undefined predicate or type General Syntax error typo, forgets to write \u0026ldquo;:parameters\u0026rdquo;, add extra parenthesis, forget question mark in front of a variable name to differentiate it from a constant. duplicated definition the modeller repeats some definitions cyclic type declaration when two types are directly or indirectly declared to be subtypes of each other, forming a cycle undeclared parameters the modeller tries to use a variable in the definition of a task that wasn\u0026rsquo;t declared as a parameter Semantic Errors\ncomplementary effect there is an intersection between the ground negated and positive effects of a task Unsatisfied preconditions some action\u0026rsquo;s preconditions can never be fulfilled. This may be due to syntactically complementary preconditions (with identical predicates), or simply since in the given planning problem the preconditions cannot be made true. this require complex reasoning, which is as hard as planning Unused elements The modeller defines a type or predicate or a parameter in a task that is not used Redundant effects Some effect will never change the state to which the respective action is applied. There are two possibilities how this can happen: The simplest case is if some effect also occurs as a precondition (with identical parameters). The redundancy can however also be problem-dependent immutable predicate A predicate is defined which never occurs in task effects. This means the state of that predicate is constant. Good things about the paper (one paragraph) The semantic error part is not what I expected.\nhttps://github.com/ProfDrChaos/flawedPlanningModels\n","permalink":"https://sino-huang.github.io/posts/pascal-bercher-detecting-ai-planning-modelling-mistakes-potential-errors-and-benchmark-domains-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains\u003c/li\u003e\n\u003cli\u003eAuthor: Pascal Bercher et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Nov 13, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://bercher.net/publications/2023/Sleath2023PossibleModelingErrors.pdf\"\u003ehttps://bercher.net/publications/2023/Sleath2023PossibleModelingErrors.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author provided a compilation of potential modelling errors\u003c/li\u003e\n\u003cli\u003ethe author supply a public repository of 56 (flawed) benchmark domains\u003c/li\u003e\n\u003cli\u003econducted an evaluation of well-known AI planning tools for their ability to diagnose those errors, showing that not a single tool is able to spot all errors, with no tool being strictly stronger than another.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elist of errors\u003c/strong\u003e\u003c/p\u003e","title":"Pascal Bercher Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains 2023"},{"content":"[TOC]\nTitle: Eureka Human Level Reward Design via Coding Large Language Models 2023 Author: Yecheng Jason Ma et. al. Publish Year: 19 Oct 2023 Review Date: Fri, Oct 27, 2023 url: https://arxiv.org/pdf/2310.12931.pdf Summary of paper Motivation harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem. we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning. Contribution Eureka generate reward functions that outperform expert human-engineered rewards. the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) Some key terms given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling. As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice reward design problem\nCurriculum learning\nthe paper mentioned that they used curriculum learning to train the model. here are the key aspects of Curriculum learning: gradual complexity increase Curriculum learning starts by training models on easier or simpler tasks before gradually increasing the complexity of the tasks improved learning efficiency and generalisation Structured learning path the curriculum provides a structured learning path, allowing the model to build upon previously learned concepts implementation implementing curriculum learning may involve designing a curriculum, which is a sequence of tasks of increasing complexity relation to other concepts curriculum learning shares similarities with concepts like transfer learning and multi-task learning, but with a focus on the structured, gradual increase in task complexity. Some insights for the practical implementation How does the model ensure the (semantic) correctness of the reward function ?\nEUREKA requires the environment specification to provided to the LLM. they directly feeding the raw environment code as context. reason: the environment source code typically reveals what the environment semantically entails and which variables can and should be used to compose a reward function for the specified task they used evolutionary search to address the execution error and sub-optimality challenges. a very big assumption behind the success of this method evolutionary search, i.e., refine the reward in the next iteration based on the performance in the current iteration. simply specifying the mutation operator as a text prompt that suggests a few general ways to modify an existing reward code based on a textual summary of policy training textual summary of policy training : after the policy training, we evaluate the policy performance basically it is a loop to refine the reward function. How does the model avoid the computer vision part ?\nthey directly access the source code to obtain \u0026ldquo;processed\u0026rdquo; observations. https://github.com/eureka-research/Eureka/blob/main/eureka/envs/isaac/shadow_hand_obs.py some important parameters are explicitly stated (e.g., position, velocity, angle velocity) maybe we shall also do this to avoid multimodal and CV part. Minor comments Potential future work having LLMs to generate reward function directly to train agents is something like replacing human experts with LLMs to design reward functions for learning agents.\nThe pipeline of this work is as follows:\nthe human users gave the task descriptions -\u0026gt; the LLM convert the descriptions into a reward function -\u0026gt; the reward function helps to train the agent that can accomplish the task -\u0026gt; the human users can give further requirements in the next iteration -\u0026gt; the LLM will further adjust the reward function to fit the users\u0026rsquo; need. Hybrid model Nir suggested that we can have a hybrid model such that conditions and effects expressed partially in predicate logic, and partially specified through imperative programming languages\nI think the it really depends on the type of the tasks\nif the task is about planning or scheduling, e.g., Sudoku, then imperative programming has nothing to do with this if the task is about low-level controls that have no explicit discrete procedures, then defining a reward function (this work) is suitable if the task description explicitly contains the steps, then converting it to imperative programming language is suitable. so it really depends on what is the task description is\ne.g., cooking task provided with steps info -\u0026gt; imperative programming e.g., \u0026ldquo;your task is to stack block A on top of B\u0026rdquo; -\u0026gt; predicate logic as conditions and effects so the hybrid model is somehow like a big model containing multiple specialised models that handles various types of tasks.\nImperative programming version of the action definition both reward function generation for actions or direct imperative function for actions can be used as auxiliary information to tune the PDDL action definition\nReward function example from GPT4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def reward_function(state, action): reward = 0 # Assuming `is_on_ladder` and `is_moving_down` are functions that # determine whether the agent is on a ladder and moving down, respectively. if is_on_ladder(state) and is_moving_down(action): reward += 1 # Give a positive reward for moving down a ladder # Optionally, penalize the agent for not moving down a ladder while on it elif is_on_ladder(state) and not is_moving_down(action): reward -= 1 # Optionally, penalize or reward other behaviors # ... return reward Imperative function example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class Environment: def __init__(self): self.agent_position = (0, 0) # Assume the agent starts at the top-left corner self.ladder_position = (5, 5) # Assume there\u0026#39;s a ladder at position (5, 5) def get_agent_position(self): return self.agent_position def move_agent(self, direction): x, y = self.agent_position if direction == \u0026#34;down\u0026#34; and self.is_ladder_below(): y += 1 elif direction == \u0026#34;left\u0026#34;: x -= 1 elif direction == \u0026#34;right\u0026#34;: x += 1 elif direction == \u0026#34;up\u0026#34;: y -= 1 self.agent_position = (x, y) def is_ladder_below(self): x, y = self.agent_position return (x, y + 1) == self.ladder_position def is_on_ladder(self): return self.agent_position == self.ladder_position def climb_down_ladder(env): while not env.is_on_ladder(): # Move towards the ladder agent_position = env.get_agent_position() ladder_position = env.ladder_position if agent_position[0] \u0026lt; ladder_position[0]: env.move_agent(\u0026#34;right\u0026#34;) elif agent_position[0] \u0026gt; ladder_position[0]: env.move_agent(\u0026#34;left\u0026#34;) elif agent_position[1] \u0026lt; ladder_position[1]: env.move_agent(\u0026#34;down\u0026#34;) # Now on the ladder, climb down for _ in range(5): # Assume ladder is 5 cells tall env.move_agent(\u0026#34;down\u0026#34;) def check_climb_down_complete(env): agent_position = env.get_agent_position() ladder_position = env.ladder_position # Check if the agent\u0026#39;s y-coordinate is the same or below the ladder\u0026#39;s bottom y-coordinate if agent_position[1] \u0026gt;= ladder_position[1] and env.is_on_ladder(): return True # Climbing down the ladder is complete else: return False # Climbing down the ladder is not complete # Usage: env = Environment() climb_down_ladder(env) both reward and imperative action function contains state checking (i.e., is_on_ladder and is_moving_down etc. ) the imperative programming version of the \u0026ldquo;climb_down_ladder\u0026rdquo; action contains a while loop that controls the agent to move towards the ladder before climbing down. This is different from the PDDL version action definition, where is_on_ladder is the precondition of the action. ","permalink":"https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Eureka Human Level Reward Design via Coding Large Language Models 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Yecheng Jason Ma et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 19 Oct 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Oct 27, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2310.12931.pdf\"\u003ehttps://arxiv.org/pdf/2310.12931.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20231027164539472\" loading=\"lazy\" src=\"/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eharnessing LLMs to learn complex low-level manipulation tasks, remains an open problem.\u003c/li\u003e\n\u003cli\u003ewe bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEureka generate reward functions that outperform expert human-engineered rewards.\u003c/li\u003e\n\u003cli\u003ethe generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF)\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20231030132136067\" loading=\"lazy\" src=\"/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030132136067.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003egiven detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling.\u003c/li\u003e\n\u003cli\u003eAs many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ereward design problem\u003c/strong\u003e\u003c/p\u003e","title":"Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023"},{"content":"[TOC]\nTitle: Evaluating Large Language Models Trained on Code Author: Mark Chen et. al. OPENAI Publish Year: 14 Jul 2021 Review Date: Mon, Oct 16, 2023 url: https://arxiv.org/pdf/2107.03374.pdf Summary of paper Motivation it is the research paper behind Github Copilot tech more recently, language models have also fueled progress towards the longstanding challenge of program synthesis. Contribution we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. limitation difficulty with docstrings describing long chain of operations and with binding operations to variables. Some key terms HumanEval\na new evaluation set we release to measure functional correctness for synthesizing programs from docstrings. preliminary test\nour early investigation of GPT-3 revealed that it could generate simple program from Python docstring. thus we hypothesise that a specialised GPT model, could excel at a variety of coding tasks. evaluation framework\ngenerative models for code are predominantly benchmarked by matching samples against reference solution, where the match can be exact or fuzzy (as in BLEU score) however, Ren et. al. (2020) finds that BLEU has problem capturing semantic features specific to code, and suggests several semantic modification to the score. More fundamentally, match-based metrics are unable to account for the large and complex space of programs functionally equivalent to a reference solution. functional correctness\nthis is a more suitable evaluation metric compared to match-based one\nwhere a sample is considered correct if it passes a set of unit tests\nMethods observation\nthey found that surprisingly, they did not observe improvements when starting from a pre-trained language model, possible because the fine-tuning dataset is so large. Nevertheless, models fine-tuned from GPT converge more quickly. supervised fine-tuning\nmultiple samples generation and ranking\n**back-translation to pick the sample **\nissue: this heuristic appears to overfit quickly. ","permalink":"https://sino-huang.github.io/posts/mark-chen-evaluating-large-language-models-trained-on-code-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Evaluating Large Language Models Trained on Code\u003c/li\u003e\n\u003cli\u003eAuthor: Mark Chen et. al. OPENAI\u003c/li\u003e\n\u003cli\u003ePublish Year: 14 Jul 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Oct 16, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2107.03374.pdf\"\u003ehttps://arxiv.org/pdf/2107.03374.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eit is the research paper behind Github Copilot tech\u003c/li\u003e\n\u003cli\u003emore recently, language models have also fueled progress towards the longstanding challenge of program synthesis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"limitation\"\u003elimitation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003edifficulty with docstrings describing long chain of operations and with binding operations to variables.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHumanEval\u003c/strong\u003e\u003c/p\u003e","title":"Mark Chen Evaluating Large Language Models Trained on Code 2021"},{"content":"[TOC]\nTitle: Code Llama Open Foundation Model for Code Author: Baptiste Roziere et. al. META AI Publish Year: 2023 Review Date: Mon, Oct 16, 2023 url: https://scontent.fmel13-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107\u0026ccb=1-7\u0026_nc_sid=3c67a6\u0026_nc_ohc=Hcg6QsYJx1wAX_okEZO\u0026_nc_ht=scontent.fmel13-1.fna\u0026oh=00_AfAYtfHJfYeomAQWiMUTRo96iP8d4sZrlIfD_KAeYlYaDQ\u0026oe=6531E8CF Summary of paper Motivation CODE Llama, support for large input contexts, and zero-shot instruction following ability for programming tasks. Contribution CODE llama reaches SOTA performance among open models on several code benchmarks Some key terms By training on domain-specific datasets, LLM have proved effective more broadly on applications that require advanced natural language understanding.\nA prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation\ncode-training from foundation models\nOur comparison (Section 3.4.1) shows that initializing our model with Llama 2 outperforms the same architecture trained on code only for a given budget. infilling\ncode infilling is the task of predicting the missing part of a program given a surrounding context. Applications include code completion at the cursor\u0026rsquo;s position in code IDEs, type inference and generation of in-code documentation. instruction fine-tuning\nDataset\nCODE LLAMA is trained predominantly on a near-deduplicated dataset of publicly available code. we also source 8% of our sample data from natural language dataset related to code. Preliminary experiments suggested that adding batches sampled from our natural language dataset improves the performance of our models on MBPP. prefix-suffix-middle (PSM)\nessentially, fill-in-the-middle(FIM) is implemented such that, with a simple modification to training data and without changing the model architecture, causal docoder-based autoregressive (AR) language models can learn infilling without compromising their normal left-to-right generative capability. self-instruct\ninstead of human feedback, we use execution feedback to select data to train our instruct model Major comments NO information about back-translation objective in this paper.\n","permalink":"https://sino-huang.github.io/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Code Llama Open Foundation Model for Code\u003c/li\u003e\n\u003cli\u003eAuthor: Baptiste Roziere et. al. META AI\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Oct 16, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://scontent.fmel13-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107\u0026amp;ccb=1-7\u0026amp;_nc_sid=3c67a6\u0026amp;_nc_ohc=Hcg6QsYJx1wAX_okEZO\u0026amp;_nc_ht=scontent.fmel13-1.fna\u0026amp;oh=00_AfAYtfHJfYeomAQWiMUTRo96iP8d4sZrlIfD_KAeYlYaDQ\u0026amp;oe=6531E8CF\"\u003ehttps://scontent.fmel13-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107\u0026ccb=1-7\u0026_nc_sid=3c67a6\u0026_nc_ohc=Hcg6QsYJx1wAX_okEZO\u0026_nc_ht=scontent.fmel13-1.fna\u0026oh=00_AfAYtfHJfYeomAQWiMUTRo96iP8d4sZrlIfD_KAeYlYaDQ\u0026oe=6531E8CF\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20231016025929381\" loading=\"lazy\" src=\"/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCODE Llama, support for large input contexts, and zero-shot instruction following ability for programming tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCODE llama reaches SOTA performance among open models on several code benchmarks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20231016032328670\" loading=\"lazy\" src=\"/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/image-assets/image-20231016032328670.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBy training on domain-specific datasets, LLM have proved effective more broadly on applications that require advanced natural language understanding.\u003c/p\u003e","title":"Baptiste Roziere Code Llama Open Foundation Model for Code 2023"},{"content":"[TOC]\nTitle: Improved Baselines With Visual Instruction Tuning Author: Haotian Liu et. al. Publish Year: Oct 5 2023 Review Date: Sun, Oct 8, 2023 url: https://arxiv.org/pdf/2310.03744.pdf Summary of paper Motivation we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. Contribution with simple modifications to LLaVA, namely, using CLIP-ViT with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, they establish stronger baseline. Some key terms Improvement one: MLP cross modal connector\nImprovement two: Incorporating academic task related data such as VQA\nthe two improvements lead to better multimodal understanding capabilities Background instruction-following LMM\ntraining an instruction-following LMM usually follows a two-stage protocol. first, the vision-language alignment pretraining stage leverage image-text pairs to align the visual features with the language model\u0026rsquo;s world embedding space (BLIP) second, the visual instruction tuning stage tunes the model on visual instructions, to enable the model to follow user\u0026rsquo;s diverse requests on instructions that involve the visual contents. existing limitation\nLLaVA failed short on academic benchmarks that typically require short-form answers. this was attributed to the fact that LLaVA has not been pretrained on large-scale data. also we need a more proper prompt to regularize the output length MLP Vision Language Connector by changing from a linear projection to an MLP, they found that improving the vision-language connector\u0026rsquo;s representation power with two layer MLP can improve LLaVA\u0026rsquo;s multimodal capabilities, compared with the original linear projection design. ","permalink":"https://sino-huang.github.io/posts/haotian-liu-improved-baselines-with-visual-instruction-tuning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Improved Baselines With Visual Instruction Tuning\u003c/li\u003e\n\u003cli\u003eAuthor: Haotian Liu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Oct 5 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Oct 8, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2310.03744.pdf\"\u003ehttps://arxiv.org/pdf/2310.03744.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20231008103914399\" loading=\"lazy\" src=\"/posts/haotian-liu-improved-baselines-with-visual-instruction-tuning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewith simple modifications to LLaVA, namely, using CLIP-ViT with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, they establish stronger baseline.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eImprovement one: MLP cross modal connector\u003c/strong\u003e\u003c/p\u003e","title":"Haotian Liu Improved Baselines With Visual Instruction Tuning 2023"},{"content":"[TOC]\nTitle: Christabel Wayllace Goal Recognition Design With Stochastic Agent Action Outcomes 2016 Author: Christable Wayllace et. al. Publish Year: IJCAI 2016 Review Date: Fri, Oct 6, 2023 url: https://www.ijcai.org/Proceedings/16/Papers/464.pdf Summary of paper Motivation in this paper, they generalize the Goal Recognition Design (GRD) problem to Stochastic GRD (S-GRD) problems, which handle stochastic action outcomes. Some key terms Plan and goal recognition problem\nit aims to identify the actual plan or goal of an agent given its behaviour. Goal Recognition Design\ndesign the underlying environment of the agent, in such a way that the agent is forced to reveal its goals as early as possible. How we block some of the action, and if the agent\u0026rsquo;s policy is to go with the shortest path, then we can reveal its goal earlier. worst-case distinctiveness\na goodness measure that assesses the ease of performing goal recognition within an environment. The wcd of a problem is the longest sequence of actions an agent can take without revealing its goal. the objective in a GRD problem is then to find a subset of feasible actions to make infeasible such that the resulting wcd is minimised. ","permalink":"https://sino-huang.github.io/posts/christabel-wayllace-goal-recognition-design-with-stochastic-agent-action-outcomes-2016/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Christabel Wayllace Goal Recognition Design With Stochastic Agent Action Outcomes 2016\u003c/li\u003e\n\u003cli\u003eAuthor: Christable Wayllace et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: IJCAI 2016\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Oct 6, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.ijcai.org/Proceedings/16/Papers/464.pdf\"\u003ehttps://www.ijcai.org/Proceedings/16/Papers/464.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, they generalize the Goal Recognition Design (GRD) problem to Stochastic GRD (S-GRD) problems, which handle stochastic action outcomes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ePlan and goal recognition problem\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit aims to identify the actual plan or goal of an agent given its behaviour.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eGoal Recognition Design\u003c/strong\u003e\u003c/p\u003e","title":"Christabel Wayllace Goal Recognition Design With Stochastic Agent Action Outcomes 2016"},{"content":"[TOC]\nTitle: PDDL Domain Repair Fixing Domains With Incomplete Action Effects Author: Alba Gragera et. al. Publish Year: ICAPS 2023 Review Date: Wed, Sep 20, 2023 url: https://icaps23.icaps-conference.org/demos/papers/2791_paper.pdf Summary of paper Contribution in this paper, they present a tool to repair planning models where the effects of some actions are incomplete. The received input is compiled to a new extended planning task, in which actions are permitted to insert possible missing effects. The solution is a plan that achieves the goals of the original problem while also alerting users of the modification made. essentially it is a search algorithms helps to make the PDDL domain valid Essentially\nthere is a ground truth action plan, though the domain action definition is broken. the program will search how to maintain the action plan while repairing the domain model they actually design a PDDL domain and problem model for doing this search they are close sourced. ","permalink":"https://sino-huang.github.io/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: PDDL Domain Repair Fixing Domains With Incomplete Action Effects\u003c/li\u003e\n\u003cli\u003eAuthor: Alba Gragera et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: ICAPS 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Sep 20, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://icaps23.icaps-conference.org/demos/papers/2791_paper.pdf\"\u003ehttps://icaps23.icaps-conference.org/demos/papers/2791_paper.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230920232918668\" loading=\"lazy\" src=\"/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, they present a tool to repair planning models where the effects of some actions are incomplete. The received input is compiled to a new extended planning task, in which actions are permitted to insert possible missing effects. The solution is a plan that achieves the goals of the original problem while also alerting users of the modification made.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230920233613949\" loading=\"lazy\" src=\"/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/image-assets/image-20230920233613949.png\"\u003e\u003c/p\u003e","title":"Alba Gragera Pddl Domain Repair Fixing Domains With Incomplete Action Effects 2023"},{"content":"[TOC]\nTitle: Exploring the Limitations of Using LLMs to Fix Planning Tasks Author: Alba Gragera et. al. Publish Year: icaps23.icaps-conference Review Date: Wed, Sep 20, 2023 url: https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf Summary of paper Motivation In this work, the authors present ongoing efforts on exploring the limitations of LLMs in task requiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks. Contribution investigate how good LLMs are at repairing planning tasks when the prompt is given in PDDL and when it is given in natural language. also they tested on incomplete initial state and also incomplete domains which lack a necessary action effect to achieve the goals. in all cases, LLMs are used as stand-alone, and they directly assess the correctness of the solutions it generates. conclusion: they demonstrate that although LLMs can in principle facilitate iterative refinement of PDDL models through user interaction, their limited reasoning abilities render them insufficient for identifying meaningful changes to ill-defined planning models that result into solvable planning tasks.\nSome key terms inherent limitations of LLMs\nLLMs learn statistically patterns to predict the most probable next word or sentence, recent evidence strongly suggests that current LLMs are poor at task requiring planning capabilities such as plan generation, plan reuse or replanning. however, their web-scale knowledge and their utility as code assistants (Feng et al. 2020) suggest that they could assist users during the formalization phase. challenges of fixing domains\nFixing planning tasks in cases where the error lies in the domain model is not trivial due to the number of potential changes to the set of actions (Lin and Bercher 2021). Observations CHATGPT is better at dealing with missing initial states. errors in action definitions are really hard to get repaired. (solvable rate is 3/20) the authors found that communicating in natural language could help LLMs to find bugs. comment: however, it requires extra efforts converting NL descriptions backto PDDL and this can also be error-prone Good things about the paper (one paragraph) very useful experiment results Incomprehension I would say the authors should make more efforts refining their prompts so that GPT can produce better results. (though they have put this point in their future work) Potential future work A good insight of this paper is that, it summarised two important categories of the common PDDL errors \u0026ndash; 1) missing initial states; 2) ill-defined action definition, we shall develop ad-hoc modules targeting them. ","permalink":"https://sino-huang.github.io/posts/alba-gragera-exploring-the-limitations-of-using-llms-to-fix-planning-tasks-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Exploring the Limitations of Using LLMs to Fix Planning Tasks\u003c/li\u003e\n\u003cli\u003eAuthor: Alba Gragera et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: icaps23.icaps-conference\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Sep 20, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf\"\u003ehttps://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230920210538214\" loading=\"lazy\" src=\"/posts/alba-gragera-exploring-the-limitations-of-using-llms-to-fix-planning-tasks-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn this work, the authors present ongoing efforts on exploring the limitations of LLMs in task requiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003einvestigate how good LLMs are at repairing planning tasks when the prompt is given in PDDL and when it is given in natural language.\u003c/li\u003e\n\u003cli\u003ealso they tested on incomplete initial state and also incomplete domains which lack a necessary action effect to achieve the goals.\u003c/li\u003e\n\u003cli\u003ein all cases, LLMs are used as stand-alone, and they directly assess the correctness of the solutions it generates.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003econclusion:\u003c/strong\u003e they demonstrate that although LLMs can in principle facilitate iterative refinement of PDDL models through user interaction, their limited reasoning abilities render them insufficient for identifying meaningful changes to ill-defined planning models that result into solvable planning tasks.\u003c/p\u003e","title":"Alba Gragera Exploring the Limitations of Using LLMs to Fix Planning Tasks 2023"},{"content":"[TOC]\nTitle: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy Author: Tathagata Chakraborti Publish Year: 30 May 2017 Review Date: Tue, Sep 19, 2023 url: https://arxiv.org/pdf/1701.08317.pdf Summary of paper Motivation Past work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence) in this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate. Contribution They show how explanation can be seen as a \u0026ldquo;model reconciliation problem\u0026rdquo; (MRP), where AI system in effect suggests changes to the human\u0026rsquo;s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human\u0026rsquo;s mindset about the domain and task model such that the plan generated from the AI system fits human\u0026rsquo;s expectation. Some key terms Definition of a classical planning problem\nexplicable: i.e., generate plans that also make sense with respect to the humans\u0026rsquo; model.\nlimitation: Such explicability requirement however puts additional constraints on the agent’s plans, and may not always be feasible being called on to \u0026ldquo;explain\u0026rdquo; its plan: when the robot\u0026rsquo;s plan is different from what the human would expect given his model of the world, t he robot wil lbe called on to \u0026ldquo;explain\u0026rdquo; its plan\nthe author posit that such explanations should be seen as the robot\u0026rsquo;s attempt to move the human\u0026rsquo;s model to be in conformance with its own. model reconciliation problem (MRP)\nwhich aims to make minimal changes to the human\u0026rsquo;s model to bring it closer to the robot\u0026rsquo;s model comment: would this also apply to the domain fixing situation? this paper\u0026rsquo;s assumption: the human\u0026rsquo;s model is made available and is in PDDL format just like the robot\u0026rsquo;s one. here, the planner does not change its own behavior, but rather corrects the human’s incorrect perception of its model via explanations. multi model explanations\nin the paper, the author gave an example how an AI system should explain their plan when the action definition of the robot\u0026rsquo;s PDDL model differs from ones of the human\u0026rsquo;s PDDL model I think there is a implicit assumption that the robot\u0026rsquo;s PDDL model is self-justifying. (i.e., no semantic errors within its own system) formal mathematical terms\n$\\delta_{\\mathcal M}(\\mathcal I, \\pi) \\models G$: a sequential transition from initial state $\\mathcal I$ to goal state $G$\n$C(\\pi^{}, M^{R}) = C^{}_{M^{R}}$ : the total cost of optimal policy $\\pi^*$ under robot PDDL model $M$.\n$c_a$ is a single cost of action $a$\n$\\mathcal F$ is the fluent universe. fluent is different from state, a state is a compound of fluents new action set $\\Lambda$ containing unit model change actions such that there is only a single change to a domain at a time. $s_1 \\Delta s_2$ means the number of changes from state $s_1$ to $s_2$ Objective of this work: transforming model space from human model $\\mathcal M^{H}$ to $\\hat{\\mathcal M}$ the difference between the cost of the robot-generated optimal policy $\\pi^*$ in the transformed model and the cost of the local optimal policy should be smaller than the one between the cost in Human model and the cost of its local optimal policy. Good things about the paper (one paragraph) it provided a new paradigm about how to regard the explanation of PDDL models. The math part is however, too heavy for layman. Potential future work Limitation:\nthis paper assumed that the expert\u0026rsquo;s domain model is available and is in PDDL format. However, usually expert\u0026rsquo;s model is encoded in natural language and there is no available ground truth in PDDL format. ","permalink":"https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy\u003c/li\u003e\n\u003cli\u003eAuthor: Tathagata Chakraborti\u003c/li\u003e\n\u003cli\u003ePublish Year: 30 May 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Sep 19, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/1701.08317.pdf\"\u003ehttps://arxiv.org/pdf/1701.08317.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230920160252585\" loading=\"lazy\" src=\"/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePast work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence)\u003c/li\u003e\n\u003cli\u003ein this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThey show how explanation can be seen as a \u0026ldquo;model reconciliation problem\u0026rdquo; (MRP), where AI system in effect suggests changes to the human\u0026rsquo;s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human\u0026rsquo;s mindset about the domain and task model such that the plan generated from the AI system fits human\u0026rsquo;s expectation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition of a classical planning problem\u003c/strong\u003e\u003c/p\u003e","title":"Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017"},{"content":"[TOC]\nTitle: Plansformer \u0026ndash; Tool Demonstrating Generation of Symbolic Plans Using Transformers Author: Vishal Pallagani et. al. Publish Year: IJCAI-23 Review Date: Sat, Sep 16, 2023 url: https://www.ijcai.org/proceedings/2023/0839.pdf Summary of paper Motivation making a bridge between planning in LLM and planning in traditional automatic planner Design of Plansformer in the evaluation phase, planner testing helps to validate the plan (both the syntax validation and plan optimality validation), model testing helps to force a linguistic consistency (in this case it supervise the semantics). Function of this Plansformer The Plansformer operates as an AI planner designed for plan generation, not for creating PDDLs from natural language descriptions. ","permalink":"https://sino-huang.github.io/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Plansformer \u0026ndash; Tool Demonstrating Generation of Symbolic Plans Using Transformers\u003c/li\u003e\n\u003cli\u003eAuthor: Vishal Pallagani et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: IJCAI-23\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Sep 16, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.ijcai.org/proceedings/2023/0839.pdf\"\u003ehttps://www.ijcai.org/proceedings/2023/0839.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003emaking a bridge between planning in LLM and planning in traditional automatic planner\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"design-of-plansformer\"\u003eDesign of Plansformer\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230916142131826\" loading=\"lazy\" src=\"/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/image-assets/image-20230916142131826.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ein the evaluation phase, planner testing helps to validate the plan (both the syntax validation and plan optimality validation), model testing helps to force a linguistic consistency (in this case it supervise the semantics).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"function-of-this-plansformer\"\u003eFunction of this Plansformer\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe Plansformer operates as an AI planner designed for plan generation,  not for creating PDDLs from natural language descriptions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230916144140801\" loading=\"lazy\" src=\"/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/image-assets/image-20230916144140801.png\"\u003e\u003c/p\u003e","title":"Vishal Pallagani Plansformer Tool Demonstrating Generation of Symbolic Plans Using Transformers 2023"},{"content":"[TOC]\nTitle: BLIP2 - Boostrapping Language Image Pretraining 2023 Author: Junnan Li et. al. Publish Year: 15 Jun 2023 Review Date: Mon, Aug 28, 2023 url: https://arxiv.org/pdf/2301.12597.pdf Summary of paper The paper titled \u0026ldquo;BLIP-2\u0026rdquo; proposes a new and efficient pre-training strategy for vision-and-language models. The cost of training such models has been increasingly prohibitive due to the large scale of the models. BLIP-2 aims to address this issue by leveraging off-the-shelf, pre-trained image encoders and large language models (LLMs) that are kept frozen during the pre-training process.\nKey Components: Querying Transformer (Q-Former): A lightweight transformer that uses learnable query vectors to extract features from the frozen image encoder. It acts as an information bottleneck between the frozen image encoder and the frozen LLM. Two-Stage Pre-training Strategy: First Stage: Vision-language representation learning, where Q-Former learns to extract visual features most relevant to the text. Second Stage: Vision-to-language generative learning, where Q-Former is trained to produce visual representations that can be interpreted by the LLM. Advantages: Efficiency: BLIP-2 has significantly fewer trainable parameters compared to existing methods but achieves state-of-the-art performance. Versatility: It performs well on various vision-language tasks like visual question answering, image captioning, and image-text retrieval. Zero-Shot Capabilities: Powered by LLMs, BLIP-2 can perform zero-shot image-to-text generation following natural language instructions. The paper claims that BLIP-2 outperforms existing models like Flamingo80B by 8.7% on zero-shot VQAv2 while having 54x fewer trainable parameters.\nQ-former Information Bottleneck: The term \u0026ldquo;information bottleneck\u0026rdquo; refers to Q-Former\u0026rsquo;s role in selectively passing the most useful visual features from the frozen image encoder to the frozen LLM. It filters and condenses the information, ensuring that only the most relevant visual features are used for generating the desired text output.\nLearnable Query Embeddings: Q-Former employs a set of learnable query vectors (or embeddings) that serve as input to the image transformer. These queries interact with each other through self-attention layers and with frozen image features through cross-attention layers. They can also interact with the text.\n","permalink":"https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  BLIP2 - Boostrapping Language Image Pretraining 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Junnan Li et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 15 Jun 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Aug 28, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.12597.pdf\"\u003ehttps://arxiv.org/pdf/2301.12597.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe paper titled \u0026ldquo;BLIP-2\u0026rdquo; proposes a new and efficient pre-training strategy for vision-and-language models. The cost of training such models has been increasingly prohibitive due to the large scale of the models. BLIP-2 aims to address this issue by leveraging off-the-shelf, pre-trained image encoders and large language models (LLMs) that are kept frozen during the pre-training process.\u003c/p\u003e","title":"Junnan_li Blip2 Boostrapping Language Image Pretraining 2023"},{"content":"[TOC]\nTitle: Llama Adapter V2 Author: Peng Gao et. al. Publish Year: 28 Apr 2023 Review Date: Mon, Aug 28, 2023 url: https://arxiv.org/pdf/2304.15010.pdf Summary of paper The paper presents LLaMA-Adapter V2, an enhanced version of the original LLaMA-Adapter designed for multi-modal reasoning and instruction following. The paper aims to address the limitations of the original LLaMA-Adapter, which could not generalize well to open-ended visual instructions and lagged behind GPT-4 in performance.\nKey Features of LLaMA-Adapter V2: More Learnable Parameters: The new version unlocks additional learnable parameters like norms, biases, and scales. This distributes the instruction-following ability across the entire LLaMA model, not just the adapters. Early Fusion Strategy: Visual tokens are fed only into the early layers of the Large Language Model (LLM), which helps in better incorporation of visual knowledge. Joint Training Paradigm: It introduces a joint training approach that optimizes disjoint groups of learnable parameters for image-text pairs and instruction-following data. This helps to minimize the interference between the two tasks and improves multi-modal reasoning. Expert Models: During inference, additional expert models like captioning and OCR systems are incorporated to enhance the model\u0026rsquo;s image understanding capabilities without incurring additional training costs. Advantages: Parameter Efficiency: Only 14 million additional parameters are introduced over the original LLaMA model. Improved Performance: The new version performs better in open-ended multi-modal instructions and even excels in chat interactions. The paper suggests that LLaMA-Adapter V2 is a more parameter-efficient and capable model for handling visual instructions and multi-modal reasoning tasks.\n","permalink":"https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Llama Adapter V2\u003c/li\u003e\n\u003cli\u003eAuthor: Peng Gao et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 28 Apr 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Aug 28, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2304.15010.pdf\"\u003ehttps://arxiv.org/pdf/2304.15010.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe paper presents LLaMA-Adapter V2, an enhanced version of the original LLaMA-Adapter designed for multi-modal reasoning and instruction following. The paper aims to address the limitations of the original LLaMA-Adapter, which could not generalize well to open-ended visual instructions and lagged behind GPT-4 in performance.\u003c/p\u003e","title":"Peng_gao Llama Adapter V2 2023"},{"content":"[TOC]\nTitle: Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning 2022 Author: Rodrigo Toro Icarte et. al. Publish Year: 2022 AI Access Foundation Review Date: Thu, Aug 17, 2023 url: https://arxiv.org/abs/2010.03950 Summary of paper Motivation in most RL applications, however, users have to program the reward function and hence, there is the opportunity to make the reward function visible and RL agent can exploit the function\u0026rsquo;s internal structure to learn optimal policies in a more sample efficient manner. Contribution different methodology of RL for Reward Machines compared to their previous studies, this work tested a collection of RL methods that can exploit a reward machine\u0026rsquo;s internal structure to improve sample efficiency Some key terms counterfactual experiences for reward machines (CRM)\nthis policy learn over the cross product $\\pi(a|s, u)$, but use counterfactual reasoning to generate synthetic experiences. these experiences can then be used by an off-policy learning method this means based on the property of RM, we can calculate imagined but correct experiences and use that for our training. but we know that by having a complicated reward machine, you have already make this MDP problem complicated, and this method is just for alleviating a bit of the situation. Q-learning for reward machine\nlearn separate q functions Hierarchical RL for RM\nthe agent will learn a set of options for the cross-product MDP, that focus on learning how to move from one RM state to another RM state. it means learn a good option that can transit states in RM such that it can reach the goal state. the sub-policy is corresponding to each edge in the RM the set of options will be $\\mathcal A = {\\langle u, \\delta_u(u, \\sigma)\\rangle | u \\in U, \\sigma \\in 2^{\\mathcal P}}$ however, it terminates when reward state transits to another, which means RM provides accurate information for transitions Results Potential future work the HRM thing, this might be related to LRS improvement ","permalink":"https://sino-huang.github.io/posts/rodrigo-reward-machines-exploiting-reward-function-structure-in-rl-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning 2022\u003c/li\u003e\n\u003cli\u003eAuthor: Rodrigo Toro Icarte et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022 AI Access Foundation\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Aug 17, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2010.03950\"\u003ehttps://arxiv.org/abs/2010.03950\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein most RL applications, however, users have to program the reward function and hence, there is the opportunity to make the reward function visible and RL  agent can exploit the function\u0026rsquo;s internal structure to learn optimal policies in a more sample efficient manner.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003edifferent methodology of RL for Reward Machines\u003c/li\u003e\n\u003cli\u003ecompared to their previous studies, this work tested a collection of RL methods that can exploit a reward machine\u0026rsquo;s internal structure to improve sample efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ecounterfactual experiences for reward machines (CRM)\u003c/strong\u003e\u003c/p\u003e","title":"Rodrigo Reward Machines Exploiting Reward Function Structure in Rl 2022"},{"content":"[TOC]\nTitle: Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning Author: Rodrigo Toro Icarte et. al. Publish Year: PMLR 2018 Review Date: Thu, Aug 17, 2023 url: http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf Summary of paper Motivation proposing a reward machine while exposing reward function structure to the learner and supporting decomposition. Contribution in contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case. Some key terms intro\nthere is less reason to hide the reward function from the agent. this work our method is able to reward behaviours to varying degrees in manners that cannot be expressed by previous approaches. i.e., reward the behaviours in advanced way reward machine\na RM allows for composing different reward functions in flexible ways, including concatenations, loops and conditional rules. after every transition, the reward machine output the reward function the agent should use at that time. the author claim that the structure of RM can give agent knowledge that the problem consists of multiple stages and thus can use the information to decompose the task. Q-learning for reward machines (QRM)\ncan exploit a reward machine\u0026rsquo;s internal structure to decompose the problem and thereby improving sample efficiency. it uses q learning to update each sub-task policy in parallel. we show that QRM is guaranteed to converge to an optimal policy in the tabular case. Bellman equation\nReward machine for task specification The intuition is that the agent will be rewarded by different reward functions at different times, depending on the transitions made in the RM. definition of reward machine\ndefinition of simple reward machine the reward only relies on the internal state of RM rather than the transitions in the world. Limitation the RM has a clear understanding about the when the world state transits to their targeted high-level ones. a ground-truth labelling function A posible solution for LRS\nwe need different q functions \u0026ndash; meaning different sub policies Major comments it is still very challenging to think about how to ground this method to simulated Atari games where there is no prepared labelling function for you. ","permalink":"https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Rodrigo Toro Icarte et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: PMLR 2018\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Aug 17, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf\"\u003ehttp://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230817111402194\" loading=\"lazy\" src=\"/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eproposing a reward machine while exposing reward function structure to the learner and supporting decomposition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eintro\u003c/strong\u003e\u003c/p\u003e","title":"Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018"},{"content":"[TOC]\nTitle: Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language Author: William Berrios et. al. Publish Year: 28 Jun 2023 Review Date: Mon, Jul 3, 2023 url: https://arxiv.org/pdf/2306.16410.pdf Summary of paper Contribution proposing LENS, a modular approach that addresses computer vision tasks by harnessing the few-shot, in-context learning abilities of language models through natural language descriptions of visual inputs LENS enables any off-the-shelf LLM to have visual capabilities without auxiliary training or data LENS framework a redundant text prompt might be helpful LENS components\nLENS consists of 3 distinct vision modules and 1 reasoning module, each serving a specific purpose based on the task at hand. These components are as follows: Prompt design Potential future work How to encode input image to text prompt, this paper provides a good approach\nwe may combine this model with the Boosting Language Models Reasoning With Chain of Knowledge Prompting ","permalink":"https://sino-huang.github.io/posts/william_berrios-towards-language-models-that-can-see-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language\u003c/li\u003e\n\u003cli\u003eAuthor: William Berrios et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 28 Jun 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jul 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2306.16410.pdf\"\u003ehttps://arxiv.org/pdf/2306.16410.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230703193548354\" loading=\"lazy\" src=\"/posts/william_berrios-towards-language-models-that-can-see-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eproposing LENS, a modular approach that addresses computer vision tasks by harnessing the few-shot, in-context learning abilities of language models through natural language descriptions of visual inputs\u003c/li\u003e\n\u003cli\u003eLENS enables any off-the-shelf LLM to have visual capabilities without auxiliary training or data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"lens-framework\"\u003eLENS framework\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230703195306870\" loading=\"lazy\" src=\"/posts/william_berrios-towards-language-models-that-can-see-2023/image-assets/image-20230703195306870.png\"\u003e\u003c/p\u003e","title":"William_berrios Towards Language Models That Can See 2023"},{"content":"[TOC]\nTitle: From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought Author: Lionel Wong et. al. Publish Year: 23 Jun 2023 Review Date: Sun, Jul 2, 2023 url: https://arxiv.org/pdf/2306.12672.pdf Summary of paper Motivation leverage a theory of linguistic meaning to build machines that think in more human-like ways.\nwe frame linguistic meaning as a context-sensitive mapping from NL into a probabilistic language of thought (PLoT) \u0026ndash; a general-purpose symbolic substrate for probabilistic, generative world modelling\nSome key terms language and thinking\nTheories of cognition have long considered human language and thinking to be deeply related, but fundamentally distinct. Thinking, in many traditional cognitive theories, revolves around goal-directed world modeling, inference, and decision making—constructing mental models of the world that reflect prior beliefs, can be updated from new observations, and support rational prediction and decision making toward’s one’s goals (Craik, 1967; Gentner \u0026amp; Stevens, 2014; Johnson-Laird, 1980, 1989; Lake, Ullman, Tenenbaum, \u0026amp; Gershman, 2017; Morgan, 1999; Nersessian et al., 2010). Language, in contrast, centers around communicating these thoughts to others, and receiving their thoughts in turn. In most linguistic theories, human languages are mappings between the internal representations of thought and an externalizable symbol system, which might be phonemes, signs, or glyphs (Frege, 1892; Heim \u0026amp; Kratzer, 1998; Lewis, 1976). framework\nour rational meaning construction framework decomposes language-informed thinking into two modules: A meaning function translates natural language into probabilistic programming language (PPL) statements that represent linguistic meaning with respect to a symbolic world model. An inference function computes probabilities over the space of possible worlds consistent with and conditioned on information in the language. Knowledge, observation and query Knowledge maps to generative world models Observations maps to condition statements Query maps to query statement Examples Planning\nOur example probabilistic generative model describes a prior over agents with different preferences for the nearby restaurants shown on the map, as well as the relative cost of getting to each one on bike or on foot. Integrating a model-based planner into this generative model allows it to express a prior on how agents will actually act based on their desires, balancing these preferences against whether restaurants are open, and whether or not they have a bike. Observations and queries about the agents, their goals, and about the world itself updates a unified belief distribution, reflecting how agents plan in the world and how observing their actions drives inferences about the latent state in the world. Good things about the paper (one paragraph) very detailed paper Potential future work converting language into code based might be a key to have a solid reasoning process in term of generating the reasoning outcome, the authors suggested having a probabilistic inference function to generate the probability distribution of the choices. ","permalink":"https://sino-huang.github.io/posts/lionel_wong-from-word-models-to-world-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought\u003c/li\u003e\n\u003cli\u003eAuthor: Lionel Wong et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 23 Jun 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jul 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2306.12672.pdf\"\u003ehttps://arxiv.org/pdf/2306.12672.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eleverage a theory of linguistic meaning to build machines that think in more human-like ways.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewe frame linguistic meaning as a context-sensitive mapping from NL into a probabilistic language of thought (PLoT) \u0026ndash; a general-purpose \u003cstrong\u003esymbolic substrate for probabilistic, generative world modelling\u003c/strong\u003e\u003c/p\u003e","title":"Lionel_wong From Word Models to World Models 2023"},{"content":"[TOC]\nTitle: Boosting Language Models Reasoning With Chain of Knowledge Prompting Author: Jianing Wang et. al. Publish Year: 10 Jun 2023 Review Date: Sun, Jul 2, 2023 url: https://arxiv.org/pdf/2306.06427.pdf Summary of paper Motivation \u0026ldquo;Chain of Thought (CoT)\u0026rdquo; aims at designing a simple prompt like \u0026ldquo;Let\u0026rsquo;s think step by step\u0026rdquo; however, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chain To mitigate this brittleness, we propose a novel Chain-of-Knowlege knowledge evidence in the form of structure triple Contribution Benefiting from CoK, we additional introduce a F^2 -Verification method to estimate the reliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Some key terms F^2 Verification\nIt is known that LLMs are not capable of inspecting the prediction, so the quality of the generated rationale and the final answer can not be guaranteed. We attribute this problem to two factors: 1) some steps in the rationale may not correspond to the fact, contributing to the wrongness, and 2) the relation between the final answer and the reasoning chain is still ambiguous. F^2 verification to estimate the answer reliability towards both Factuality and Faithfulness. Factuality Verification: check the matching degree between each generated evidience triple and the ground-truth knowledge from KBs. Faithfulness Verification: if the reasoning process derived from the model can accurately expressed by an explanation, we call it faithful. How: SimCSE sentence encoder to calculate the similarity between explanation sentence H and the concatenation of [Query, Triplet and Answer] Potential future work This paper provides a good way of how to verify the answer of LLMs ","permalink":"https://sino-huang.github.io/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Boosting Language Models Reasoning With Chain of Knowledge Prompting\u003c/li\u003e\n\u003cli\u003eAuthor: Jianing Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Jun 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jul 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2306.06427.pdf\"\u003ehttps://arxiv.org/pdf/2306.06427.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230702162535381\" loading=\"lazy\" src=\"/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u0026ldquo;Chain of Thought (CoT)\u0026rdquo; aims at designing a simple prompt like \u0026ldquo;Let\u0026rsquo;s think step by step\u0026rdquo;\u003c/li\u003e\n\u003cli\u003ehowever, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chain\u003c/li\u003e\n\u003cli\u003eTo mitigate this brittleness, we propose a novel Chain-of-Knowlege knowledge evidence in the form of structure triple\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBenefiting from CoK, we additional introduce a F^2 -Verification method to estimate the reliable response, the wrong evidence can be indicated to prompt the LLM to rethink.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230702174714043\" loading=\"lazy\" src=\"/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/image-assets/image-20230702174714043.png\"\u003e\u003c/p\u003e","title":"Jianning_wang Boosting Language Models Reasoning With Chain of Knowledge Prompting 2023"},{"content":"[TOC]\nTitle: Leveraging Pretrained Large Language Models to Construct and Utilise World Models for Model Based Task Planning Author: Lin Guan et. al. Publish Year: 24 May 2023 Review Date: Sun, Jun 4, 2023 url: https://arxiv.org/pdf/2305.14909.pdf Summary of paper Motivation However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. Contribution introduce a alternative paradigm that construct an explicit world (domain) model in planning domain definition language (PDDL) and then use it to plan with sound domain-independent planners. users can correct the PDDL before the real planning. Findings GPT-4 can readily correct all the errors according to natural language feedback from PDDL validators and humans. Some key terms approach\nWhen providing the planner with a set of actions and their brief natural language descriptions, instead of directly mapping user commands to plans, we utilise LLMs to extract a symbolic representation of the action in the form of PDDL action models. our modular method essentially divides the planning process into two distinct parts, namely modelling the causal dependencies of actions and determining the appropriate sequence of actions to accomplish the goals. autocorrection for PDDL\nPDDL validator in VAL. how to utilise the generated PDDL action models\nfirst way: we use this PDDL to search for a plan using classical planner. second way: we use PDDL model to validate the plan in natural language. by providing corrective feedback in the form of unmet preconditions or goal conditions the author said in the second way, both plan generated by PDDL model and the plan generated by LLM planner can incorporate. The prompting design Our approach involves prompting pre-trained LLMs with the following information: (a) detailed instructions for the PDDL generation task, outlining components of upcoming inputs and desired outputs; (b) one or two examples from other domains (e.g., the classical Blocksworld domain) for illustrating the input and output formats; (c) a description of the current domain, including contextual information about the agent’s tasks and physical constraints due to the specific embodiment of the agent; (d) a description of the agent’s action; and (e) a dynamically updated list of predicates that the LLM can reuse to maintain consistent use of symbols across multiple actions. concrete example\nResults Issue\nThis suggests that our framework relies heavily on GPT-4’s improved capability in understanding symbols, and future work may investigate how to enable the use of more lightweight models (e.g., by fine-tuning on some PDDL datasets) Major comments important assumption in the problem setting\nThe authors assume that the agent is equipped with the low-level control policies corresponding to these high-level skills. however, one has to consider that interacting with LLM may take a very long computational time and we have to carefully design the feedback loop when we want to train the low-level control policies when LLM is involved. Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/lin_guan-leveraging-pretrained-llm-to-construct-and-utilise-world-models-for-model-based-task-planning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Leveraging Pretrained Large Language Models to Construct and Utilise World Models for Model Based Task Planning\u003c/li\u003e\n\u003cli\u003eAuthor: Lin Guan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 24 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Jun 4, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.14909.pdf\"\u003ehttps://arxiv.org/pdf/2305.14909.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230604120324310\" loading=\"lazy\" src=\"/posts/lin_guan-leveraging-pretrained-llm-to-construct-and-utilise-world-models-for-model-based-task-planning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHowever, methods that use LLMs directly as planners\nare currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eintroduce a alternative paradigm that construct an explicit world (domain) model in planning domain definition language (PDDL) and then use it to plan with sound domain-independent planners.\u003c/li\u003e\n\u003cli\u003eusers can correct the PDDL before the real planning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"findings\"\u003eFindings\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGPT-4 can readily correct all the errors according to natural language feedback from PDDL validators and humans.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eapproach\u003c/strong\u003e\u003c/p\u003e","title":"Lin_guan Leveraging Pretrained Llm to Construct and Utilise World Models for Model Based Task Planning 2023"},{"content":"[TOC]\nTitle: Neural Machine Translation for Code Generation Author: Dharma KC et. al. Publish Year: 22 May 2023 Review Date: Sun, May 28, 2023 url: https://arxiv.org/pdf/2305.13504.pdf Summary of paper Motivation Recently, NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. Conclusion NMT-based architecture are getting quite popular for source generation from various input. The NMT-based code generation is useful in multiple domains such as code generation from input binary or assembly (decompilation), code-to-code translation, code repair, bug fixing, and many more. some open problems source code has long dependencies in multiple places next-token prediction technique may lost the dependency information Methods that can break down a problem into small problems, generate code for such subprograms, and evaluate them are good potential research direction sample efficiency Current code generation does not combine code abstraction to higher-level abstractions as human do. Execution-guided synthesis currently works with DSLs, but extending them to real-world source code generation is a research direction. Retrieve-and-Edit framework ","permalink":"https://sino-huang.github.io/posts/dharma_kc-neural-machine-translation-for-code-generation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Neural Machine Translation for Code Generation\u003c/li\u003e\n\u003cli\u003eAuthor: Dharma KC et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, May 28, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.13504.pdf\"\u003ehttps://arxiv.org/pdf/2305.13504.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRecently, NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNMT-based architecture are getting quite popular for source generation from various input. The NMT-based code generation is useful in multiple domains such as code generation from input binary or assembly (decompilation), code-to-code translation, code repair, bug fixing, and many more.\u003c/li\u003e\n\u003cli\u003esome open problems\n\u003cul\u003e\n\u003cli\u003esource code has long dependencies in multiple places\n\u003cul\u003e\n\u003cli\u003enext-token prediction technique may lost the dependency information\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMethods that can break down a problem into small problems, generate code for such subprograms, and evaluate them are good potential research direction\u003c/li\u003e\n\u003cli\u003esample efficiency\u003c/li\u003e\n\u003cli\u003eCurrent code generation does not combine code abstraction to higher-level abstractions as human do.\u003c/li\u003e\n\u003cli\u003eExecution-guided synthesis currently works with DSLs, but extending them to real-world source code generation is a research direction.\u003c/li\u003e\n\u003cli\u003eRetrieve-and-Edit framework\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Dharma_kc Neural Machine Translation for Code Generation 2023"},{"content":"[TOC]\nTitle: Language Models Meet World Models: Embodied Experiences Enhance Language Models Author: Jiannan Xiang et. al. Publish Year: 22 May 2023 Review Date: Fri, May 26, 2023 url: https://arxiv.org/pdf/2305.10626v2.pdf Summary of paper Motivation LLM often struggle with simple reasoning and planning in physical environment the limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. Contribution we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. the experiments in a virtual physical world simulation environment will be used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking etc. to preserve the generalisation ability of LM models, we use elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Some key terms limitation of current ChatGPT fail to track the world state. Consequently, they lack robust and comprehensive embodied knowledge necessary for reasoning and planning associated with physical environments definition of world model\nworld models areembodies simulators that emulate physical interactions in real world environments. two ways to collect embodied experience\ngoal-oriented planning and random exploration Specifically, goal-oriented planning aims to gather experiences associated with planning and goal-oriented agent behaviors, while random exploration focuses on accumulating experiences that involve object and world state tracking. In goal-oriented planning, the process will be stored as an embodied experiences. after gathering the embodied experiments\nwe will use them to construct a set of fine-tuning tasks (e.g., plan generation, activity recognition, and tracking) definition of EWC\ncheck https://arxiv.org/pdf/1612.00796.pdf We show that EWC is substantially more effective than the popular KL regularization Low-Rank Adaptation (LoRA)\nConclusion \u0026amp; Limitations the present work is limited to a household environment as a single world model. In the future, we intend to study how to integrate embodied experiences from different work models and generalise knowledge learned from one world model to different domain. Potential future work this paper present a good way of finetuning LLMs to fit to planning problems. ","permalink":"https://sino-huang.github.io/posts/jiannan_xiang-language-models-meet-world-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language Models Meet World Models: Embodied Experiences Enhance Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Jiannan Xiang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, May 26, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.10626v2.pdf\"\u003ehttps://arxiv.org/pdf/2305.10626v2.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLLM often struggle with simple reasoning and planning in physical environment\u003c/li\u003e\n\u003cli\u003ethe limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities.\u003c/li\u003e\n\u003cli\u003ethe experiments in a virtual physical world simulation environment will be used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking etc.\u003c/li\u003e\n\u003cli\u003eto preserve the generalisation ability of LM models, we use elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230527163509701\" loading=\"lazy\" src=\"/posts/jiannan_xiang-language-models-meet-world-models-2023/image-assets/cover.png\"\u003e\u003c/p\u003e","title":"Jiannan_xiang Language Models Meet World Models 2023"},{"content":"[TOC]\nTitle: PG3 Policy Guided Planning for Generalised Policy Generation Author: Ryan Yang et. al. Publish Year: 21 Apr 2022 Review Date: Wed, May 24, 2023 url: https://arxiv.org/pdf/2204.10420.pdf Summary of paper Motivation a longstanding objective in classical planning is to synthesise policies that generalise across multiple problems from the same domain this work, we study generalised policy search-based methods with a focus on the score function used to guide the search over policies Contribution we study a specific instantiation of policy search where planning problems are PDDL-based and policies are lifted decision lists. Some key terms what is generalised planning and generalised policy search (GPS)\nGPS is a flexible paradigm for generalised planning. In this family of methods, a search is performed through a class of generalised (goal-conditioned) policies, with the search informed by a score function that maps candidates policies to scalar values. there has been relatively less work on the score function. the score function plays an important role: if the score are uninformative or misleading, the search will languish in less promising regions of policy space. Problem setting in generalised planning\nGiven: PDDL domain Training problems PLanner Goal: Learn a goal-conditioned policy that generalises to all test problems in domain. How does PG3 work\nsearch through the space of candidate policies candidate policies representation is a lifted decision list which consists of an ordered list of rules Evaluation process\ncalculate how many problems can a given candidate policy is able to solve policy evaluation\nexecutes the policy on the training tasks and records the number of success extremely sparse, effectively forcing an exhaustive search until reaching a region of non-zero scores. plan comparison\nplans on the training tasks and records the agreement between the found plans and the candidate policy. (in practice the score is the priority score, lower is better) ","permalink":"https://sino-huang.github.io/posts/ryan_yang-pg3-policy-guided-planning-for-generalised-policy-generation-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: PG3 Policy Guided Planning for Generalised Policy Generation\u003c/li\u003e\n\u003cli\u003eAuthor: Ryan Yang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 21 Apr 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, May 24, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2204.10420.pdf\"\u003ehttps://arxiv.org/pdf/2204.10420.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230524195832214\" loading=\"lazy\" src=\"/posts/ryan_yang-pg3-policy-guided-planning-for-generalised-policy-generation-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ea longstanding objective in classical planning is to synthesise policies that generalise across multiple problems from the same domain\u003c/li\u003e\n\u003cli\u003ethis work, we study generalised policy search-based methods with a focus on the score function used to guide the search over policies\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe study a specific instantiation of policy search where planning problems are PDDL-based and policies are lifted decision lists.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ewhat is generalised planning and generalised policy search (GPS)\u003c/strong\u003e\u003c/p\u003e","title":"Ryan_yang PG3 Policy Guided Planning for Generalised Policy Generation 2022"},{"content":"[TOC]\nTitle: Tree of Thoughts: Deliberate Problem Solving with LLM Author: Shunyu Yao et. al. Publish Year: 17 May 2023 Review Date: Wed, May 24, 2023 url: https://arxiv.org/pdf/2305.10601.pdf Summary of paper Motivation might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. Contribution limitation\n","permalink":"https://sino-huang.github.io/posts/shunyu_yao-tree-of-thoughts-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Tree of Thoughts: Deliberate Problem Solving with LLM\u003c/li\u003e\n\u003cli\u003eAuthor: Shunyu Yao et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 17 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, May 24, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.10601.pdf\"\u003ehttps://arxiv.org/pdf/2305.10601.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230524163530127\" loading=\"lazy\" src=\"/posts/shunyu_yao-tree-of-thoughts-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003emight benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions.\u003c/li\u003e\n\u003cli\u003esearch through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg alt=\"image-20230524164011231\" loading=\"lazy\" src=\"/posts/shunyu_yao-tree-of-thoughts-2023/image-assets/image-20230524164011231.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003elimitation\u003c/strong\u003e\u003c/p\u003e","title":"Shunyu_yao Tree of Thoughts 2023"},{"content":"[TOC]\nTitle: Generalised Planning in Pddl Domains With Pretrained Large Language Models Author: Tom Silver et. al. Publish Year: 18 May 2023 Review Date: Tue, May 23, 2023 url: https://arxiv.org/pdf/2305.11014.pdf Summary of paper Motivation in particular, we consider PDDL domains and use GPT-4 to synthesize Python programs, we also consider Chain of Thought (CoT) summarisation, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program we consider automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback. Contribution we find that GPT4 is a surprisingly powerful generalised planner. we also conclude that automated debugging is very important, that CoT summarisation has non-uniform impact, that GPT4 is far superior to GPT3.5, and that just two training tasks are often sufficient for strong generalisation. Some key terms the problem\nIt remains challenging to efficiently synthesize programs from few training tasks that generalise to a wide variety of held-out tasks. process\nwe prompt GPT-4 to write a natural language summary of the PDDL domain We then ask it to describe a solution strategy before finally implementing the strategy in Python We automatically provide feedback to GPT-4 in the case where it fails to solve training tasks definition of generalised planning\nbackground on PDDL\nA PDDL domain is characterized by a name, a set of types, a set of predicates and a set of operators. PDDL problem is characterised by a domain, a set of objects an initial state and a goal. The objective of the work\nat evaluation time, a set of held-out evaluation tasks \u0026ndash; typically involving many more objects \u0026ndash; are used to measure performance. The objective is to use the training tasks to synthesise a program that will produce valid plans for all the evaluation tasks. prompting method\nwe hypothesized that decomposing generalised planning into three stages \u0026ndash; domain summarisation, strategy proposal and strategy implementation \u0026ndash; would improve performance Experiments and results Can GPT-4 be used for generalised (PDDL) planning? Are the synthesized programs efficient? Does CoT summarization help? Does automated debugging help? To what extent does GPT-4 rely on names in the PDDL How does GPT-4 compare to GPT3.5 Do each of the four error types help? (Python exception, Timeout, Plan Syntax, Plan Semantics) experiment environment\nAblation\nPotential future work Strange thing about the paper\nI am a bit confused as the paper is asking the LLM to synthesise python program to generate a plan rather than asking a PDDL planner to generate a plan. Isn\u0026rsquo;t it true that a PDDL planner can directly generate a valid plan based on the PDDL domain and problem files. Why do we need a LLM to synthesise a python program to generate plan? Does it means that it still requires the LLM model to perform functional reasoning ability? limitation and direction of future work\nthe author did not know why CoT hamper the plan generation A major limitation of this work and previous work on generalized planning is that it is easy enough to hand-design generalized plans for all of the domains considered. In some cases, it may be considerably easier to specify PDDL domain and problem descriptions than it is to directly specify a generalized plan. ","permalink":"https://sino-huang.github.io/posts/tom_silver-generalised-planning-in-pddl-domains-with-pretrained-large-language-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Generalised Planning in Pddl Domains With Pretrained Large Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Tom Silver et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 18 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, May 23, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.11014.pdf\"\u003ehttps://arxiv.org/pdf/2305.11014.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein particular, we consider PDDL domains and use GPT-4 to synthesize Python programs,\u003c/li\u003e\n\u003cli\u003ewe also consider Chain of Thought (CoT) summarisation, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program\u003c/li\u003e\n\u003cli\u003ewe consider automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe find that GPT4 is a surprisingly powerful generalised planner.\u003c/li\u003e\n\u003cli\u003ewe also conclude that automated debugging is very important, that CoT summarisation has non-uniform impact, that GPT4 is far superior to GPT3.5, and that just two training tasks are often sufficient for strong generalisation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ethe problem\u003c/strong\u003e\u003c/p\u003e","title":"Tom_silver Generalised Planning in PDDL Domains With Pretrained Large Language Models 2023"},{"content":"[TOC]\nTitle: HuggingGPT: Solving AI tasks with ChatGPT and its Friends in Hugging Face Author: Yongliang Shen et. al. Publish Year: 2 Apr 2023 Review Date: Tue, May 23, 2023 url: https://arxiv.org/pdf/2303.17580.pdf Summary of paper Motivation while there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this Contribution specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging face, execute each subtask with the selected AI model, and summarize the response according to the execution results. Some key terms Model\nthe bridge\nwe notice that each AI model can be denoted as a form of language by summarizing its model function. therefore, by incorporating these model descriptions into prompt, LLMs can be considered as the brain to manage AI models. process\ntask planning: using ChatGPT to analyse the request of users to understand their intention, and disassemble them into possible solvable tasks via prompts model selection: to solve the planned tasks, ChatGPT selects expert models that are hosted on Hugging Face based on model descriptions task execution: invoke and execute each selected model, and return the results to ChatGPT response generation: using ChatGPT to integrate the prediction of all models, and generate answers for users Prompt design in ChatGPT\nPotential future work limitation of HuggingGPT\nstability\nrebellion that occurs during the inference of large language models. LLM occasionally fail to conform to instructions when inferring, and the output format may defy expectations, leading to exceptions in the program workflow. ","permalink":"https://sino-huang.github.io/posts/yongliang-hugginggpt-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: HuggingGPT: Solving AI tasks with ChatGPT and its Friends in Hugging Face\u003c/li\u003e\n\u003cli\u003eAuthor: Yongliang Shen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2 Apr 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, May 23, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2303.17580.pdf\"\u003ehttps://arxiv.org/pdf/2303.17580.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewhile there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks.\u003c/li\u003e\n\u003cli\u003ewe advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003especifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging face, execute each subtask with the selected AI model, and summarize the response according to the execution results.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e\u003c/p\u003e","title":"Yongliang Hugginggpt 2023"},{"content":"[TOC]\nTitle: Translating Natural Language to Planning Goals With LLM Author: Yaqi Xie et. al. Publish Year: 10 Feb 2023 Review Date: Mon, May 22, 2023 url: https://arxiv.org/pdf/2302.05128.pdf Summary of paper Motivation Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problem LLM can act as a natural interface between the planner and human users Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. Contribution We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language) Some key terms Architecture\nPotential future work the experiment is only about generating PDDL goal from the PDDL texts however, in ALFRED environment, PDDL problem file will not be provided during testing. ","permalink":"https://sino-huang.github.io/posts/yaqi_xie-translating-natural-language-to-planning-goals-with-llm-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Translating Natural Language to Planning Goals With LLM\u003c/li\u003e\n\u003cli\u003eAuthor: Yaqi Xie et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 22, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.05128.pdf\"\u003ehttps://arxiv.org/pdf/2302.05128.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUnfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problem\u003c/li\u003e\n\u003cli\u003eLLM can act as a natural interface between the planner and human users\u003c/li\u003e\n\u003cli\u003eOur empirical results on GPT 3.5 variants show that LLMs are much\nbetter suited towards translation rather than planning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as\nis often the case in natural language)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/p\u003e","title":"Yaqi_xie Translating Natural Language to Planning Goals With Llm 2023"},{"content":"[TOC]\nTitle: LLM+P Empowering Large Language Models With Optimal Planning Proficiency Author: Bo Liu Publish Year: 5 May 2023 Review Date: Mon, May 22, 2023 url: https://arxiv.org/pdf/2304.11477.pdf Summary of paper Motivation However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal plans. Contribution introduce LLM+P, it takes in a natural language description of a planning problem, then return a correct plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL) limitation of the paper: In this paper, we do not ask the LLM to recognize that it has been posed a prompt that is suitable for processing using the proposed LLM+P pipeline. Some key terms limitation of LLMs\nLLMs have become amazingly proficient at linguistic competence — knowing how to say things; but they are not nearly as good at functional competence — knowing what to say. Architecture\nassumptions\nA chatbot knows when to trigger LLM+P based on its conversation with a human user A domain PDDL file is provided for the problem the user asks for A simple problem description in natural language and its corresponding problem PDDL file are also provided beforehand. (used as context) Results\nPotential future work ALFRED contains the exploration process. how can ensure that all the required objects are found during exploration ","permalink":"https://sino-huang.github.io/posts/bo_liu-llmp-empowering-large-language-models-with-optimal-planning-proficiency-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: LLM+P Empowering Large Language Models With Optimal Planning Proficiency\u003c/li\u003e\n\u003cli\u003eAuthor: Bo Liu\u003c/li\u003e\n\u003cli\u003ePublish Year: 5 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 22, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2304.11477.pdf\"\u003ehttps://arxiv.org/pdf/2304.11477.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHowever, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal plans.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eintroduce LLM+P, it takes in a natural language description of a planning problem, then return a correct plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003elimitation of the paper\u003c/strong\u003e: In this paper, we do not ask the LLM to recognize that it has been posed a prompt that is suitable for processing using the proposed LLM+P pipeline.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of LLMs\u003c/strong\u003e\u003c/p\u003e","title":"Bo_liu Llmp Empowering Large Language Models With Optimal Planning Proficiency 2023"},{"content":"[TOC]\nTitle: Distilling Script Knowledge From Large Language Models for Constrainted Language Planning Author: Siyu Yuan et. al. Publish Year: 18 May 2023 Review Date: Mon, May 22, 2023 url: https://arxiv.org/pdf/2305.05252.pdf Summary of paper Motivation to accomplish everyday goals, human usually plan their actions in accordance with step-by-step instructions, such instruction are discovered as goal-oriented scripts. In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Contribution the dataset Experiments show that, when trained on CoScript, smaller models such as T5 (Raffel et al., 2020) can achieve good performance, even surpassing that of LLMs Some key terms limitation of previous work\nOur empirical study finds that LLMs tend to plan fluently but unfaithfully to the constraints. methodology\nThus, we employ an over-generate-then-filter approach (Wiegreffe et al., 2022) to satisfy the quality of the generated scripts to constraints. Filtering process\ndue to the diverse expressions of language, we rely not on rules and patterns, but on the semantic similarity between goals and scripts for filtering. we first collect a set of goals, consisting of the target goal as positive sample and others generated from the same abstract goal (they are hard negatives then). we convert scripts and goals into instructGPT embeddings (text-embedding-ada-002) and calculate cosine similarity as similarity scores to measure semantic similarity. we only keep the script if the the positive pairs scores the highest in the goal set. Assumption: the corresponding goal and script embeddings should be similar. types of constraints (definition)\nGood things about the paper (one paragraph) Major comments it releases Coscript dataset Potential future work over-generate-then-filter approach might be a good way for LLMs ","permalink":"https://sino-huang.github.io/posts/siyu_yuan-distilling-script-knowledge-from-large-language-models-for-constrainted-language-planning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Distilling Script Knowledge From Large Language Models for Constrainted Language Planning\u003c/li\u003e\n\u003cli\u003eAuthor: Siyu Yuan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 18 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 22, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.05252.pdf\"\u003ehttps://arxiv.org/pdf/2305.05252.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eto accomplish everyday goals, human usually plan their actions in accordance with step-by-step instructions, such instruction are discovered as goal-oriented scripts.\u003c/li\u003e\n\u003cli\u003eIn this paper, we define the task of constrained language planning for\nthe first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, \u003cem\u003eCoScript\u003c/em\u003e, which consists of 55,000\nscripts.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe dataset\u003c/li\u003e\n\u003cli\u003eExperiments show that, when trained on CoScript, smaller models such as T5 (Raffel et al., 2020) can achieve good performance, even surpassing that of LLMs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of previous work\u003c/strong\u003e\u003c/p\u003e","title":"Siyu_yuan Distilling Script Knowledge From Large Language Models for Constrainted Language Planning 2023"},{"content":"[TOC]\nTitle: BLIP Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation 2022 Author: Junnan Li et. al. Publish Year: 15 Feb 2022 Review Date: Mon, May 22, 2023 url: https://arxiv.org/pdf/2201.12086.pdf Summary of paper Motivation performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision Contribution BLIP effectively utilises the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. Some key terms Architecture\nCapFilt\nmotivation: the al-texts often do not accurately describe the visual content of the images, making them a noisy signal that is suboptimal for learning vision-language alignment Specifically, the captioner is an image-grounded text decoder. It is finetuned with the LM objective to decode texts given images. Given the web images Iw, the captioner generates synthetic captions Ts with one caption per image. The filter is an image-grounded text encoder. It is finetuned with the ITC and ITM objectives to learn whether a text matches an image. The filter removes noisy texts in both the original web texts Tw and the synthetic texts Ts, where a text is considered to be noisy if the ITM head predicts it as unmatched to the image. Finally, we combine the filtered image-text pairs with the human-annotated pairs to form a new dataset, which we use to pre-train a new model. ","permalink":"https://sino-huang.github.io/posts/junnan_li-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-understanding-and-generation-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  BLIP Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation 2022\u003c/li\u003e\n\u003cli\u003eAuthor: Junnan Li et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 15 Feb 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 22, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2201.12086.pdf\"\u003ehttps://arxiv.org/pdf/2201.12086.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eperformance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBLIP effectively utilises the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/p\u003e","title":"Junnan_li BLIP Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation 2022"},{"content":"[TOC]\nTitle: Natural Language Decomposition and Interpretation of Complex Utterances Author: Jacob Andreas Publish Year: 15 May 2023 Review Date: Mon, May 22, 2023 url: https://arxiv.org/pdf/2305.08677.pdf Summary of paper Motivation natural language interface often require supervised data to translate user request into structure intent representations however, during data collection, it can be difficult to anticipate and formalise the full range of user needs we introduce an approach for equipping a simple language to code model to handle complex utterances via a process of hierarchical natural language decomposition. Contribution Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches. Some key terms Methodology\nan approach called DECOMP, which decomposes complex utterances into a sequence of simpler natural language steps, each resembling an elementary utterance that an existing parser can handle, then uses this parser to map each elementary utterance to a sub-program. Good things about the paper (one paragraph) introduce new dataset that map utterances to programs : DeCU\nDeCU contains 872 elementary utterances paired with programs.\nExample prompt Top of the prompt is elementary utterances similar to the task Bottom of the prompt is the complex utterance decomposition examples the decomposition examples provide a demonstration of how to generate program fragments for a step conditioned on previous steps and help bridge any possible domain shift from elementary to complex utterances. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 Utterance : Find all meetings happening this week Program : val s1 = findEvents ( queryAt (` this `[ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : How many days in the next week do I have meetings ? Program : val s = List [ DayOfWeek ]( Sunday , Monday , Tuesday , Wednesday , Thursday , Friday , Saturday ). filter ( day = \u0026gt; findEvents ( queryAt ( day /\u0026amp; ( next [ Interval [ Date ]] and isWeek ))). nonEmpty ). size \u0026lt;EOS \u0026gt; Utterance : Find the All hands that is on next Monday . Program : val s2 = theEvent ( called (\u0026#34; All hands \u0026#34;) and queryAt ( theDate ( next [ Date ] /\u0026amp; Monday ))) \u0026lt;EOS \u0026gt; Utterance : Who all are on the Vacation meet ? Program : val s = theEvent ( called (\u0026#34; Vacation \u0026#34;)). attendees . all \u0026lt;EOS \u0026gt; Utterance : Reschedule all of list s2 to next week Program : val s3 = s2 . map (x = \u0026gt; modifyEvent (x , startsAt (( next [ Interval [ Date ]] and isWeek )))) \u0026lt;EOS \u0026gt; Utterance : Create a list of the days in a week Program : val s1 = List [ DayOfWeek ]( Monday , Tuesday , Wednesday , Thursday , Friday , Saturday , Sunday ) \u0026lt;EOS \u0026gt; Utterance : Do we have vacation all day this time ? Program : val s = theEvent ( called (\u0026#34; vacation \u0026#34;)). isAllDay \u0026lt;EOS \u0026gt; Utterance : Find event next week titled \u0026#39; sync call \u0026#39; Program : val s1 = theEvent ( called (\u0026#34; sync call \u0026#34;) and queryAt ( next [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Find all meetings with my reports this week that are an hour long . Program : val s1 = findEvents ( with_ ( me . directReports ) and queryAt (` this `[ Interval [ Date ]] and isWeek ) and lastsFor (1. hours )) \u0026lt;EOS \u0026gt; Utterance : Find all meetings on Monday . Program : val s1 = findEvents ( queryAt ( Monday )). head \u0026lt;EOS \u0026gt; Utterance : Find all meetings on Tuesday . Program : val s2 = findEvents ( queryAt ( Tuesday )). head \u0026lt;EOS \u0026gt; Utterance : Find all meetings on calendar Program : val s1 = findEvents0 \u0026lt;EOS \u0026gt; Utterance : Find all of the user \u0026#39;s reports . Program : val s1 = me . directReports \u0026lt;EOS \u0026gt; Utterance : Find all one on one events from last week and return the size of that list . Program : val s1 = findEvents ( queryAt ( last [ Interval [ Date ]] and isWeek ) and isOneOnOne ). size \u0026lt;EOS \u0026gt; Utterance : What do I have next week ? Program : val s = findEvents ( queryAt ( next [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Find emails from last week Program : val s = findEmails ( messageSentAt ( last [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Find events with Abby this week Program : val s1 = findEvents ( with_ (\u0026#34; Abby \u0026#34;) and queryAt (` this `[ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Find meeting with Hao this week . Program : val s1 = theEvent ( with_ (\u0026#34; Hao \u0026#34;) and queryAt (` this `[ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Find the next meeting with Jack Program : val s1 = theEvent ( next [ Event ] and with_ (\u0026#34; Jack \u0026#34;) ) \u0026lt;EOS \u0026gt; Utterance : Find meetings called All - Hands . Program : val s = findEvents ( called (\u0026#34; All - Hands \u0026#34;) ) \u0026lt;EOS \u0026gt; Utterance : Find all events happening today Program : val s2 = findEvents ( queryAt ( today )) \u0026lt;EOS \u0026gt; Utterance : What all day events do I have this week ? Program : val s = findEvents ( isAllDay and queryAt (` this `[ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Check my availability for next week Program : val s = availabilityIncludingMe ( createAt ( next [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Create a meeting with Jenn for next week Program : val s = createEvent ( with_ (\u0026#34; Jenn \u0026#34;) and createAt ( next [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Can you find the emails from Lisa from last week Program : val s = findEmails ( messageWithSender (\u0026#34; Lisa \u0026#34;) and messageSentAt ( last [ Interval [ Date ]] and isWeek )) \u0026lt;EOS \u0026gt; Utterance : Make me a meeting at 3 pm mountain time and refuse any meetings that happen at the same time Step 1: Grab 3 pm mountain time Program 1: val s1 = 3. pm inZone TimeZone (\u0026#34; MST \u0026#34;) Step 2: Create an event using time and timezone s1 Program 2: val s2 = createEvent ( createAt ( s1 )) Step 3: Find events using time and timezone s1 Program 3: val s3 = findEvents ( queryAt ( s1 )) Step 4: Decline each event in list s3 Program 4: val s4 = s3 . map (x = \u0026gt; respond (x , withResponse ( ResponseStatusType . declined ))) \u0026lt;EOS \u0026gt; Utterance : Change my meetings with Abby and those with Dan this week to start 5 minutes later Step 1: Find events with Abby this week Program 1: val s1 = findEvents ( with_ (\u0026#34; Abby \u0026#34;) and queryAt (` this `[ Interval [ Date ]] and isWeek )) Step 2: Find events with Dan and without Abby this week Program 2: val s2 = findEvents ( with_ (\u0026#34; Dan \u0026#34;) and not ( with_ (\u0026#34; Abby \u0026#34;) ) and queryAt (` this `[ Interval [ Date ]] and isWeek )) Step 3: Set all meetings from s1 to start 5 minutes later Program 3: val s3 = s1 . map (x = \u0026gt; modifyEvent (x , startsAt (x. start . local . time + 5. minutes ))) Step 4: Set all meetings from s2 to start 5 minutes later Program 4: val s4 = s2 . map (x = \u0026gt; modifyEvent (x , startsAt (x. start . local . time + 5. minutes ))) \u0026lt;EOS \u0026gt; Utterance : Can you decline all the meetings where at least one attendee reports to Jeff . Step 1: Grab Jeff \u0026#39;s reports Program 1: val s1 = thePerson (\u0026#34; Jeff \u0026#34;). directReports Step 2: Decline all events in which each person in list s2 is invited to Program 2: val s2 = s1 . map (x = \u0026gt; findEvents ( with_ (x)). map (z = \u0026gt; respond (z , withResponse ( ResponseStatusType . declined )))) \u0026lt;EOS \u0026gt; Utterance : Can you tell me how many emails I received on Monday ? Step 1: Query for how many emails were received on last Monday . Program 1: val s1 = findEmails ( messageSentAt ( last [ Date ] /\u0026amp; Monday )). size \u0026lt;EOS \u0026gt; Utterance : Do I have a meeting with Ben on Monday or with Charlie on Wednesday ? Step 1: Do I have a meeting with Ben on Monday or with Charlie on Wednesday . Program 1: val s1 = findEvents ( with_ (\u0026#34; Ben \u0026#34;) and queryAt ( Monday )). nonEmpty || findEvents ( queryAt ( Wednesday ) and with_ (\u0026#34; Charlie \u0026#34;)). nonEmpty \u0026lt;EOS \u0026gt; Utterance : how long do i have before my next appointment ? Step 1: Describe duration of time from now until next Meeting Program 1: val s1 = durationFromNow ( theEvent ( next [ Event ]) . start ) \u0026lt;EOS \u0026gt; Utterance : How many 1/1 meetings in total I had in the last week ? Step 1: Find all one on one events from last week Program 1: val s1 = findEvents ( queryAt ( last [ Interval [ Date ]] and isWeek ) and isOneOnOne ) Step 2: Get the size of that list s1 . Program 2: val s2 = s1 . size \u0026lt;EOS \u0026gt; Utterance : If I have a meeting with Henry this week , change it to be with Nick Step 1: Find meeting with Henry this week . Program 1: val s1 = theEvent ( with_ (\u0026#34; Henry \u0026#34;) and queryAt (` this `[ Interval [ Date ]] and isWeek )) Step 2: Update s1 to replace Henry with Nick , otherwise do nothing Program 2: val s2 = modifyEvent (s1 , with_ (\u0026#34; Nick \u0026#34;) and not ( with_ (\u0026#34; Henry \u0026#34;))) \u0026lt;EOS \u0026gt; Utterance : I need to swap the calls that are on Monday and Tuesday . Step 1: Find all meetings on Monday . Program 1: val s1 = findEvents ( queryAt ( Monday )). head Step 2: Find all meetings on Tuesday . Program 2: val s2 = findEvents ( queryAt ( Tuesday )). head Step 3: Swap the date for all meetings found in s1 with Tuesday . Program 3: val s3 = modifyEvent (s1 , startsAt ( Tuesday )) Step 4: Swap the start date for all meetings found in s2 with Monday . Program 4: val s4 = modifyEvent (s2 , startsAt ( Monday )) \u0026lt;EOS \u0026gt; Utterance : Change the duration of the team sync call today to two hours , and schedule an event with identical title 10 days from now . Step 1: Change the duration of the team sync call today to 2 hours Program 1: val s1 = modifyEvent ( theEvent ( called (\u0026#34; team sync call \u0026#34;) and queryAt ( today )) , lastsFor (2. hours )) Step 2: create an event with the same title as s1 10 days from now Program 2: val s2 = createEvent ( createAt ( today + 10. days ) and called (\u0026#34; team sync call \u0026#34;)) \u0026lt;EOS \u0026gt; Utterance : Check the upcoming meetings to find out any conflicts with vacation next week Step 1: Find all meetings next week Program 1: val s1 = findEvents ( queryAt ( next [ Interval [ Date ]] and isWeek )) Step 2: Find vacation next week ","permalink":"https://sino-huang.github.io/posts/harsh_jhamtani-natural-language-decomposition-and-interpretation-of-complex-utterances-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Natural Language Decomposition and Interpretation of Complex Utterances\u003c/li\u003e\n\u003cli\u003eAuthor: Jacob Andreas\u003c/li\u003e\n\u003cli\u003ePublish Year: 15 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 22, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.08677.pdf\"\u003ehttps://arxiv.org/pdf/2305.08677.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230522105001304\" loading=\"lazy\" src=\"/posts/harsh_jhamtani-natural-language-decomposition-and-interpretation-of-complex-utterances-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003enatural language interface often require supervised data to translate user request into structure intent representations\u003c/li\u003e\n\u003cli\u003ehowever, during data collection, it can be difficult to anticipate and formalise the full range of user needs\u003c/li\u003e\n\u003cli\u003ewe introduce an approach for equipping a simple language to code model to handle complex utterances via a process of hierarchical natural language decomposition.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eExperiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMethodology\u003c/strong\u003e\u003c/p\u003e","title":"Harsh_jhamtani Natural Language Decomposition and Interpretation of Complex Utterances 2023"},{"content":"[TOC]\nTitle: Segment Anything Author: Alexander Kirillov et. al. Publish Year: 5 Apr 2023 Review Date: Sun, May 21, 2023 url: https://arxiv.org/pdf/2304.02643.pdf Summary of paper Motivation we introduce the segment anything project: a new task, model and dataset for image segmentation.\nUsing the model in a data collection loop, we built the largest segmentation dataset to date.\nContribution the model is designed and trained to be promptable, so it can transfer zero-shot to new images distributions and tasks. background CLIP and ALIGN use contrastive learning to train text and image encoders that align the two modalities. goal of the authors\nbuild a foundation model for image segmentation seek to develop a promptable model and pre-trained it on a broad dataset Some key terms the plan hinges on three components\ntask, model, and data what task will enable zero-shot generalisation what is the corresponding model architecture what data can power this task and model promptable segmentation mask\na prompt simply specify what to segment in an image model for the promptable segmentation task\nthe model must support flexible prompts and must be ambiguity-aware Segment Anything task\nwe start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground /background points, a rough box or mask, free-form text the requirement of a valid mask simply means that even when a prompt is ambiguous and could refer to multiple objects, the output should be a reasonable mask for at least one of those objects. this is similar to expecting a language model to output a coherent response to an ambiguous prompt. interactive segmentation\ninteractive segmentation is a technique for picking objects of interest in images according to users\u0026rsquo; input interactions. Resolving ambiguity\nWith one output, the model will average multiple valid masks if given an ambiguous prompt. To address this, we modify the model to predict multiple output masks for a single prompt this require human annotation though during training, we backprop only minimum loss over masks. Potential future work linear classifier might be a good way to map output tokens to segmentations. tackle ambiguity issue might be helpful ","permalink":"https://sino-huang.github.io/posts/alexander_kirillov-segment-anything-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Segment Anything\u003c/li\u003e\n\u003cli\u003eAuthor: Alexander Kirillov et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 5 Apr 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, May 21, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2304.02643.pdf\"\u003ehttps://arxiv.org/pdf/2304.02643.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230521115752020\" loading=\"lazy\" src=\"/posts/alexander_kirillov-segment-anything-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewe introduce the segment anything project: a new task, model and dataset for image segmentation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsing the model in a data collection loop, we built the largest segmentation dataset to date.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe model is designed and trained to be promptable, so it can transfer zero-shot to new images distributions and tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"background\"\u003ebackground\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCLIP and ALIGN use contrastive learning to train text and image encoders that align the two modalities.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003egoal of the authors\u003c/strong\u003e\u003c/p\u003e","title":"Alexander_kirillov Segment Anything 2023"},{"content":"[TOC]\nTitle: ImageBind One Embedding Space to Bind Them All Author: Rohit Girdhar et. al. Publish Year: 9 May 2023 Review Date: Mon, May 15, 2023 url: https://arxiv.org/pdf/2305.05665.pdf Summary of paper Motivation we present ImageBind, an approach to learn a joint embedding across six different modalities ImageBind can leverage recent large scale vision-language models, and extend their zero shot capabilities to new modalities just using their natural pairing with images. Contribution we show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. Some key terms multimodality binding\nideally, for a single joint joint embedding space, visual features should be learned by aligning to all these sensors however, this requires acquiring all types and combinations of paired data with same set of images, which is infeasible. for existing models, the final embeddings are limited to the pairs of modalities used for training. Thus, video-audio embeddings cannot directly used for image-text tasks and vice versa. ImageBind method\nwe show that just aligning each modality\u0026rsquo;s embedding to image embeddings leads to an emergent alignment across all of modalities. ImageBind outperforms specialist models trained with direct data pair supervision. The goal is to learn a single joint embedding space for all modalities by using images to bind them together. we align each modalities\u0026rsquo;s embedding to image embeddings. The objective loss is InfoNCE\n","permalink":"https://sino-huang.github.io/posts/rohit_gridhar-imagebind-one-embedding-space-to-bind-them-all-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: ImageBind One Embedding Space to Bind Them All\u003c/li\u003e\n\u003cli\u003eAuthor: Rohit Girdhar et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 9 May 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, May 15, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2305.05665.pdf\"\u003ehttps://arxiv.org/pdf/2305.05665.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe present ImageBind, an approach to learn a joint embedding across six different modalities\u003c/li\u003e\n\u003cli\u003eImageBind can leverage recent large scale vision-language models, and extend their zero shot capabilities to new modalities just using their natural pairing with images.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003emultimodality binding\u003c/strong\u003e\u003c/p\u003e","title":"Rohit_gridhar Imagebind One Embedding Space to Bind Them All 2023"},{"content":"[TOC]\nTitle: Hierarchical Temporal Aware Video Language Pre Training Author: Qinghao Ye, Fei Huang et. al. Publish Year: 30 Dec 2022 Review Date: Thu, Apr 6, 2023 url: https://arxiv.org/pdf/2212.14546.pdf Summary of paper Motivation most previous methods directly inherit or adapt typical image-language pre-training paradigms to video-language pretraining, thus not fully exploiting the unique characteristic of video, i.e., temporal. Contribution this paper, the two novel pretraining tasks for modeling cross-modal alignment between moments and texts as well as the temporal relations of video-text pairs. specifically, we propose a cross-modal moment exploration task to explore moments in videos, which results in detailed video moment representations besides, the inherent temporal relations are capture by alignment video-text pairs as a whole in different time resolutions with multimodal temporal relation exploration tasks Some key terms limitation of previous work\nthey treat video within global perspective, thus failing to consider fine-grained temporal information and relations which are essential to video-language pre-training directly treating the video globally has two main limitations less effective in modelling the fine-grained moment information including atomic actions and moments so, we vary time resolution and generate two views (long and short) for the input video. As a result, the short view video clip tends to represent the moment information and the long-view video may express more event-level information e.g., the short view video clip only describes the moment of \u0026ldquo;lick fingers\u0026rdquo; rather than \u0026ldquo;eating ice cream\u0026rdquo;. ignoring the temporal relations implicitly existed in the video. Knowing the event expressed by the text, the moment \u0026ldquo;eating ice cream\u0026rdquo; can be inferred from the moment \u0026ldquo;lick fingers\u0026rdquo; shown by short-view video. METHOD cross-modal moment exploration (CME)\nwe first generate long-view and short-view videos with different time resolutions to build hierarchy of the input video. then, based on the similarities of words and short-view videos, we select the most relevant words as positive and leave the rest of words as hard negatives The CME pre-training task is applied to align the positive words and short-view video representations in the same embedding space multimodal temporal exploration (MTRE)\nto capture association between moments and the event, we match different views for the same video\nhowever, directly matching two views visually would be noisy due to the background similarity MTRE -\u0026gt; the short view video guided by most relevant words and the long-view video guided by text will be aligned.\nwe aim to minimizing the negative cosine similarity\nPotential future work Models and demo are available on ModelScope.\nhttps://github.com/modelscope/modelscope\n","permalink":"https://sino-huang.github.io/posts/qinghao_hitea-hierarchical-temporal-aware-video-language-pre-training-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Hierarchical Temporal Aware Video Language Pre Training\u003c/li\u003e\n\u003cli\u003eAuthor: Qinghao Ye, Fei Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 30 Dec 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Apr 6, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2212.14546.pdf\"\u003ehttps://arxiv.org/pdf/2212.14546.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230406100437893\" loading=\"lazy\" src=\"/posts/qinghao_hitea-hierarchical-temporal-aware-video-language-pre-training-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003emost previous methods directly inherit or adapt typical image-language pre-training paradigms to video-language pretraining, thus not fully exploiting the unique characteristic of video, i.e., temporal.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis paper, the two novel pretraining tasks for modeling cross-modal alignment between moments and texts as well as the temporal relations of video-text pairs.\u003c/li\u003e\n\u003cli\u003especifically, we propose a cross-modal moment exploration task to explore moments in videos, which results in detailed video moment representations\u003c/li\u003e\n\u003cli\u003ebesides, the inherent temporal relations are capture by alignment video-text pairs as a whole in different time resolutions with multimodal temporal relation exploration tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of previous work\u003c/strong\u003e\u003c/p\u003e","title":"Qinghao_hitea Hierarchical Temporal Aware Video Language Pre Training 2022"},{"content":"[TOC]\nTitle: Guiding Pretraining in Reinforcement Learning With Large Language Models Author: Yuqing De, Jacob Andreas et. al. Publish Year: 13 Feb 2023 Review Date: Wed, Apr 5, 2023 url: https://arxiv.org/pdf/2302.06692.pdf Summary of paper Motivation intrinstically motivated exploration methods address sparse reward problem by rewarding agents for visiting novel states or transitions. Contribution we describe a method that uses background knowledge from text corpora to shape exploration. This method, call Exploring with LLMs, reward an agent for achieving goals suggested by a language model prompted with a description of agent\u0026rsquo;s current state. Some key terms How does ELLM work\nELLM uses a pretrained large language model to suggest plausibly useful goals in a task-agnostic way. task-agnostic -\u0026gt; kind of exploration ELLM prompts an LLM to suggest possible goals given an agent\u0026rsquo;s current context and reward agents for accomplishing those suggestions. as a result, exploration is biased toward completion of goals that are diverse, context-sensitive and human-meaningful. ELLM-trained agents exhibit better coverage of useful behaviours during pretraining, and outperform or match baselines when fine-tuned on downstream tasks why intrinsically motivated exploration reward is not good\nnot everything novel or unpredictable is useful: noisy TVs and the movements of leaves on a tree may provide an infinite amount of novelty, but do not lead to meaningful behaviours. more recent approaches compute novelty in higher-level representation space such as language, but can continue driving the agent to explore behaviours that are highly unlikely to every be useful Method $C_{state}(o_t)$, $C$ is the state captioner $\\Delta$ is the cosine similarity function. we reward the agent for achieving any of the $k$ suggested goals by taking the maximum of the goal-specific rewards Potential future work we may want to reuse the state captioner for our future work check Appendix H of the paper ","permalink":"https://sino-huang.github.io/posts/jacob_andreas-guiding-pretraining-in-reinforcement-learning-with-llms-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Guiding Pretraining in Reinforcement Learning With Large Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Yuqing De, Jacob Andreas et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 13 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Apr 5, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.06692.pdf\"\u003ehttps://arxiv.org/pdf/2302.06692.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230405100408131\" loading=\"lazy\" src=\"/posts/jacob_andreas-guiding-pretraining-in-reinforcement-learning-with-llms-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eintrinstically motivated exploration methods address sparse reward problem by rewarding agents for visiting novel states or transitions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe describe a method that uses background knowledge from text corpora to shape exploration.\u003c/li\u003e\n\u003cli\u003eThis method, call Exploring with LLMs, reward an agent for achieving goals suggested by a language model prompted with a description of agent\u0026rsquo;s current state.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHow does ELLM work\u003c/strong\u003e\u003c/p\u003e","title":"Jacob_andreas Guiding Pretraining in Reinforcement Learning With Llms 2023"},{"content":"[TOC]\nTitle: Scaling Expert Language Models With Unsupervised Domain Discovery Author: Luke Zettlemoyer et. al. Publish Year: 24 Mar, 2023 Review Date: Mon, Apr 3, 2023 url: https://arxiv.org/pdf/2303.14177.pdf Summary of paper Contribution we introduce a simple but efficient method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalise embarrassingly parallel training by automatically discovering the domain for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Some key terms Cluster-Branch-Train-Merge (C-BTM)\nWe use unsupervised clustering to discover domains in a corpus, and train an ELM on each cluster independently.\nAt inference time, we sparsely activate a subset of the trained ELMs. We ensemble ELMs by weighting their output with the distances between an embedding of the current context and each expert\u0026rsquo;s cluster center.\n","permalink":"https://sino-huang.github.io/posts/luke_zettlemoyer-scaling-expert-language-models-with-unsupervised-domain-discovery-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Scaling Expert Language Models With Unsupervised Domain Discovery\u003c/li\u003e\n\u003cli\u003eAuthor: Luke Zettlemoyer et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 24 Mar, 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Apr 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2303.14177.pdf\"\u003ehttps://arxiv.org/pdf/2303.14177.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230403152538135\" loading=\"lazy\" src=\"/posts/luke_zettlemoyer-scaling-expert-language-models-with-unsupervised-domain-discovery-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe introduce a simple but efficient method to asynchronously train large, sparse language models on arbitrary text corpora.\u003c/li\u003e\n\u003cli\u003eOur method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.\u003c/li\u003e\n\u003cli\u003eThis approach generalise embarrassingly parallel training by automatically discovering the domain for each expert, and eliminates nearly all the communication overhead of existing sparse language models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCluster-Branch-Train-Merge (C-BTM)\u003c/strong\u003e\u003c/p\u003e","title":"Luke_zettlemoyer Scaling Expert Language Models With Unsupervised Domain Discovery 2023"},{"content":"[TOC]\nTitle: How Robust Is GPT 3.5 to Predecessors a Comprehensive Study on Language Understanding Tasks Author: Xuanting Chen et. al. Publish Year: 2023 Review Date: Mon, Apr 3, 2023 url: https://arxiv.org/ftp/arxiv/papers/2303/2303.00293.pdf Summary of paper Motivation GPT3.5, their robustness, and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI Contribution Our study yielded the following findings by comparing GPT 3.5 with finetuned models competitive results on test sets: GPT3.5 achieves SOTA results in some NLU tasks compared to supervised models fine-tuned with task-specific data. In particular GPT-3.5 performs well in reading comprehension and sentiment analysis tasks, but face challenges in sequence tagging and relation extraction tasks. Lack of robustness: GPT-3.5 still encounter significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language inference and sentiment analysis tasks, respectively. However, it is worth noting that GPT3.5 achieves remarkable robustness on certain tasks, such as reading comprehension and WSC tasks Robustness instability: In few-shot scenarios, GPT-3.5’s robustness improvement varies greatly across different tasks. For example, GPT-3.5 shows significant improvement in aspect-based sentiment analysis tasks while the robustness actually decreases in natural language inference (Section 4.3.1) and semantic matching (Section 4.3.2) tasks. Prompt sensitivity: changes in input prompts have a significant impact on the results, and GPT-3.5\u0026rsquo;s robustness to prompt variations. still requires improvement. Number sensitivity: GPT3.5 is more sensitive to numerical inputs than pre-training fine-tuning models. For example, in the NumWord transformation, which involves replacing numerical words in sentences with different numerical values, GPT3.5 exhibits a significantly high level of sensitivity. Task labels sensitivity: we speculate that the task construction during the instruction tuning stage may significantly impact the model\u0026rsquo;s performance. In the case of IMDB binary sentiment classification dataset, the model outputs a large number of \u0026ldquo;neutral\u0026rdquo; responses, which are not included in the application label space, resulting in a performance drop Significant improvement in zero/few-shot scenarios: in zero-shot and few-shot scenario, GPT3.5 outperforms existing LLMs in most NLU tasks, especially in reading comprehension, natural language inference and semantic matching tasks Ability for in-context learning: Compared to 0-shot, GPT 3.5 performs better on most tasks in the 1-shot setting. Additionally, performance does no vary significantly between the 1-shot, 3-shot, 6-shot, 9-shot settings for most tasks. However, providing additional examples in the prompts can be advantageous for sequence tagging tasks ","permalink":"https://sino-huang.github.io/posts/xuanting_chen-how-robust-is-gpt35-to-predecessors-a-comprehensive-study-on-language-understanding-tasks/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: How Robust Is GPT 3.5 to Predecessors a Comprehensive Study on Language Understanding Tasks\u003c/li\u003e\n\u003cli\u003eAuthor: Xuanting Chen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Apr 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/ftp/arxiv/papers/2303/2303.00293.pdf\"\u003ehttps://arxiv.org/ftp/arxiv/papers/2303/2303.00293.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGPT3.5, their robustness, and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOur study yielded the following findings by comparing GPT 3.5 with finetuned models\u003c/li\u003e\n\u003cli\u003ecompetitive results on test sets: GPT3.5 achieves SOTA results in some NLU tasks compared to supervised models fine-tuned with task-specific data. In particular GPT-3.5 performs well in reading comprehension and sentiment analysis tasks, but face challenges in sequence tagging and relation extraction tasks.\u003c/li\u003e\n\u003cli\u003eLack of robustness: GPT-3.5 still encounter significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language inference and sentiment analysis tasks, respectively. However, it is worth noting that GPT3.5 achieves remarkable robustness on certain tasks, such as reading comprehension and WSC tasks\u003c/li\u003e\n\u003cli\u003eRobustness instability:  In few-shot scenarios, GPT-3.5’s robustness improvement varies greatly across different tasks. For example, GPT-3.5 shows significant improvement in aspect-based sentiment analysis tasks while the robustness actually decreases in natural language inference (Section 4.3.1) and semantic matching (Section 4.3.2) tasks.\u003c/li\u003e\n\u003cli\u003ePrompt sensitivity: changes in input prompts have a significant impact on the results, and GPT-3.5\u0026rsquo;s robustness to prompt variations. still requires improvement.\u003c/li\u003e\n\u003cli\u003eNumber sensitivity: GPT3.5 is more sensitive to numerical inputs than pre-training fine-tuning models. For example, in the NumWord transformation, which involves replacing numerical words in sentences with different numerical values, GPT3.5 exhibits a significantly high level of sensitivity.\u003c/li\u003e\n\u003cli\u003eTask labels sensitivity: we speculate that the task construction during the instruction tuning stage may significantly impact the model\u0026rsquo;s performance. In the case of IMDB binary sentiment classification dataset, the model outputs a large number of \u0026ldquo;neutral\u0026rdquo; responses, which are not included in the application label space, resulting in a performance drop\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSignificant improvement in zero/few-shot scenarios:\u003c/strong\u003e in zero-shot and few-shot scenario, GPT3.5 outperforms existing LLMs in most NLU tasks, especially in reading comprehension, natural language inference and semantic matching tasks\u003c/li\u003e\n\u003cli\u003eAbility for in-context learning: Compared to 0-shot, GPT 3.5 performs better on most tasks in the 1-shot setting. Additionally, performance does no vary significantly between the 1-shot, 3-shot, 6-shot, 9-shot settings for most tasks. However, providing additional examples in the prompts\ncan be advantageous for sequence tagging tasks\u003c/li\u003e\n\u003c/ul\u003e","title":"Xuanting_chen How Robust Is GPT 3.5 to Predecessors a Comprehensive Study on Language Understanding Tasks"},{"content":"[TOC]\nTitle: A Picture Is Worth a Thousand Words Language Models Plan From Pixels Author: Anthony Liu et.al. Publish Year: 16 Mar 2023 Review Date: Mon, Apr 3, 2023 url: https://arxiv.org/pdf/2303.09031v1.pdf Summary of paper Motivation planning is a important capability of AI that perform long-horizon tasks in real-world environments. prior PLM based approaches for planning either assume observations are available in the form of text, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways. Contribution in contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM Some key terms why we need the ability to reason about plans\nThe ability to reason about plans is critical for performing long-horizon tasks (Erol, 1996; Sohn et al.,2018; Sharma et al., 2022), compositional generalisation (Corona et al., 2021) and generalisation to unseen tasks and environments (Shridhar et al.,2020). visually grounded planning with PLMs\nthe ability to adapt plans based on interaction and visual feedback from the environment. multimodal embedding\ndirectly inserting visual observations as PLM input embeddings the visual encoder and PLM are jointly trained for the target task, an approach we call Visual Prompt Planning (VP^2) by teaching the PLM to user observations for planning in an end to end manner, we remove the dependency on external data such as captions and affordability information that was used in prior work. Visual Prompt Planning if goal description, actions and observations are available in the form of discrete token sequences, predicting the next action is similar to a language modelling task and a PLM can be fine-tuned for next action prediction maximise $\\log p_{LM}(a_t | cxt_{t})$, where $cxt_{t} = concat(g, o_1, a_1, o_2, a_2,\u0026hellip;, a_{t-1}, o_t)$. However, observations may not be available in the form of text or discrete tokens in practice and we attempt to tackle this scenario observation encoder, we need to encode observations (image rather than video in this case) into a sequence of token and we have an end to end training objective this is for offline RL Potential future work this direct encoding visual observations into prompt is very convenient. ","permalink":"https://sino-huang.github.io/posts/anthony_liu-a-picture-is-worth-a-thousand-words-language-models-plan-from-pixels-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: A Picture Is Worth a Thousand Words Language Models Plan From Pixels\u003c/li\u003e\n\u003cli\u003eAuthor: Anthony Liu et.al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 16 Mar 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Apr 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2303.09031v1.pdf\"\u003ehttps://arxiv.org/pdf/2303.09031v1.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230403112936880\" loading=\"lazy\" src=\"/posts/anthony_liu-a-picture-is-worth-a-thousand-words-language-models-plan-from-pixels-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eplanning is a important capability of AI that perform long-horizon tasks in real-world environments.\u003c/li\u003e\n\u003cli\u003eprior PLM based approaches for planning either assume observations are available in the form of text, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ewhy we need the ability to reason about plans\u003c/strong\u003e\u003c/p\u003e","title":"Anthony_liu a Picture Is Worth a Thousand Words Language Models Plan From Pixels 2023"},{"content":"[TOC]\nTitle: Grounded Decoding Guiding Text Generation With Grounded Models for Robot Control Author: WenLong Huang et. al. Publish Year: 1 Mar, 2023 Review Date: Thu, Mar 30, 2023 url: https://arxiv.org/abs/2303.00855 Summary of paper Motivation Unfortunately, applying LLMs to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require. on the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them. Contribution thus if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realisable according to grounded models of the environment. we frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives. Potential future work the work is related to using LMs info as a prior bias the problem framing is straightforward ","permalink":"https://sino-huang.github.io/posts/wenlong_huang-grounded-decoding-guiding-text-generation-with-grounded-models-for-robot-control-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Grounded Decoding Guiding Text Generation With Grounded Models for Robot Control\u003c/li\u003e\n\u003cli\u003eAuthor: WenLong Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 1 Mar, 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 30, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2303.00855\"\u003ehttps://arxiv.org/abs/2303.00855\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230331172240565\" loading=\"lazy\" src=\"/posts/wenlong_huang-grounded-decoding-guiding-text-generation-with-grounded-models-for-robot-control-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUnfortunately, applying LLMs to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.\u003c/li\u003e\n\u003cli\u003eon the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethus if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realisable according to grounded models of the environment.\u003c/li\u003e\n\u003cli\u003ewe frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"potential-future-work\"\u003ePotential future work\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ethe work is related to using LMs info as a prior bias\u003c/li\u003e\n\u003cli\u003ethe problem framing is straightforward\u003c/li\u003e\n\u003c/ul\u003e","title":"Wenlong_huang Grounded Decoding Guiding Text Generation With Grounded Models for Robot Control 2023"},{"content":"[TOC]\nTitle: Learning Generative Models With Goal Conditioned Reinforcement Learning Author: Mariana Vargas Vieyra et. al. Publish Year: 26 Mar 2023 Review Date: Thu, Mar 30, 2023 url: https://arxiv.org/abs/2303.14811 Summary of paper Contribution we present a novel framework for learning generative models with goal-conditioned reinforcement learning we define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent) Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals. during training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals At inference we generate new samples with S-agent. Some key terms Goal-Conditioned Reinforcement Learning (GCRL) framework\nin GCRL the agent aims at a particular state called the goal. at each step, the environment yields a loss that accounts how far the agent is from the goal it is targeting (potential based reward shaping) intuitively, our learning procedure consists of training a family of goal-conditioned agent (GC-policy) that learn to reach the different elements in the training set by producing a trajectory of intermediate representations, departing from the fixed initial state (the alternative view for reverse diffusion process) at the same time, we obtain the generative model by learning a mixture policy of these goal-conditioned policies where the goal is sampled uniformly at random from the training set. the goal-conditioned agents are used for training only. At inference time, we generate trajectories with the mixture policy and collect the states reached at the final step. Algorithm S-agent generates proposal GC-agent selects proposal as action selection what is proposal\nis the action vector But in this setting, the agent need to select the best proposal among $A$ proposals and set $A$ changes for each step. Therefore, we traded a GCRL task with continuous action space for a non-stationary GCRL task with discrete action space. Potential future work The paper redefined the diffusion model as a goal-conditioned RL problem\n","permalink":"https://sino-huang.github.io/posts/mariana_learning-generative-models-with-goal-conditioned-reinforcement-learning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Learning Generative Models With Goal Conditioned Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Mariana Vargas Vieyra et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 26 Mar 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 30, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2303.14811\"\u003ehttps://arxiv.org/abs/2303.14811\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230330212401597\" loading=\"lazy\" src=\"/posts/mariana_learning-generative-models-with-goal-conditioned-reinforcement-learning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe present a novel framework for learning generative models with goal-conditioned reinforcement learning\u003c/li\u003e\n\u003cli\u003ewe define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent)\u003c/li\u003e\n\u003cli\u003eGiven a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals.\n\u003cul\u003e\n\u003cli\u003eduring training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals\u003c/li\u003e\n\u003cli\u003eAt inference we generate new samples with S-agent.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGoal-Conditioned Reinforcement Learning (GCRL) framework\u003c/strong\u003e\u003c/p\u003e","title":"Mariana_learning Generative Models With Goal Conditioned Reinforcement Learning 2023"},{"content":"[TOC]\nTitle: Deep RL With Hierarchical Action Exploration for Dialogue Generation Author: Itsugun Cho et. al. Publish Year: 22 Mar 2023 Review Date: Thu, Mar 30, 2023 url: https://arxiv.org/pdf/2303.13465v1.pdf Summary of paper Motivation Approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. Contribution this paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. we introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene the sampling. Some key terms limitation of the maximum likelihood estimation (MLE) objective for the probability distribution of responses\nHowever, this supervised technique is insufficient to learn a long-term behaviour since the corpus often contains suboptimal dialogues, and MLE cannot model the future direction of the conversation. if we instead view the open-domain dialogue as a control problem, frameworks such as reinforcement learning (RL) could allow agents automatically adjust policy concerning the pre-defined appraisal function via a trial-and-error process. word generation based on the elevated abstraction category\nif we apprehend which abstract category of actions can obtain a higher Q-value, then generating responses of that category for the greedy policy will make training more efficient. we designed a coarse-grained Q-function by category-represented responses aiming to lock the optimal category and a fine-grained Q-function by token-represented response striving to extract the optimal action. in this way, the infinite action space is divided into several blocks at the high-level abstraction and thus can ergodic entire action space to adapt policy on the fly. four reward functions\nthe cosine similarity between the agent\u0026rsquo;s response and dull responses (e.g., \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo;) . An expression that lack emotional engagement may limit the development of dialogue. the outpouring of surprise emotion (this is from the training dataset, the mood of the human user) the length of responses the asking questions (reinforce the agent to ask questions) Potential future work The paper showed that a coarse-grained approach is more efficient. ","permalink":"https://sino-huang.github.io/posts/itsugun_cho-deep-rl-with-hierarchical-action-exploration-for-dialogue-generation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Deep RL With Hierarchical Action Exploration for Dialogue Generation\u003c/li\u003e\n\u003cli\u003eAuthor: Itsugun Cho et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Mar 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 30, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2303.13465v1.pdf\"\u003ehttps://arxiv.org/pdf/2303.13465v1.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eApproximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental.\u003c/li\u003e\n\u003cli\u003ewe introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene the sampling.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of the maximum likelihood estimation (MLE) objective for the probability distribution of responses\u003c/strong\u003e\u003c/p\u003e","title":"Itsugun_cho Deep Rl With Hierarchical Action Exploration for Dialogue Generation 2023"},{"content":"[TOC]\nTitle: How to talk so AI will learn: Instructions, descriptions, and autonomy Author: Theodore R. Sumers et. al. Publish Year: NeurIPS 2022 Review Date: Wed, Mar 15, 2023 url: https://arxiv.org/pdf/2206.07870.pdf Summary of paper Motivation yet today, we lack computational models explaining such language use Contribution To address this challenge, we formalise learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviours. (obtain intent (preference) from the presentation (behaviour)) we show that instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently. We then define a pragmatic listener agent that robustly infers the speaker\u0026rsquo;s reward function by reasoning how the speaker expresses themselves. (language reward module?) we hope these insights facilitate a shift from developing agents that obey language to agents that learn from it. Some key terms two distinct types of language\ninstructions which provide information about the desired policy descriptions which provide information about the reward function Issues\nprior work has highlighted the difficulty of specifying our desires via numerical reward functions here, we explore language as a means to communicate them while most previous work on language input to AI systems focuses on instructions, we study instructions alongside more abstract, descriptive language from instructions to descriptions how we learn from reward functions from others\ninverse reinforcement learning However, natural language affords richer, under-explored forms of teaching\nfor example, an expert teaching a seminar might instead describe how to recognise edible or toxic mushrooms based on their features, thus providing highly generalised information (it means instead of the instruction, the text info also includes the rationales) discovery\nThe horizon quantifies notions of autonomy described in previous work. If the horizon is short, the agent is closely supervised; at longer horizons, they are expected to act more independently. We then analyse instructions (which provide a partial policy) and descriptions (which provide partial information about the reward function). We show that instructions are optimal at short horizons, while descriptions are optimal at longer ones. Limitations\nIn practice, it is difficult to specify a reward function to obtain desired behaviour, motivating learning the reward function from social input.\nmost social learning methods assume the expert is simply acting optimally, but recent pragmatic methods instead assume the expert is actively teaching.\nLearning reward functions from descriptions\nRather than expressing specific goals, reward-descriptive language encodes abstract information about preferences or the world. Other related lines of work use language which describes agent behaviours. However, a smaller body of work uses it for RL : by learning reward function directly from existing bodies of text or interactive, free-form language input. our work provides a formal model of such language in order to compare it with more typically studied instructions. Speaker model is essential\nwe show how a carefully specified speaker model, incorporating both instructions and descriptions, allows our pragmatic listener to robustly infer a speaker\u0026rsquo;s reward function. this allows our pragmatic listener to robustly infer a speaker\u0026rsquo;s reward function Reward assumption\nwe assume rewards are a linear function of these features (e.g., green mushrooms tend to be tasty) do not discuss the perturbed reward problem Formalising learning from language in contextual bandits\nH=1 means it is no more action sequence for H \u0026gt; 1, there is no more single-step action supervision difference between descriptions and instructions\ninstructions maps to actions descriptions maps to reward signals what is the point\nas the horizon lengthens, however, descriptions generalize better, thus allowing agents to solve unseen states based on the rationale information ","permalink":"https://sino-huang.github.io/posts/theodore_r_sumers-how-to-talk-so-ai-will-learn-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: How to talk so AI will learn: Instructions, descriptions, and autonomy\u003c/li\u003e\n\u003cli\u003eAuthor: Theodore R. Sumers et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: NeurIPS 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Mar 15, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2206.07870.pdf\"\u003ehttps://arxiv.org/pdf/2206.07870.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230315211204247\" loading=\"lazy\" src=\"/posts/theodore_r_sumers-how-to-talk-so-ai-will-learn-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eyet today, we lack computational models explaining such language use\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTo address this challenge, we formalise learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviours.\n\u003cul\u003e\n\u003cli\u003e(obtain intent (preference) from the presentation (behaviour))\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewe show that instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently.\u003c/li\u003e\n\u003cli\u003eWe then define a pragmatic listener agent that robustly infers the speaker\u0026rsquo;s reward function by reasoning how the speaker expresses themselves. (language reward module?)\u003c/li\u003e\n\u003cli\u003ewe hope these insights facilitate a shift from developing agents that obey language to agents that learn from it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003etwo distinct types of language\u003c/strong\u003e\u003c/p\u003e","title":"Theodore_r_sumers How to Talk So Ai Will Learn 2022"},{"content":"[TOC]\nTitle: Diffusion Policy Visuomotor Policy Learning via Action Diffusion Author: Cheng Chi et. al. Publish Year: 2023 Review Date: Thu, Mar 9, 2023 url: https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf Summary of paper Contribution introducing a new form of robot visuomotor policy that generates behaviour via a \u0026ldquo;conditional denoising diffusion process\u0026rdquo; on robot action space Some key terms Explicit policy\nlearning this is like imitation learning Implicit policy\naiming to minimise the estimation of the energy function learning this is like a standard reinforcement learning diffusion policy\nprovide a smooth gradient to refining action over each iteration. Method Diffusion policy\nin this formulation, instead of directly outputting an action, the policy infers the action-score gradient, conditioned on visual observations, for K demonising iterations Benefits of this approach\nExpressing multimodal action distributions. diffusion policy can express arbitrary normalizable distributions, which includes multimodal action distributions, a well-known challenge for policy learning High-dimensional output space this property allows the policy to jointly infer a sequence of future actions instead of single-step actions, which is critical for encouraging temporal action consistency Stable training Training energy-based policy (think about reinforcement learning) often requires negative sampling to estimate an intractable normalisation constant, which is known to cause training instability. Diffusion policy might be more stable. ","permalink":"https://sino-huang.github.io/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Diffusion Policy Visuomotor Policy Learning via Action Diffusion\u003c/li\u003e\n\u003cli\u003eAuthor: Cheng Chi et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 9, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf\"\u003ehttps://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230309193732709\" loading=\"lazy\" src=\"/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eintroducing a new form of robot visuomotor policy that generates behaviour via a \u0026ldquo;conditional denoising diffusion process\u0026rdquo; on robot action space\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eExplicit policy\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elearning this is like imitation learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImplicit policy\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eaiming to minimise the estimation of the energy function\u003c/li\u003e\n\u003cli\u003elearning this is like a standard reinforcement learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ediffusion policy\u003c/strong\u003e\u003c/p\u003e","title":"Cheng_chi Diffusion Policy Visuomotor Policy Learning via Action Diffusion 2023"},{"content":"[TOC]\nTitle: Framer: Planning Models From Natural Language Action Descriptions Author: Alan Lindsay et. al. Publish Year: 2017 Review Date: Thu, Mar 9, 2023 url: https://core.ac.uk/download/pdf/322329049.pdf Summary of paper Motivation for modelling assisting and model generation tools, there is a underlying assumption that the user can formulate the problem using some formal language. this motivates us to generate planning domain models directly from NL descriptions. Some key terms approach\nwe start from NL descriptions of actions and use NL analysis to construct structured representation, from which we construct formal representations of action sequences ? only action sequence? what about the environment the generated action sequence provide the necessary structured input for inducing a PDDL domain, using domain model acquisition technology. we use an estimate of functional similarity, so sentences that describe similar behaviour are represented by the same planning operator. problem modelling\nModelling problems appropriately for use by a computer program has been identified as a key bottleneck in the exploitation of various AI technology. in Automated Planning, this has inspired a growing body of work that aims to support the modelling process including domain acquisition tools. PDDL\ncheck 2.1 domain and problem definition section of the introduction of PDDL book PDDL divides the definition of a planning problem into two parts: the domain defines the state variables (facts that may be true or false) and actions, while the problem defines initial state and the goal condition. the domain definition is a general model of the relevant aspects of the world in which we are planning, while the problem definition is a specific problem instance in this domain that specifies where we begin and what we must achieve. the :predicates section of the domain definition contains the list of the model\u0026rsquo;s state variables. These are binary variables, meaning they represent facts that are either true or false. PDDL versions\nPDDL 2.1 and + : extends the numeric and temporal representation to hybrid planning PDDL 2.2: add Axioms, which add more expressive conditions to classical planning, and timed initial literals, which provide a syntactic convenience for defining a schedule of predictable events in temporal planning PDDL 3.0: add syntax for temporally extended goals and preferences to classical planning PDDL 3.1: defined the restricted syntax for specifying action costs Domain Model acquisition tool LOCM2\nLOCM2 is used to generate domain file from the action sequence why domain file domain file specifies the action info and the state variables info( i.e., predicates) Potential future work maybe we can generate a more complicated PDDL domain file based on the low-level descriptions yes we may generate useful action information based on the collected NL descriptions it might be difficult to generate predicates and types also, the initial exploration is to generate the initial state of the PDDL problem definition, there should not be ambiguous initial state information. (? or there could be, ask Nir about this idea) ","permalink":"https://sino-huang.github.io/posts/alan_lindsay-framer-planning-models-from-natural-language-action-descriptions-2017/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Framer: Planning Models From Natural Language Action Descriptions\u003c/li\u003e\n\u003cli\u003eAuthor: Alan Lindsay et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 9, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://core.ac.uk/download/pdf/322329049.pdf\"\u003ehttps://core.ac.uk/download/pdf/322329049.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230309192920549\" loading=\"lazy\" src=\"/posts/alan_lindsay-framer-planning-models-from-natural-language-action-descriptions-2017/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003efor modelling assisting and model generation tools, there is a underlying assumption that the user can formulate the problem using some formal language.\u003c/li\u003e\n\u003cli\u003ethis motivates us to generate planning domain models directly from NL descriptions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eapproach\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ewe start from NL descriptions of actions and use NL analysis to construct structured representation, from which we construct formal representations of action sequences\n\u003col\u003e\n\u003cli\u003e\u003cem\u003e? only action sequence? what about the environment\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ethe generated action sequence provide the necessary structured input for inducing a PDDL domain, using domain model acquisition technology.\u003c/li\u003e\n\u003cli\u003ewe use an estimate of functional similarity, so sentences that describe similar behaviour are represented by the same planning operator.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eproblem modelling\u003c/strong\u003e\u003c/p\u003e","title":"Alan_lindsay Framer Planning Models From Natural Language Action Descriptions 2017"},{"content":"[TOC]\nTitle: Language-Driven Representation Learning for Robotics Author: Siddharth Karamcheti et. al. Publish Year: 24 Feb 2023 Review Date: Fri, Mar 3, 2023 url: https://arxiv.org/pdf/2302.12766.pdf Summary of paper Motivation recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control but robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration amongst others. Contribution first, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite (i.e., high-level semantics) We then introduce Voltron, a framework for language driven representation learning from human videos and associated captions. Voltron trades off language conditioned visual reconstruction to learn low-level visual patterns (mask auto-encoding) and visually grounded language generation to encode high-level semantics. (hindsight relabelling and contrastive learning) Some key terms How can we learn visual representations that generalise across the diverse spectrum of problems in robot learning?\nrecent approaches for learning visual representations for robotics use pretraining objectives that reflect different inductive biases for what the learned representations should capture. Masked Visual Pretraining proposes using masked autoencoding to prioritise visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction Separately, resusable representations for robotic manipulation eschews pixel reconstruction for two contrastive learning objectives: time contrastive learning and video language alignment (hindsight relabeling, phrase corruption) Voltron Framework central to our approach is language-driven learning on top of a masked autoencoding backbone. We sample a mask once, and apply it uniformly across all frames in the video to prevent leakage (in video) A Voltron model comprises a multimodal encoder that takes in a visual context and (optional) language utterance producing a dense representation a visual reconstructor that attempts to reconstruct the masked-out visual context from the encoder\u0026rsquo;s representation of what is visible and a language generator that predicts the language annotation for the video given the encoded visual context. when can we focus on the high-level semantic features\nwhen we favour language generation, we need the ability to comprehend high-level semantic features. what is $\\alpha$ balance of language conditioning \u0026amp; language generation for each video caption pairs (v,c) seen at training, we draw $z \\sim Bernoulli(\\alpha)$ with $z=0$ we condition on the original language utterance, while $z=1$ we generate the original language utterance, conditioning in the encoder on the \u0026lt;NULL\u0026gt; token. Results Potential future work it seems that we can utilise the method mentioned in this paper for our project pretraining the model using video caption dataset something-something-v2 dataset ","permalink":"https://sino-huang.github.io/posts/siddharth_karamcheti-language-driven-representation-learning-for-robotics-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language-Driven Representation Learning for Robotics\u003c/li\u003e\n\u003cli\u003eAuthor: Siddharth Karamcheti et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 24 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Mar 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.12766.pdf\"\u003ehttps://arxiv.org/pdf/2302.12766.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003erecent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks.\u003c/li\u003e\n\u003cli\u003eleveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control\u003c/li\u003e\n\u003cli\u003ebut robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration amongst others.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003efirst, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite (i.e., high-level semantics)\u003c/li\u003e\n\u003cli\u003eWe then introduce Voltron, a framework for language driven representation learning from human videos and associated captions.\n\u003cul\u003e\n\u003cli\u003eVoltron trades off language conditioned visual reconstruction to learn low-level visual patterns (mask auto-encoding) and visually grounded language generation to encode high-level semantics. (hindsight relabelling and contrastive learning)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eHow can we learn visual representations that generalise across the diverse spectrum of problems in robot learning?\u003c/strong\u003e\u003c/p\u003e","title":"Siddharth_karamcheti Language Driven Representation Learning for Robotics 2023"},{"content":"[TOC]\nTitle: Does Vision Accelerate Hierarchical Generalisation of Neural Language Learners Author: Tatsuki Kuribayashi Publish Year: 1 Feb 2023 Review Date: Fri, Mar 3, 2023 url: https://arxiv.org/pdf/2302.00667.pdf Summary of paper Motivation we want to know if the visual information improves hierarchical generalisaiton of the language model\nContribution our results have exhibited that vision accelerated a proper linguistic generlisation in the simplified, artificial setting, but LMs struggled with the proper generalisation in the noisy, realistic setting. These mixed results have indicated several possibilities; for example, an image can potentially boost language acquisition, but learners\u0026rsquo; additional visual/linguistic **prior knowledge should be needed t**o robustly make use of raw images for efficient language acquisition. ","permalink":"https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Does Vision Accelerate Hierarchical Generalisation of Neural Language Learners\u003c/li\u003e\n\u003cli\u003eAuthor: Tatsuki Kuribayashi\u003c/li\u003e\n\u003cli\u003ePublish Year: 1 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Mar 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.00667.pdf\"\u003ehttps://arxiv.org/pdf/2302.00667.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewe want to know if the visual information improves hierarchical generalisaiton of the language model\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303153510788\" loading=\"lazy\" src=\"/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153510788.png\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303153540288\" loading=\"lazy\" src=\"/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153540288.png\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303153621365\" loading=\"lazy\" src=\"/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153621365.png\"\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eour results have exhibited that vision accelerated a proper linguistic generlisation in the simplified, artificial setting,\u003c/li\u003e\n\u003cli\u003ebut LMs struggled with the proper generalisation in the noisy, realistic setting. These mixed results have indicated several possibilities; for example, an image can potentially boost language acquisition, but learners\u0026rsquo; additional visual/linguistic **\u003cu\u003eprior knowledge should be needed t\u003c/u\u003e**o robustly make use of raw images for efficient language acquisition.\u003c/li\u003e\n\u003c/ul\u003e","title":"Tatsuki_kuribayashi Does Vision Accelerate Hierarchical Generalisation of Neural Language Learners 2023"},{"content":"[TOC]\nTitle: Jing_cheng_pang Natural Language Conditioned Reinforcement Learning With Inside Out Task Language Development and Translation 2023 Author: Jing-Cheng Pang et. al. Publish Year: 18 Feb 2023 Review Date: Fri, Mar 3, 2023 url: https://arxiv.org/pdf/2302.09368.pdf Summary of paper Motivation previous approaches generally implemented language-conditioned RL by providing human instructions in natural language and training a following policy this is outside-in approach the policy needs to comprehend the NL and manage the task simultaneously. However, the unbounded NL examples often bring much extra complexity for solving concrete RL tasks, which can distract policy learning from completing the task Contribution we investigate an inside-out scheme for natural language-conditioned RL by developing a task language (TL) that is task-related and unique. The TL is used in RL to achieve high effective policy training. besides, a translator is trained to translate NL into TL. experiments indicate that the new model not only better comprehends NL instructions but also leads to better instruction following policy that improves 13.4% success rate and adapts to unseen expressions of NL instruction. ","permalink":"https://sino-huang.github.io/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Jing_cheng_pang Natural Language Conditioned Reinforcement Learning With Inside Out Task Language Development and Translation 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Jing-Cheng Pang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 18 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Mar 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.09368.pdf\"\u003ehttps://arxiv.org/pdf/2302.09368.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303152538759\" loading=\"lazy\" src=\"/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eprevious approaches generally implemented language-conditioned RL by providing human instructions in natural language and training a following policy\n\u003cul\u003e\n\u003cli\u003ethis is outside-in approach\u003c/li\u003e\n\u003cli\u003ethe policy needs to comprehend the NL and manage the task simultaneously.\u003c/li\u003e\n\u003cli\u003eHowever, the unbounded NL examples often bring much extra complexity for solving concrete RL tasks, which can distract policy learning from completing the task\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe investigate an inside-out scheme for natural language-conditioned RL by developing a task language (TL) that is task-related and unique. The TL is used in RL to achieve high effective policy training.\u003c/li\u003e\n\u003cli\u003ebesides, a translator is trained to translate NL into TL.\u003c/li\u003e\n\u003cli\u003eexperiments indicate that the new model not only better comprehends NL instructions but also leads to better instruction following policy that improves 13.4% success rate and adapts to unseen expressions of NL instruction.\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20230303161350807\" loading=\"lazy\" src=\"/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/image-assets/image-20230303161350807.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"Jing_cheng_pang Natural Language Conditioned Reinforcement Learning With Inside Out Task Language Development and Translation 2023"},{"content":"[TOC]\nTitle: Multi-Level Compositional Reasoning for Interactive Instruction Following Author: Suvaansh Bhambri et. al. Publish Year: 2023 Review Date: Fri, Mar 3, 2023 url: https://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf Summary of paper Motivation The task given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks. Contribution we propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction. at the highest level, we infer a sequence of human-interpreatable subgoals to be executed based on the language instructions by a high-level policy composition controller. at the middle level, we discriminatively control the agent\u0026rsquo;s navigation by a master policy by alternating between a navigation policy and various independent interaction policies. finally, at the lowest level, we infer manipulation actions with the corresponding object masks using appropriate interaction policy. Model ","permalink":"https://sino-huang.github.io/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Multi-Level Compositional Reasoning for Interactive Instruction Following\u003c/li\u003e\n\u003cli\u003eAuthor: Suvaansh Bhambri et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Mar 3, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf\"\u003ehttps://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303112210161\" loading=\"lazy\" src=\"/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe task given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction.\u003c/li\u003e\n\u003cli\u003eat the highest level, we infer a sequence of human-interpreatable subgoals to be executed based on the language instructions by a high-level policy composition controller.\u003c/li\u003e\n\u003cli\u003eat the middle level, we discriminatively control the agent\u0026rsquo;s navigation by a master policy by alternating between a navigation policy and various independent interaction policies.\u003c/li\u003e\n\u003cli\u003efinally, at the lowest level, we infer manipulation actions with the corresponding object masks using appropriate interaction policy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"model\"\u003eModel\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230303143459781\" loading=\"lazy\" src=\"/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/image-assets/image-20230303143459781.png\"\u003e\u003c/p\u003e","title":"Suvaansh_bhambri Multi Level Compositional Reasoning for Interactive Instruction Following 2023"},{"content":"[TOC]\nTitle: The Wisdom of Hindsight Makes Language Models Better Instruction Followers Author: Tianjun Zhang et. al. Publish Year: 10 Feb 2023 Review Date: Thu, Mar 2, 2023 url: https://arxiv.org/pdf/2302.05206.pdf Summary of paper Motivation Reinforcement learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the pipeline for reward and value networks Contribution in this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn\u0026rsquo;t require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for alignment language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilise the hindsightly relabeled instructions based on feedback. Some key terms fine-tuning language model\nthe most widely adopted approach is to deploy reinforcement learning (RL) algorithms to optimize for a manually defined or learned \u0026ldquo;alignment score\u0026rdquo;. Impressive progress has been made in this direction, including the more recently released GPT series model (OpenAI, 2022) it is less data-efficient if it only makes use of the success instruction-output pairs, completely abandoning the ones that do not align. Hindsight Instruction Relabeling (HIR)\nadopts the central idea of relabeling the instructions in a hindsight fashion based on the generated outputs of the language model. HIR alternates between two phases an online sampling phrase to generate a dataset of instruction-output pairs, along with an offline learning phrase that relabels the instructions of each pair and performs standard supervised learning Offline Relabeling\nThe key component of our algorithm is the offline relabelling part. In this part, for every instruction-output pair $(p,q,o)$ that are not necessarily aligned $p$ space of instructional prompt $p$ $q$ state space of input token sequence, used as query $o$ is the output sequence (actions) we relabel this pair with a new instruction that can align with the outcome of the model $(p*, q, o)$ The new instruction $p*$ is generated based on the feedback function $\\mathcal R(p,q,o)$ and the instruction generation function $\\phi(p,q,o,r)$, which can either be learned or scripted. EXAMPLE in the framework of RLHF, if the learned reward model $\\mathcal R(p,q,o)$ generates a score that ranks about 75% as in the training data, we can give additional scripted instructions to the model such as \u0026ldquo;give me an answer that rank about 75% in training data\u0026rdquo;. Conceptual Comparison between HIR and baseline methods ","permalink":"https://sino-huang.github.io/posts/tianjun_zhang-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: The Wisdom of Hindsight Makes Language Models Better Instruction Followers\u003c/li\u003e\n\u003cli\u003eAuthor: Tianjun Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.05206.pdf\"\u003ehttps://arxiv.org/pdf/2302.05206.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230302190916037\" loading=\"lazy\" src=\"/posts/tianjun_zhang-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eReinforcement learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the pipeline for reward and value networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner.\u003c/li\u003e\n\u003cli\u003eSuch an algorithm doesn\u0026rsquo;t require any additional parameters except for the original language model and maximally reuses the pretraining pipeline.\u003c/li\u003e\n\u003cli\u003eTo achieve this, we formulate instruction alignment problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for alignment language models with instructions.\u003c/li\u003e\n\u003cli\u003eThe resulting two-stage algorithm shed light to a family of reward-free approaches that utilise the hindsightly relabeled instructions based on feedback.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003efine-tuning language model\u003c/strong\u003e\u003c/p\u003e","title":"Tianjun_zhang the Wisdom of Hindsight Makes Language Models Better Instruction Followers 2023"},{"content":"[TOC]\nTitle: Learning by Asking for Embodied Visual Navigation and Task Completion Author: Ying Shen et. al. Publish Year: 9 Feb 2023 Review Date: Thu, Mar 2, 2023 url: https://arxiv.org/pdf/2302.04865.pdf Summary of paper Motivation despite recent progress on related vision-language benchmarks, most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments. Contribution we introduce an Embodied Learning by asking (ELBA) model that learns when to ask and what to ask for vision-dialog navigation and task completion. in contrast to prior work, our proposed model can ask questions both in templates and free-form formats\nwe demonstrate the effectiveness of the proposed approach and show that ELBA outperforms baselines on vision-dialog task completion\nwe further verify that the ability to dynamically ask questions improves task performance in embodied household tasks\nMethod overview at every time step $t$, the Actioner encodes the state information $s_{t-1}$ and outputs the action and object distribution (i.e., pick an action, pick an object to interact with) $p_t^a, p_t^o$ . The confusion module then determines the agent\u0026rsquo;s confusion level by measuring either the entropy of the predicted distribution or the gradient magnitude of the model if the confusion level exceeds a certain threshold, the agent will try to ask a quesiton based on the state history, the PLANNER predicts high-level future sub-goals instructions which are later used to generate candidate answers The QA generator then creates a set of candidate question-answer pairs based on the planner outputs. THe QA evaluator assigns a score to each QA pair, indicating their suitability for the current state, and ranks all QA pairs The agent samples a pair from the top-k QA pairs and ask the corresponding question if the confusion level decreases after incorporating the chosen QA pair When to ask If the confusion level (e.g., the entropy of action and object distribution) exceeds the threshold (meaning that the agent is not confident about its next move), the agent will try to ask a question What to ask The QA generator generates a set of candidate question-answer pairs from the extracted future sub-goals. Then QA evaluator assigns a score $\\phi(q_t^i, a_t^i)$ to each candidate question-answer pair by measuring the similarity between the state information and the question-answer pair. The motivation is that, based on the current state information, the most suitable question-answer pair should have the highest similarity score among all candidate pairs. ","permalink":"https://sino-huang.github.io/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Learning by Asking for Embodied Visual Navigation and Task Completion\u003c/li\u003e\n\u003cli\u003eAuthor: Ying Shen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 9 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.04865.pdf\"\u003ehttps://arxiv.org/pdf/2302.04865.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230302175239223\" loading=\"lazy\" src=\"/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003edespite recent progress on related vision-language benchmarks, most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230302175434615\" loading=\"lazy\" src=\"/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/image-assets/image-20230302175434615.png\"\u003e\u003c/p\u003e","title":"Ying_shen Learning by Asking for Embodied Visual Navigation and Task Completion 2023"},{"content":"[TOC]\nTitle: Benchmarks for Automated Commonsense Reasoning a Survey Author: Ernest Davis Publish Year: 9 Feb 2023 Review Date: Thu, Mar 2, 2023 url: https://arxiv.org/pdf/2302.04752.pdf Summary of paper we mainly focus on the section where the author discusses about features of commonsense reasoning generally. Terms clarify what we mean by common sense\nwhat is exactly \u0026ldquo;commonsensical\u0026rdquo;? Claims about common sense that seem true to the author Commonsense knowledge is common. In talking to other person, we do not have to explain common sense reasoning or enumerate common sense facts. We can assume that they know that unsupported things fall down, that outside the tropics, days in temperate regions are generally warmer than winter, and so on. Common sense is largely sensible. Any individual person or even an entire society may have various foolish or mistaken beliefs, but for the most part common sense knowledge correponds to the realities of the world as people experience it. Common sense supports reasoning. For example a person who knows that Central Park is in New York and the Golden Gate Bridge is in San Francisco and that New York and San Francisco are 3000 miles apart will realize that they cannot walk from one to the other in fifteen minutes. commonsense reasoning is integrated with other cognitive abilities Common sense extends across tasks and modalities Common sense is a broad scope Commonsense knowledge can be distinguished from common knowledge, encyclopaedic knowledge and expert knowledge Half-truths about commonsense knowledge Commonsense knowledge is language-independent The English-language bias is as pervasive in commmonsense reasoning as in other areas of AI. Impressively, versions of ConceptNet with at least 10,000 concepts exist in 83 different languages, and a few commonsense benchmarks have been translated (table 4) but most resources and benchmarks only exist in English or in a symbolic form in which the symbols are in fact English words or short phrases. Commonsense knowledge is the same for people of different cultures and of different historical periods Even if a belief has been commonsense knowledge for everyone at all times up to the present, that does not mean that that will continue in the future. Commonsense reasoning is fast and intuitive; it falls within \u0026ldquo;System 1\u0026rdquo; Processes in System 1 characteristically are executed quickly, do not require conscious thought, are not open to introspection, in at least in some cases are not controllable (one cannot decide not to interpret what one is seeing), and do not place a cognitive burden on working memory; vision is a paradigmatic example. Processes in System 2 are the reverse: slow, consciously carried out, consciously controllable, instrospectable, and taxing on working memory. System 2 processes can call on system 1 but not vice versa, since a fast process cannot use a slow subroutine. encyclopaedic and expert knowledge can also be called on in System 1 activities Commonsense knowledge can be expressed using simple language it seems plausible: basic vocabulary tends to refer to the well-known concepts and relations which are the subject of commonsense knowledge however, there is a very large exception here, which is commonsense spatial knowledge. Natural language is notoriously ill-suited to the description of characteristics of shapes and positions that are easily apprehended (bad expressivity of natural language) An untrue claim about commonsense knowledge commonsense knowledge is not logically complex However, in physical reasoning, understanding the physical characteristics could be quite complex (e.g., considering angry birds). But humans are good at playing angry birds. ","permalink":"https://sino-huang.github.io/posts/ernest_davis-benchmarks-for-automated-commonsense-reasoning-a-survey-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Benchmarks for Automated Commonsense Reasoning a Survey\u003c/li\u003e\n\u003cli\u003eAuthor: Ernest Davis\u003c/li\u003e\n\u003cli\u003ePublish Year: 9 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Mar 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.04752.pdf\"\u003ehttps://arxiv.org/pdf/2302.04752.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ewe mainly focus on the section where the author discusses about features of commonsense reasoning generally.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"terms\"\u003eTerms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eclarify what we mean by common sense\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewhat is exactly \u0026ldquo;commonsensical\u0026rdquo;?\u003c/li\u003e\n\u003cli\u003eClaims about common sense that seem true to the author\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCommonsense knowledge is common.\u003c/strong\u003e In talking to other person, we do not have to explain common sense reasoning or enumerate common sense facts. We can assume that they know that unsupported things fall down, that outside the tropics, days in temperate regions are generally warmer than winter, and so on.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommon sense is largely sensible\u003c/strong\u003e. Any individual person or even an entire society may have various foolish or mistaken beliefs, but for the most part common sense knowledge correponds to the realities of the world as people experience it.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommon sense supports reasoning\u003c/strong\u003e. For example a person who knows that Central Park is in New York and the Golden Gate Bridge is in San Francisco and that New York and San Francisco are 3000 miles apart will realize that they cannot walk from one to the other in fifteen minutes.\u003c/li\u003e\n\u003cli\u003ecommonsense reasoning is integrated with other cognitive abilities\u003c/li\u003e\n\u003cli\u003eCommon sense extends across tasks and modalities\u003c/li\u003e\n\u003cli\u003eCommon sense is a broad scope\u003c/li\u003e\n\u003cli\u003eCommonsense knowledge can be distinguished from common knowledge, encyclopaedic knowledge and expert knowledge\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHalf-truths about commonsense knowledge\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCommonsense knowledge is language-independent\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe English-language bias is as pervasive in commmonsense reasoning as in other areas of AI. Impressively, versions of ConceptNet with at least 10,000 concepts exist in 83 different languages, and a few commonsense benchmarks have been translated (table 4) but most\nresources and benchmarks only exist in English or in a symbolic form in which the symbols are in fact English words or short phrases.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommonsense knowledge is the same for people of different cultures and of different historical periods\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eEven if a belief has been commonsense knowledge for everyone at all times up to the present, that does not mean that that will continue in the future.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommonsense reasoning is fast and intuitive; it falls within \u0026ldquo;System 1\u0026rdquo;\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eProcesses in System 1 characteristically are executed quickly, do not require conscious thought, are not open to introspection, in at least in some cases are not controllable (one cannot decide not to interpret what one is seeing), and do not place a cognitive burden on working memory; vision is a paradigmatic example. Processes in System 2 are the reverse: slow, consciously carried out, consciously controllable, instrospectable, and taxing on working memory. System 2 processes can call on system 1 but not vice versa, since a fast process cannot use a slow subroutine.\u003c/li\u003e\n\u003cli\u003eencyclopaedic and expert knowledge can also be called on in System 1 activities\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCommonsense knowledge can be expressed using simple language\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eit seems plausible: basic vocabulary tends to refer to the well-known concepts and relations which are the subject of commonsense knowledge\u003c/li\u003e\n\u003cli\u003ehowever, there is a very large exception here, which is commonsense spatial knowledge. Natural language is notoriously ill-suited to the description of characteristics of shapes and positions that are easily apprehended (bad expressivity of natural language)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAn untrue claim about commonsense knowledge\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cu\u003ecommonsense knowledge is not logically complex\u003c/u\u003e\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eHowever, in physical reasoning, understanding the physical characteristics could be quite complex (e.g., considering angry birds). But humans are good at playing angry birds.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","title":"Ernest_davis Benchmarks for Automated Commonsense Reasoning a Survey 2023"},{"content":"[TOC]\nTitle: Anti Exploration by Random Network Distillation Author: Alexander Nikulin et. al. Publish Year: 31 Jan 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2301.13616.pdf Summary of paper Motivation despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning ?? wait, why we want to penalizing out-of-distribution actions? Contribution With a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. Some key terms why we want uncertainty-based penalization\nthe use of ensembles for uncertainty-based penalization has proven to be one of the most effective approaches for offline RL. what is ensemble-based algorithm\nsuch as SAC-N, EDAC (An et al., 2021), and MSG (Ghasemipour et al., 2022) build a group of Q-networks to solve the task what is offline reinforcement learning\nIn offline reinforcement learning, a policy must be learned from a fixed dataset D collected under a different policy or mixture of policies, without any environment interaction. this setting poses unique fundamental challenges, since the learning policy is unable to explore and has to deal with distributional shift and extrapolation errors for actions not represented in the training dataset. Offline RL and Anti-Exploration\nThere are numerous approaches for Offline RL, a substantial part of which constrain the learned policy to stay within the support of the training dataset, thus reducing or avoiding extrapolation errors. for our work, it is essential to understand how such a constraint can be framed as anti-exploration. Similarly to online RL, where novelty bonuses are used as additive intrinsic rewards for effective exploration, in offline RL, novelty bonuses can induce conservatism, reducing the reward in unseen state-action pairs. Hence the name anti-exploration, since the same approaches from exploration can be used, but a bonus is subtracted from the extrinsic reward instead of being added to it. ","permalink":"https://sino-huang.github.io/posts/alexander_nikulin-anti-exploration-by-random-network-distillation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Anti Exploration by Random Network Distillation\u003c/li\u003e\n\u003cli\u003eAuthor: Alexander Nikulin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 31 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Mar 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.13616.pdf\"\u003ehttps://arxiv.org/pdf/2301.13616.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230301221745373\" loading=\"lazy\" src=\"/posts/alexander_nikulin-anti-exploration-by-random-network-distillation-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003edespite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003e?? wait, why we want to penalizing out-of-distribution actions?\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWith a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue.\u003c/li\u003e\n\u003cli\u003eWe show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ewhy we want uncertainty-based penalization\u003c/strong\u003e\u003c/p\u003e","title":"Alexander_nikulin Anti Exploration by Random Network Distillation 2023"},{"content":"[TOC]\nTitle: Learning Pessimism for Reinforcement Learning Author: Edoardo Cetin et. al. Publish Year: 2023 Review Date: Wed, Mar 1, 2023 url: https://kclpure.kcl.ac.uk/portal/files/196848783/10977.CetinE.pdf Summary of paper Motivation Off-policy deep RL algorithms commonly compensate for overestimation bias during temporal difference learning by utilizing pessimistic estimates of the expected target returns Contribution we propose Generalised Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimise the magnitude of the target returns bias with trivial computational cost. Some key terms We attribute recent improvements on RL algs to two main linked advances:\nmore expressive models to capture uncertainty better strategies to counteract detrimental biases from the learning process. Overestimation problem in Critic model\nWithin this process, overestimation bias naturally arises from the maximization performed over the critic’s performance predictions, and consequently, also over the critic’s possible errors Solution directly model the overestimation bias $p_\\beta(s\u0026rsquo;,a\u0026rsquo;,\\phi,\\theta)$ Minor comments citation\nSample efficiency and generality are two directions in which reinforcement learning (RL) algorithms are still lacking, yet, they are crucial for tackling complex real-world problems (Mahmood et al. 2018). Incomprehension I am not so sure why the author only mentioned SAC and neglect other advanced RL algorithms that solves Atari games quite well.\n","permalink":"https://sino-huang.github.io/posts/edoardo_cetin-learning-pessimism-for-reinforcement-learning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Learning Pessimism for Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Edoardo Cetin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Mar 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://kclpure.kcl.ac.uk/portal/files/196848783/10977.CetinE.pdf\"\u003ehttps://kclpure.kcl.ac.uk/portal/files/196848783/10977.CetinE.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230301210301465\" loading=\"lazy\" src=\"/posts/edoardo_cetin-learning-pessimism-for-reinforcement-learning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOff-policy deep RL algorithms commonly compensate for overestimation bias during temporal difference learning by utilizing pessimistic estimates of the expected target returns\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose Generalised Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular\u003c/li\u003e\n\u003cli\u003ewe propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimise the magnitude of the target returns bias with trivial computational cost.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWe attribute recent improvements on RL algs to two main linked advances:\u003c/strong\u003e\u003c/p\u003e","title":"Edoardo_cetin Learning Pessimism for Reinforcement Learning 2023"},{"content":"[TOC]\nTitle: Toolformer: Language Models Can Teach Themselves to Use Tools 2023 Author: Timo Schick et. al. META AI research Publish Year: 9 Feb 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2302.04761.pdf Summary of paper Motivation LMs exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also struggle with basic functionality, such as arithmetic or factual lookup. Contribution In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model that incorporate a range of tools, including a calculator, a Q\u0026amp;A system, a search engine, a translation system and a calendar. Some key terms limitation of language models\nmodels have several inherent limitations that can at best be partially addressed by further scaling these limitations include an inability to access up-to-date information on recent event (Komeili et al., 2022) and the related tendency to hallucinate facts (Maynez et al., 2020; Ji et al., 2022), difficul- ties in understanding low-resource languages (Lin et al., 2021), a lack of mathematical skills to perform precise calculations (Patel et al., 2021) and an unawareness of the progression of time (Dhingra et al., 2022). Toolformer, which fulfills the following desiderata\nthe use of tools should be learned in a self-supervised way without requiring large amounts of human annotations. This is important not only because of the costs associated with such annotations, but also because what humans find useful may be different from what a model finds useful the LM should not lose any of its generality and should be able to decide for itself when and how to use which tool. In contrast to existing approaches, this enables a much more comprehensive use of tools that is not tied to specific tasks. Methods We let a LM annotate a huge language modelling dataset with potential API calls. We then use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens Finally, we finetune the LM itself on the API calls that it considers useful illustrated for a question answering tool: given an input text $x$, we first sample a position $i$ and corresponding API call candidates $c_i^1, c_i^2,\u0026hellip;,c_i^k$. We then execute these API calls can filter out all calls which do not reduce the loss $L_i$ over the next tokens Approach details\nAPI call representation Filtering API calls The loss just calculates the cumulative negation of the word prediction probability log likelihood after the $i^{th}$ position this measures how the prefix $z$ , the API call and answer, affects the text generation. $L_i^-$ is the baseline loss Highlight of this paper it offers a very practical way (based on in-context prompting) to utilise GPT-J, the Language model to combine with other AI systems to increase its performance in other tasks such as arithmetic tasks etc. ","permalink":"https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Toolformer: Language Models Can Teach Themselves to Use Tools 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Timo Schick et. al. META AI research\u003c/li\u003e\n\u003cli\u003ePublish Year: 9 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Mar 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.04761.pdf\"\u003ehttps://arxiv.org/pdf/2302.04761.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230301201424679\" loading=\"lazy\" src=\"/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLMs exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale.\u003c/li\u003e\n\u003cli\u003eThey also struggle with basic functionality, such as arithmetic or factual lookup.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.\u003c/li\u003e\n\u003cli\u003eWe introduce Toolformer, a model that incorporate a range of tools, including a calculator, a Q\u0026amp;A system, a search engine, a translation system and a calendar.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003elimitation of language models\u003c/strong\u003e\u003c/p\u003e","title":"Timo_schick Toolformer Language Models Can Teach Themselves to Use Tools 2023"},{"content":"[TOC]\nTitle: Knowledge Is a Region in Weight Space for Fine Tuned Language Model Author: Almog Gueta et. al. Publish Year: 12 Feb 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2302.04863.pdf Summary of paper Motivation relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets. Contribution we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. more findings\nwe show that after a pre-trained model is fine-tuned on similar datasets, the resulting fine-tuned models are close to each other in the weight space. models fine-tuned on the sae data are closer to each other than to to other models models that were fine-tuned on the same task also cluster together models fine-tuned on language tasks are not spread around the pre-trained space arbitrarily but rather correspond to a constrained region in weight space Some key terms rather than fine-tuning\nNotably, such points in weight space might not necessarily be reached via fine-tuning, but rather via spatial transformation. points on a line between the two points representing two models fine-tuned on the same dataset\nwe find that points on a line between the two points representing two models fine-tuned on the same dataset attain similar or even lower loss than the two individual models. empirical findings\nsuggesting, for example, that the best models may not lie at the edges of the region, but rather closer to its center, while fine-tuning often yields models at the edge of the region motivated by these findings, we demonstrate that a model created by averaging the weights of fine-tuned models from the same region outperforms the pre-trained model on a variety of tasks after subsequent fine-tuning. Comparing models comparing loss difference is the core idea but the loss of a given model is often incomparable across datasets or tasks to define a loss that is comparable across models, we first adopt the typical perspective that the model $f_\\theta$ consist of a representation encoder $f_w$ followed by a task-specific $f_\\phi$, i.e., $f_\\theta = f_\\phi \\circ f_w$ to calculate the loss we do the following first, remove any existing masked language modeling layers or classification heads and replace them with a new randomly initialised classification head. This leaves the rest of the weights i.e., the encoder $f_w$ fixed. We then perform linear probing i.e., we train only the new classification head on a desired target data and its label Lastly, we pass the test data through the model and report the loss with respect to the label. Projection by t-SNE\nt-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualising high-dimensional data by giving each datapoint a location in a two-or three dimensional map. ","permalink":"https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Knowledge Is a Region in Weight Space for Fine Tuned Language Model\u003c/li\u003e\n\u003cli\u003eAuthor: Almog Gueta et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 12 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Mar 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.04863.pdf\"\u003ehttps://arxiv.org/pdf/2302.04863.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230301124703839\" loading=\"lazy\" src=\"/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003erelatively little is known a bout the relationships between different models, especially those trained or tested on different datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa\u003c/li\u003e\n\u003cli\u003elanguage models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster.\u003c/li\u003e\n\u003cli\u003etraversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning\u003c/li\u003e\n\u003cli\u003eOur findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003emore findings\u003c/strong\u003e\u003c/p\u003e","title":"Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023"},{"content":"[TOC]\nTitle: Contrastive Instruction Trajectory Learning for Vision Language Navigation Author: Xiwen Liang et. al. Publish Year: AAAI 2022 Review Date: Fri, Feb 10, 2023 url: https://arxiv.org/abs/2112.04138 Summary of paper Motivation previous works learn to navigate step-by-step following an instruction. However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the temporal continuity of sub-instructions. These problems hinder agents from learning distinctive vision-and-language representations, Contribution we propose a coarse-grained contrastive learning objective to enhance vision-and-language representations by contrasting semantics of full trajectory observations and instructions respectively; a fine-grained contrastive learning objective to perceive instructions by leveraging the temporal information of the sub-instructions. a pairwise sample-reweighting mechanism for contrastive learning to sampling bias in contrastive learning. Some key terms Limitation of current VLN model\nthese VLN models only use the context within an instruction-trajectory pair while ignoring the knowledge across the pairs. For instance, they only recognise the correct actions that follow the instrucvtion while ignoring the actions that do not follow the instruction but the different between the correct actions and the wrong actions contain extra knowledge for navigation. on the other hand, previous methods do not explicitly exploit the temporal continuity inside an instruction, which may fail if the agent focuses on a wrong sub-instruction. thus, learning a fine-grained sub-instruction representation by leveraging the temporal continuity of sub-instructions could improve the robustness of navigation Method coarse-grained contrastive learning\nlearn distinctive long-horizon representations for trajectories and instructions respectively. the idea is to compute a inter-intra cross-instance contrast: enforcing embedding to be similar for positive trajectory-instruction pairs and dissimilar for intra-negative and inter-negative ones. intra-negative: change the temporal information of the instruction and selecting longer sub-optimal trajectories which deviate from the anchor one severely. inter-negative: in-batch negative sampling fine-grained contrastive learning\nlearn fine-grained representations by focusing on the temporal information of sub-instructions we generate sub-instructions (i.e., cut long instruction into sub ones) and train the agent to learn embedding distances of these sub-instructions by contrastive learning. Specifically neighbour sub-instructions are positive samples, while non-neighbour sub-instructions are intra-negative samples and different sub-instructions from other instructions are inter-negative samples. sukai comment: i don\u0026rsquo;t think this really helps ","permalink":"https://sino-huang.github.io/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Contrastive Instruction Trajectory Learning for Vision Language Navigation\u003c/li\u003e\n\u003cli\u003eAuthor: Xiwen Liang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: AAAI 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Feb 10, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/abs/2112.04138\"\u003ehttps://arxiv.org/abs/2112.04138\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230210025151701\" loading=\"lazy\" src=\"/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eprevious works learn to navigate step-by-step following an instruction. However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the \u003cstrong\u003e\u003cu\u003etemporal continuity\u003c/u\u003e\u003c/strong\u003e of sub-instructions. These problems hinder agents from learning distinctive vision-and-language representations,\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose\n\u003cul\u003e\n\u003cli\u003ea coarse-grained \u003cstrong\u003econtrastive learning\u003c/strong\u003e objective  to enhance vision-and-language representations by \u003cu\u003econtrasting semantics of full trajectory observations\u003c/u\u003e and instructions respectively;\u003c/li\u003e\n\u003cli\u003ea fine-grained contrastive learning objective to perceive instructions by leveraging the \u003cu\u003etemporal information\u003c/u\u003e of the sub-instructions.\u003c/li\u003e\n\u003cli\u003ea pairwise sample-reweighting mechanism for contrastive learning to sampling bias in contrastive learning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation of current VLN model\u003c/strong\u003e\u003c/p\u003e","title":"Xiwen_liang Contrastive Instruction Trajectory Learning for Vision Language Navigation 2022"},{"content":"[TOC]\nTitle: LAMMP Language Models as Probabilistic Priors for Perception and Action 2023 Author: Belinda Z. Li, Jacob Andreas et. al. Publish Year: 3 Feb 2023 Review Date: Fri, Feb 10, 2023 url: https://arxiv.org/pdf/2302.02801.pdf Summary of paper Motivation Language models trained on large text corpora encode rich distributional information about real-world environments and action sequences. this information plays a crucial role Contribution we describe how to leverage language models for non-linguistic perception and control tasks Our approach casts labelling and decision-making as inference in probabilistic graphical models in which language models parameterize prior distributions over labels, decisions and parameters, making it possible to integrate uncertain observations and incomplete background knowledge in a principled way. Some key terms common-sense priors\nare crucial for decision-making under uncertainty in real-world environment prior knowledge about object or event co-occurrences unlike segmented images or robot demonstrations, large text corpora are readily available and describe almost all facets of human experiences. model chaining approach\nencode the output of perceptual system as natural language strings that prompt LMs to directly generate labels or plan Sukai Comment: this is not a very good way due to information loss Method we can combine them with domain specific generative models or likelihood functions to integrate \u0026ldquo;top-down\u0026rdquo; background knowledge with \u0026ldquo;bottom-up\u0026rdquo; task-specific predictors. by Bayes rules the $p(x|y)$ is a generative model of observations given labels design a label space\nthis means we can have a meta label for labels noisy label is from from the image model (the segmentation classes from the image model) navigation task using LM to calculate prior Core improvement compared to Previous Work ","permalink":"https://sino-huang.github.io/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: LAMMP Language Models as Probabilistic Priors for Perception and Action 2023\u003c/li\u003e\n\u003cli\u003eAuthor: Belinda Z. Li, Jacob Andreas et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 3 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Feb 10, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.02801.pdf\"\u003ehttps://arxiv.org/pdf/2302.02801.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230210004929089\" loading=\"lazy\" src=\"/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLanguage models trained on large text corpora encode rich distributional information about real-world environments and action sequences.\n\u003cul\u003e\n\u003cli\u003ethis information plays a crucial role\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe describe how to leverage language models for non-linguistic perception and control tasks\u003c/li\u003e\n\u003cli\u003eOur approach casts labelling and decision-making as inference in probabilistic graphical models in which language models parameterize prior distributions over labels, decisions and parameters, making it possible to integrate uncertain observations and incomplete background knowledge in a principled way.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ecommon-sense priors\u003c/strong\u003e\u003c/p\u003e","title":"Jacob_andreas Lammp Language Models as Probabilistic Priors for Perception and Action 2023"},{"content":"[TOC]\nTitle: Multimodal Chain of Thought Reasoning in Language Models Author: Zhuosheng Zhang et. al. Publish Year: 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2302.00923.pdf Summary of paper Motivation LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. Contribution We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference. Some key terms Multimodal-CoT\nMultimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the answer. In general, there are two ways to elicit Multimodal-CoT reasoning as follows prompting LLMs fintuning small models the most immediate way to perform Multimodal-CoT is to transform the input of different modalities into one modality and prompt LLMs to perform CoT. Specificaly, it is possible to extract the caption of an image by a captioning model and then concatenate the caption with the original language input to be fed into LLM However, there is severe information loss in the captioning process, with a lack of mutual synergy in the representation space of different modalities. another solution is to fine-tune language models by fusing multimodal features. As this approach does not rely on LLMs and allows the flexibilit of adjusting model architecture to incorporate multimodal features, this paper focuses on fine-tuning model. the key challenge is that language models under 100 billion parameters tend to generate hallucinated rationals that mislead the answer inference Method By incorporating the vision features in both stages, the model is able to generate more effective rationales. Our experiments are conducted on the ScienceQA benchmark (Lu et al., 2022a), which is the latest multimodal reasoning benchmark with annotated reasoning chains. how does CoT prompting work\nthe model takes the concatenation of tokens of the question text (Q), the context text (C), and multiple options (M) as the input. The key issue arises, the rationale information may not contribute to predicting the right answer. Improving Few-Shot-CoT studies are categorized into two major research lines optimizing the demonstrations the key is the diversity of demonstration questions Partition questions of a given dataset into a few clusters. optimising the reasoning chains. problem decomposition vote over multiple reasoning paths for a test question Why rationales might not help the plausible reason might be that the model exceeds the maximum token limits before obtaining the required answer or stops generating the prediction early. To dive into how the rationales affect the answer prediction, we decouple the CoT problem into two stages, rationale generation and answer inference. We speculate that such a phenomenon of hallucination is due to a lack of necessary vision contexts for performing effective Multimodal-CoT. Model Solution rather than convert the vision information to captions, we convert the vision info into vision features use gated fusion CONCLUSION minimise the conversion from one modality to another modality because there will be information loss. ","permalink":"https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Multimodal Chain of Thought Reasoning in Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Zhuosheng Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 8, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2302.00923.pdf\"\u003ehttps://arxiv.org/pdf/2302.00923.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230208222840588\" loading=\"lazy\" src=\"/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.\u003c/li\u003e\n\u003cli\u003eto elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning.\u003c/li\u003e\n\u003cli\u003eThe key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe propose Mutimodal-CoT that incorporates vision features in a decoupled training framework.\u003c/li\u003e\n\u003cli\u003eThe framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMultimodal-CoT\u003c/strong\u003e\u003c/p\u003e","title":"Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023"},{"content":"[TOC]\nTitle: Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning Author: Siyuan Wang et. al. Publish Year: 21 Jan 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2301.08913.pdf Summary of paper Motivation language models still suffer from a heterogeneous information alignment problem and a noisy knowledge injection problem. for complex reasoning, the context contains rich knowledge that typically exists in complex and sparse form. Contribution we propose to unify structure reasoning and language model pre-training identifies four types of elementary knowledge structures from contexts to construct structured queries utilise box embedding method to conduct explicit structure reasoning along query during language modeling Some key terms What is the problem\nusing context information to avoid two problems, namely, the semantic gap between two domains and noisy knowledge introduced by retrieval. model in specific, our model starts with extracting knowledge structures from the context and constructs the structured queries with the answer entity question: how to ensure the accuracy??? Representations of query structure and the answer are learned via box embedding (Ren et. al. 2020) The training process mains to put the answer embedding inside the query box embedding in the semantic space. Knowledge Structures Identification we propose four types of elementary knowledge structures for reasoning over both simple and complex knowledge in the text as shown in Figure 3. Box embedding Pretraining we jointly optimize structure reasoning and language modelling to obtain a unified structure embedded language representation. the overall pre-training objective is the combination of the structure reasoning loss $L_{SR}$ and the masked language modelling loss $L_{LML}$ we utilise Wikipedia documents with entities and their relation constructed by (Qin et. al. 2020) to obtain knowledge structures for pre-training. ","permalink":"https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning\u003c/li\u003e\n\u003cli\u003eAuthor: Siyuan Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 21 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 8, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.08913.pdf\"\u003ehttps://arxiv.org/pdf/2301.08913.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230209160806080\" loading=\"lazy\" src=\"/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003elanguage models still suffer from a heterogeneous information alignment problem and a noisy knowledge injection problem.\u003c/li\u003e\n\u003cli\u003efor complex reasoning, the context contains rich knowledge that typically exists in complex and sparse form.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose to unify structure reasoning and language model pre-training\u003c/li\u003e\n\u003cli\u003eidentifies four types of elementary knowledge structures from contexts to construct structured queries\u003c/li\u003e\n\u003cli\u003eutilise box embedding method to conduct explicit structure reasoning along query during language modeling\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is the problem\u003c/strong\u003e\u003c/p\u003e","title":"Siyuan_wang Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning 2023"},{"content":"[TOC]\nTitle: Towards Tracing Factual Knowledge in Language Models Back to the Training Data Author: Ekin Akyurek et. al. Publish Year: EMNLP 2022 Review Date: Wed, Feb 8, 2023 url: https://aclanthology.org/2022.findings-emnlp.180.pdf Summary of paper Motivation LMs have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. Contribution we propose the problem of fact tracing identifying which training examples taught an LM to generate a particular factual assertion. prior work on training data distribution (TDA) may offer effective tools for identifying such examples, known as \u0026ldquo;proponent\u0026rdquo;. We present the first quantitative benchmark to evaluate this we compare two popular families of TDA methods gradient based embedding based Some key terms Training data distribution method (TDA)\nare the main literature concerned with linking predictions back to specific training examples (known as proponents) several obstacles have limited research on fact tracing for large, pre-trained LMs. First, since pre-training corpora are very large, it has not been clear how to obtain ground truth labels regarding which pre-training example was truly responsible for an LM\u0026rsquo;s prediction. Second, TDA methods have traditionally been computationally prohibitive. Obtaining Ground Truth Proponents\npropose a recipe, which we call \u0026ldquo;novel fact injection\u0026rdquo; first suppose that we can identify a set of \u0026ldquo;facts\u0026rdquo; that the pre-trained LM does not know \u0026ndash; we call these \u0026ldquo;novel facts\u0026rdquo; we can convert each novel fact into LM training example, and then fine tune the LM on these extra examples until it memorizes novel fact (i.e., \u0026ldquo;injecting\u0026rdquo; them into the LM) now we have ground-truth proponents for every novel fact. Mitigating computational cost\nwe propose a simple reranking setup that is commonly used in information retrieval experiments rather than running a TDA method over all training examples, we run it only over a small subset of candidate examples that is guaranteed to include the ground truth proponents as well as some distractor examples that are not true proponents. this is handled by manual selection Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Towards Tracing Factual Knowledge in Language Models Back to the Training Data\u003c/li\u003e\n\u003cli\u003eAuthor: Ekin Akyurek et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: EMNLP 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 8, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://aclanthology.org/2022.findings-emnlp.180.pdf\"\u003ehttps://aclanthology.org/2022.findings-emnlp.180.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230209232944264\" loading=\"lazy\" src=\"/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLMs have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cimg src=\"image-assets/image-20230210000202731.png\" alt=\"image-20230210000202731\" style=\"width:70%;\" /\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose the problem of fact tracing\n\u003cul\u003e\n\u003cli\u003eidentifying which training examples taught an LM to generate a particular factual assertion.\u003c/li\u003e\n\u003cli\u003eprior work on training data distribution (TDA) may offer effective tools for identifying such examples, known as \u0026ldquo;proponent\u0026rdquo;. We present the first quantitative benchmark to evaluate this\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewe compare two popular families of TDA methods\n\u003cul\u003e\n\u003cli\u003egradient based\u003c/li\u003e\n\u003cli\u003eembedding based\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTraining data distribution method (TDA)\u003c/strong\u003e\u003c/p\u003e","title":"Ekin_akyurek Towards Tracing Factual Knowledge in Language Models Back to the Training Data 2022"},{"content":"[TOC]\nTitle: Mastering Diverse Domains Through World Models Author: Danijar Hafner et. al. Publish Year: 10 Jan 2023 Review Date: Tue, Feb 7, 2023 url: https://www.youtube.com/watch?v=vfpZu0R1s1Y Summary of paper Motivation general intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but held back by the resources and knowledge required tune them for new task. Contribution we present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. we observe favourable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Some key terms World Model learning\nlearn the world dynamics (nothing about value function or reward) notes that we want to use hidden state $h_t$ to predict discrete representation $z_t$, it is because later we want to predict the future that there will not no more $x_t$ input. it also has a continue predictor to predict whether the world reaches a terminal state. Some tricks\nnormalising the rewards using symlog with symlog predictions, there is no need for truncating large rewards, introducing non-stationary through reward normalisation (*Because the reward distribution changes as the agent improves) , or adjusting network weights when new extreme values are detected. the $\\max$ there, meaning that essentially the dynamic loss and representation loss should not be more important than prediction loss. stop gradient is just stop gradient flowing into that parameters. critic learning The original critic predicts the expected value of a potentially widespread return distributions, which can slow down learning we choose a discrete regression approach for leaning the critic based on two hot encoded target Good things about the paper (one paragraph) the world model really helps to conquer the game ","permalink":"https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Mastering Diverse Domains Through World Models\u003c/li\u003e\n\u003cli\u003eAuthor: Danijar Hafner et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Feb 7, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://www.youtube.com/watch?v=vfpZu0R1s1Y\"\u003ehttps://www.youtube.com/watch?v=vfpZu0R1s1Y\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230207182123945\" loading=\"lazy\" src=\"/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003egeneral intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but held back by the resources and knowledge required tune them for new task.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters.\u003c/li\u003e\n\u003cli\u003ewe observe favourable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWorld Model learning\u003c/strong\u003e\u003c/p\u003e","title":"Danijar_hafner Mastering Diverse Domains Through World Models 2023"},{"content":"[TOC]\nTitle: What Makes Good Examples for Visual in Context Learning Author: Yuan Zhang et. al. Publish Year: 1 Feb 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.13670.pdf Summary of paper Motivation in this paper, the main focus is on an emergent ability in large vision models, known. as in-context learning this concept has been well-known in natural language processing but has only been studied very recently for large vision models. Contribution we for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples. exposing a critical issue that different in-context examples could lead to drastically different results. Our methods obtain significant improvements over random selection under various problem settings, showing the potential of using prompt retrieval in vision applications with a Model-as-a-Service (MaaS) business structure. we show that a good in-context example should be semantically similar to the query and closer in context. A model that can better balance spatial and se- mantic closedness in feature space would be more ideal for visual in-context learning. yeah, it is because the model is not that smart in a way that it can directly tell the semantic regardless of what the spatial structure looks like Some key terms existing issue of using LLM\nEntities able to develop large-scale models typically only provide users with APIs, known as Model-as-a-Service (Maas). Representative examples include GPT-3. As a result, users are unable to apply full fine-tuning or some parameter-efficient tuning techniques, such as prompt learning for model adaption, largely limiting downstream performance in-context learning\nwithout the need to update any parameter for previously unseen tasks, in-context learning simply prepends some domain-specific input-output pairs, called in-context example or prompt, to a test example, which together guide the model to produce an ideal result. in computer vision, we can pretrained a neural network to fill missing patches in grid-like images, which allows the model to perform in-context learning for unseen tasks like image segmentation. sensitivity to the prompt selection\nchoosing a good in-context example is essential for the performance Method Using IoU of the segmentation to rank the in-context examples or using human labelling after that, train a learnable feature extractor such that the cosine distance between the feature vector of two similar images should be small, while two dissimilar images should have large cosine distance (contrastive learning) the trained feature extractor helps to retrieve in-context examples from a large dataset. looks like this method assists the model to do an interpolation ","permalink":"https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: What Makes Good Examples for Visual in Context Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Yuan Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 1 Feb 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Feb 6, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.13670.pdf\"\u003ehttps://arxiv.org/pdf/2301.13670.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230207133939704\" loading=\"lazy\" src=\"/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, the main focus is on an emergent ability in large vision models, known. as in-context learning\u003c/li\u003e\n\u003cli\u003ethis concept has been well-known in natural language processing but has only been studied very recently for large vision models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples.\u003c/li\u003e\n\u003cli\u003eexposing a critical issue that different in-context examples could lead to drastically different results.\n\u003cul\u003e\n\u003cli\u003eOur methods obtain significant improvements over\nrandom selection under various problem settings, showing\nthe potential of using prompt retrieval in vision applications\nwith a Model-as-a-Service (MaaS) business structure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewe show that a good in-context example should be semantically similar to the query and closer in context.\u003c/li\u003e\n\u003cli\u003eA model that can better balance spatial and se-\nmantic closedness in feature space would be more ideal for\nvisual in-context learning.\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eyeah, it is because the model is not that smart in a way that it can directly tell the semantic regardless of what the spatial structure looks like\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eexisting issue of using LLM\u003c/strong\u003e\u003c/p\u003e","title":"Yuanhan_zhang What Makes Good Examples for Visual in Context Learning 2023"},{"content":"[TOC]\nTitle: Grounding Language Models to Images for Multimodal Generation Author: Jing Yu Koh et. al. Publish Year: 31 Jan 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.13823.pdf Summary of paper Motivation we propose an efficient method to ground pre-trained text-only language models to the visual domain How we keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved Contribution our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pre-trained language models in visually grounded settings. Related work LLMs for vision-and-language\nwe differ from previous work in that our model is capable of generating coherent multimodal output: Flammingo is incapable of producing visual output. efficient adaptation of pretrained models\nour work builds upon the insights and methods from these prior works. While previous models mostly focus on generating text-only outputs, our model is capable of processing arbitrarily interleaved image-text inputs to generate coherent interleaved image-text outputs. Method We learn translation parameters (parameterized as linear layers) to cast images into text space, and translate text embeddings into visual space. (cycle???) two training methods\nimage captioning image-text retrieval How does it output images\nit can do coreferencing to select the appropriate images Good things about the paper (one paragraph) the framework has the same function as CLIP but it utilises the pretrained large-scale visual language models. ","permalink":"https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Grounding Language Models to Images for Multimodal Generation\u003c/li\u003e\n\u003cli\u003eAuthor: Jing Yu Koh et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 31 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Feb 6, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.13823.pdf\"\u003ehttps://arxiv.org/pdf/2301.13823.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230207134638732\" loading=\"lazy\" src=\"/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose an efficient method to ground pre-trained text-only language models to the visual domain\u003c/li\u003e\n\u003cli\u003eHow\n\u003cul\u003e\n\u003cli\u003ewe keep the language model frozen, and \u003cem\u003e\u003cstrong\u003efinetune input and output linear layers\u003c/strong\u003e\u003c/em\u003e to enable cross-modality interactions. This allows our model to process arbitrarily interleaved\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eour approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pre-trained language models in visually grounded settings.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"related-work\"\u003eRelated work\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLLMs for vision-and-language\u003c/strong\u003e\u003c/p\u003e","title":"Jing_yu_koh Grounding Language Models to Images for Multimodal Generation 2023"},{"content":"[TOC]\nTitle: See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning Author: Zhenfang Chen et. al. Publish Year: 12 Jan 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.05226.pdf Summary of paper Motivation Solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect external world knowledge, and perform step-by-step reasoning to answer the questions correctly. Contribution We propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge based visual reasoning. IPVR contains three stages, see, think, and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rational to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. Some key terms human process to handle knowledge-based visual reasoning\nDominant approaches for visual and language reasoning are mainly divided into two categories\nthe first category adds additional visual perception modules to transform the visual inputs into latent inputs for LLMs, and finetunes the models with massive vision-language data. but it requires a large vision-language dataset to finetune the LLM and the new visual modules for each downstream task, which are typically computational intensive and time consuming the second category uses prompt-based methods for visual reasoning. first translate images into captions, which can then be used as textual prompt inputs for GPT3 models to answer the question. however, their model has several limitations. first the captioning process is independent of question\u0026rsquo;s semantics, limiting the caption to focus only on the image\u0026rsquo;s general aspects instead of the question-related objects second, their pipeline cannot provide a step-by-step reasoning trace, leaving the question-answering a black-box process. Method Good things about the paper (one paragraph) The model gradually adds rationales in the prompt context to assist LLM to output the answer\nAssumption is that more context information is helping LLM to predict answers rather than disrupting the prediction. Compared with existing prompting methods, it not only achieves better performance but also maintains high transparency by keeping the whole trace of each reasoning step.\n","permalink":"https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning\u003c/li\u003e\n\u003cli\u003eAuthor: Zhenfang Chen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 12 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Feb 6, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.05226.pdf\"\u003ehttps://arxiv.org/pdf/2301.05226.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230207113442635\" loading=\"lazy\" src=\"/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSolving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect external world knowledge, and perform step-by-step reasoning to answer the questions correctly.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge based visual reasoning.\u003c/li\u003e\n\u003cli\u003eIPVR contains three stages, \u003cstrong\u003esee, think, and confirm\u003c/strong\u003e. The see stage scans the image and \u003cu\u003egrounds the visual concept candidates\u003c/u\u003e with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer.\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003econfirm\u003c/strong\u003e stage further uses the LLM to generate the supporting rational to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ehuman process to handle knowledge-based visual reasoning\u003c/strong\u003e\u003c/p\u003e","title":"Zhenfang_chen See Think Confirm Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning 2023"},{"content":"[TOC]\nTitle: A Planning Based Neural Symbolic Approach for Embodied Instruction Following Author: Xiaotian Liu et. al. Publish Year: 2022 Review Date: Thu, Feb 2, 2023 url: https://embodied-ai.org/papers/2022/15.pdf Summary of paper Motivation end-to-end deep learning methods struggle at these tasks due to long-horizon and sparse rewards. Contribution Our main innovation relies on combining DL models for perception and NLP with a new egocentric planner based on successive planning problems formulated using the PDDL syntax, both for exploration and task accomplishment. our planning framework can naturally recover from action failures at any stage of the planned trajectory. Some key terms Embodied Instruction Following\nEIF requires an agent to process multimodal information and plan over long task horizons. however, end-to-end optimization leads to entangled latent state representation where compositional and long-horizon tasks are difficult to solve other approaches use neural networks to ground visual information into persistent memory structures to store information. These approaches rely on templates of existing tasks, making them difficult to generalize to new problems or unexpected action outcomes. Classical planning in EIF\nhowever, classical planning assumes full observability, Language module\nfor task goals, we extracted features directly from labels produced by FILM authors that are trained on multiple transformers limitation the template may not cover all situations does not utilise the detailed instructions. Potential future work there can be improvements on the language parser\n","permalink":"https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  A Planning Based Neural Symbolic Approach for Embodied Instruction Following\u003c/li\u003e\n\u003cli\u003eAuthor: Xiaotian Liu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Feb 2, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://embodied-ai.org/papers/2022/15.pdf\"\u003ehttps://embodied-ai.org/papers/2022/15.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230202132958709\" loading=\"lazy\" src=\"/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eend-to-end deep learning methods struggle at these tasks due to long-horizon and sparse rewards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOur main innovation relies on combining DL models for perception and NLP with a new egocentric planner based on successive planning problems formulated using the PDDL syntax, both for exploration and task accomplishment.\u003c/li\u003e\n\u003cli\u003eour planning framework can naturally recover from action failures at any stage of the planned trajectory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eEmbodied Instruction Following\u003c/strong\u003e\u003c/p\u003e","title":"Xiaotian_liu a Planning Based Neural Symbolic Approach for Embodied Instruction Following 2022"},{"content":"[TOC]\nTitle: FILM: Following Instructions in Language With Modular Methods Author: So Yeon Min et. al. Publish Year: 16 Mar 2022 Review Date: Wed, Feb 1, 2023 url: https://arxiv.org/pdf/2110.07342.pdf Summary of paper Motivation current approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. in contrast, we propose a modular method with structured representation that build a semantic map of scene and perform exploration with a semantic search policy, to achieve natural language goal. Contribution FILM consists of several modular components that each processes language instructions into structured forms (language processing) converts egocentric visual input into a semantic metric map (Semantic Mapping) predicts a search goal location (Semantic Search Policy) ? subgoal will be plotted as a dot on the semantic top-down map outputs subsequent navigation/interaction actions (Deterministic Policy) Some key terms embodied instruction following\nthe additional challenges posed by EIF are threefold the agent has to understand compositional instructions of multiple types and subtasks, choose actions from a large action space and execute them for longer horizons, and localize objects in a fine-grained manner for interaction. however, EIF remains a very challenging task for end-to-end methods as they require the neural net to simultaneously learn state-tracking, building spatial memory, exploration, long-term planning and low-level control. Task Explanation ALFRED benchmark the agent has to complete household tasks given only natural language instructions and egocentric vision. Episodes run for a significantly longer number of steps compared to benchmarks with only single subgoals; even expert trajectories, which are maximally efficient and perform only the strictly necessary actions (without any steps to search for an object), are often longer than 70 steps. An episode is deemed \u0026ldquo;success\u0026rdquo; if the agent completes all sub-tasks within 10 failed low-level actions and 1000 max steps. METHODS at the start of an episode, the Language Processing module receives the egocentric RGB frame and updates the semantic map, if the goal object of the current subtask is not yet observed, the semantic search policy predicts a \u0026ldquo;search goal\u0026rdquo; at a coarse time scale; until the next search goal is predicted, the agent navigates to the current search goal with the deterministic policy. If the goal is observed, the deterministic policy decides low-level controls for interaction actions (e.g., Pick Up object) language processing module\nmapping natural language instruction into a structured template it seems that the instruction has to be semi-structured or it looks like we need extra training data to train the model Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: FILM: Following Instructions in Language With Modular Methods\u003c/li\u003e\n\u003cli\u003eAuthor: So Yeon Min et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 16 Mar 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2110.07342.pdf\"\u003ehttps://arxiv.org/pdf/2110.07342.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230201183341643\" loading=\"lazy\" src=\"/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecurrent approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning.\u003c/li\u003e\n\u003cli\u003ein contrast, we propose a modular method with structured representation that\n\u003col\u003e\n\u003cli\u003ebuild a semantic map of scene and\u003c/li\u003e\n\u003cli\u003eperform exploration with a semantic search policy, to achieve natural language goal.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFILM consists of several modular components that each\n\u003col\u003e\n\u003cli\u003eprocesses language instructions into structured forms (language processing)\u003c/li\u003e\n\u003cli\u003econverts egocentric visual input into a semantic metric map (Semantic Mapping)\u003c/li\u003e\n\u003cli\u003epredicts a search goal location (Semantic Search Policy) ?\n\u003col\u003e\n\u003cli\u003esubgoal will be plotted as a dot on the semantic top-down map\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eoutputs subsequent navigation/interaction actions (Deterministic Policy)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eembodied instruction following\u003c/strong\u003e\u003c/p\u003e","title":"So_yeon_min Film Following Instructions in Language With Modular Methods 2022"},{"content":"[TOC]\nTitle: Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following Author: Yuki Inoue et. al. Publish Year: 7 Nov 2022 Review Date: Wed, Feb 1, 2023 url: https://arxiv.org/pdf/2211.03267.pdf Summary of paper Motivation we propose FILM++ which extends the existing work FILM with modifications that do not require extra data. furthermore, we propose Prompter, which replace FILM++\u0026rsquo;s semantic search module with language model prompting. no training is needed for our prompting based implementation while achieving better or least comparable performance. Contribution FILM++ to fill the role of the data efficient baseline. we propose Prompter, which replaces the semantic search module of FILM++ with language prompting, making it even more data efficient. Some key terms Difficulty in converting language into robot controls\nConverting free-form language instructions to step-by-step robot controls is no easy task, as agents must integrate information of multiple modalities while operating in environments full of uncertainties. it is important to minimise the data cost needed to train an agent, to ease the transition from sim to real. Function of the semantic search module\nThe semantic search module promotes efficient search by predicting the probable locations of the unobserved objects from the observed ones. Related work early attempts on ALFRED\nearly attempts on ALFRED trained single end-to-end models.= modular approaches\nmost equipped with front-end vision and language modules that process raw inputs which are then integrated in the back-end decision making module. FILM baseline language substream\nthe language substream subdivides the language instructions into a series of object-action pairs, which serve as subtasks that agents follow to complete the task in divide-and-conquer manner. an object-action pair (Faucet, ToggleObjectOn) corresponds to first finding a faucet and then turning the knob. ALFRED settings reachable distance\nIn ALFRED, an object is considered reachable if its horizontal displacement from the agent is less than 1.5 meters. FILM directly uses the depth estimation to determine the reachability. interaction offset\nbeing too close to an object can also be a source of error. this is especially true when objects change shape after interaction. so some model manually set offset of agent from the object by 50 cm for the OpenObject action, as it is the only deforming interaction in ALFRED Slice replay\nFILM++ also manually set a macro action sequence to put away the knife and return for a pick up. Look around\nFILM++ instructs the agent to look around the environment at the beginning of an episode, to promote information gathering Obstacle enlargement\na common practice during collision-free path planning is to enlarge the obstacles by the size of of the agent so that the agent can be modelled as a point. (in the semantic map) Result this shows that if having ground truth language parser, the performance will increase by 7% \u0026ndash; meaning that there is the potential to improve current language parser. highlight about the error modes\nthe table shows that over half of Prompter\u0026rsquo;s errors correspond to \u0026ldquo;Goal object not found\u0026rdquo; or \u0026ldquo;Language processing error\u0026rdquo; Prompter is particularly bad at recognising small objects such as salt shakers, and large objects that are difficult to recognise up close, such as refrigerators and floor lamps. ","permalink":"https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following\u003c/li\u003e\n\u003cli\u003eAuthor: Yuki Inoue et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 7 Nov 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Feb 1, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2211.03267.pdf\"\u003ehttps://arxiv.org/pdf/2211.03267.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230201183430332\" loading=\"lazy\" src=\"/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose FILM++ which extends the existing work FILM with modifications that do not require extra data.\u003c/li\u003e\n\u003cli\u003efurthermore, we propose Prompter, which replace FILM++\u0026rsquo;s semantic search module with language model prompting.\u003c/li\u003e\n\u003cli\u003eno training is needed for our prompting based implementation while achieving better or least comparable performance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFILM++ to fill the role of the data efficient baseline.\u003c/li\u003e\n\u003cli\u003ewe propose Prompter, which replaces the semantic search module of FILM++ with language prompting, making it even more data efficient.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDifficulty in converting language into robot controls\u003c/strong\u003e\u003c/p\u003e","title":"Yuki_inoue Prompter Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following 2022"},{"content":"[TOC]\nTitle: Dissociating Language and Thought in Large Language Models a Cognitive Perspective Author: Kyle Mahowald et. al. Publish Year: 16 Jan 2023 Review Date: Tue, Jan 31, 2023 url: https://arxiv.org/pdf/2301.06627.pdf Summary of paper Motivation the author tried to challenge the \u0026ldquo;good at language $\\implies$ good at thought\u0026rdquo; fallacy. the second fallacy is \u0026ldquo;bad at thought $\\implies$ bad at language\u0026rdquo; Contribution the author argued that LLMs have promise as scientific models of one piece of the human cognitive toolbox \u0026ndash; formal language processing \u0026ndash; but fall short of modelling human thought. in section 4, we consider several domains required for functional linguistic competence \u0026ndash; formal reasoning, world knowledge, situation modelling and social cognitive abilities Some key terms deep learning models in linguistics\nThe view that deep learning models are not of scientific interest remains common in linguistics and psycholinguistics. two kinds of linguistic competence\nformal linguistic competence (the knowledge of rules and statistical regularities of language) problem example: what counts as a valid string of language functional linguistic competence (the ability to use language in the real world, which often draws on non-linguistic capacities) separated machinery\nthe machinery dedicated to processing language is separate from the machinery responsible for memory, reasoning and social skills. functional linguistic competence\na formal language system in isolation is useless to a language user unless it can interface with the rest of perception, cognition, and action. despite the nearly complete loss of linguistic abilities, some individuals with severe aphasia have intact non-linguistic cognitive abilities. LLMs learn hierarchical structure\nwe review evidence that LLMs learn two features that are argued by many to be central to human linguistic processing: hierarchical structure and abstraction. Both of these features address primarily the syntactic aspect of formal linguistic competence. In human languages, words combine to make compositional meanings. When a sentence has multiple words, their meanings do not simply get added linearly one by one. Instead, they can be combined hierarchically some approaches have shown that distances between LLM representations of individual words in a sentence align with the sentence\u0026rsquo; hierarchical structure rather than with linear distance between the words, thereby recovering the close structural relationship between the subject and verb of a sentence even when they are linearly far apart. LLMs learn abstraction\noverall, it seems clear that LLMs achieve at least some degree of abstraction. The degree of that abstraction remains a matter of debate. LLMs are great at pretending to think\nlanguage models trained on gigantic text corpora acquire large amounts of factual knowledge, succeed at some types of mathematical reasoning and reproduce many stereotypes and social biases. how LLMs fail -\u0026gt; any test of LLMs\u0026rsquo; ability to reason must account for their ability to use word co-occurrence patterns to \u0026ldquo;hack\u0026rdquo; the task. Situation modelling\npeople can easily follow the plot of a story that spans multiple chapters or, sometimes, multiple book volumes. We can also have a three-hour conversation with a friend, and the next day the friend will expect us to remember most of what was said. We accomplish this impressive feat not by having a dedicated memory slot for every word that we read or heard, but by abstracting away linguistic information into a situation model \u0026ndash; a mental model of entities, relations between them, and a sequence of states they had been in. Social reasoning (pragmatics and intent\nwork in cognitive science and linguistics has come to recognise that these kind of grounded, context-dependent aspects of language are not just peripheral but a central part of human language production and understanding. LLMs struggle on theory of mind task, which require inferring the intentions behind others\u0026rsquo; actions. moreover, LLMs themselves lack communicative intent. Solution Architectural modularity and emergent modularity approach\nA modular language model architecture is much better aligned with the fact that real-life language use is a complex capability, requiring both language-specific knowledge (formal competence) and various non-language-specific cognitive abilities (functional competence). Whether built-in or induced to emerge, modularity can lead the models to mirror the functional organization of the human brain and, consequently, make their behavior much more humanlike. Curated data and diverse objective functions\nWe believe that a model that succeeds at real-world language use would include–—in addition to the core language component–—a successful problem solver, a grounded experiencer, a situation modeler, a pragmatic reasoner, and a goal setter the machinery required to simulate intelligence will include both domain-general components and domain specific components (such as navigation and social reasoning). This modularity could be baked in by training modular models on a mixture of carefully curated datasets using diverse objective functions. (e.g., how GPTChat combines a pure language modelling objective with a additional human feedback objective) ","permalink":"https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Dissociating Language and Thought in Large Language Models a Cognitive Perspective\u003c/li\u003e\n\u003cli\u003eAuthor: Kyle Mahowald et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 16 Jan 2023\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Jan 31, 2023\u003c/li\u003e\n\u003cli\u003eurl: \u003ca href=\"https://arxiv.org/pdf/2301.06627.pdf\"\u003ehttps://arxiv.org/pdf/2301.06627.pdf\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230131184855294\" loading=\"lazy\" src=\"/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author tried to challenge the \u0026ldquo;good at language $\\implies$ good at thought\u0026rdquo; fallacy.\u003c/li\u003e\n\u003cli\u003ethe second fallacy is \u0026ldquo;bad at thought $\\implies$ bad at language\u0026rdquo;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author argued that LLMs have promise as scientific models of one piece of the human cognitive toolbox \u0026ndash; formal language processing \u0026ndash; but fall short of modelling human thought.\u003c/li\u003e\n\u003cli\u003ein section 4, we consider several domains required for functional linguistic competence \u0026ndash; formal reasoning, world knowledge, situation modelling and social cognitive abilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003edeep learning models in linguistics\u003c/strong\u003e\u003c/p\u003e","title":"Kyle_mahowald Dissociating Language and Thought in Large Language Models a Cognitive Perspective 2023"},{"content":"[TOC]\nTitle: Planning With Diffusion for Flexible Behaviour Synthesis Author: Michael Janner et. al. Publish Year: 21 Dec 2022 Review Date: Mon, Jan 30, 2023 Summary of paper Motivation use the diffusion model to learn the dynamics tight coupling of the modelling and planning our goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical. Some key terms ideal model-based RL\nWhy neural nets + trajectory optimisation is a headache\nLong-horizon predictions are unreliable Optimizing for reward with neural net models produces adversarial examples in trajectory spaces A generative model of trajectories\nWhat is autoregressive model\nit predicts future values based on past values. Non-autoregressive prediction\nprediction is non-autoregressive: entire trajectory is predicted simultaneously Sampling from diffuser\nsampling occurs by iteratively refining randomly-initialised trajectories Flexible behaviour synthesis through distribution composition\nguidance functions transforms an unconditional trajectory model into a conditional policy for diverse tasks. Goal planning through inpainting\nTight coupling between modelling and planning\nit requires finding trajectories that are both physically realistic under $p_\\theta(\\tau)$ and high-reward (or constraint-satisfying) under $h(\\tau)$ because the dynamics information is separated from the perturbation distribution $h(\\tau)$, a single diffusion model $p_\\theta(\\tau)$ may be reused for multiple tasks in the same environment. Algorithm\nGoal-conditioned RL as Inpainting\nsome planning problems are more naturally posed as constraint satisfaction than reward maximization. In practice, this may be implemented by sampling from the unperturbed reverse process $\\tau^{i-1} \\sim p_\\theta(\\tau^{i-1} | \\tau^i)$ and replacing the sampled values with conditioning values $c_t$ after all diffusion timesteps $i \\in {0,1,\u0026hellip;,N}$ Task compositionality\nwhile diffuser contains information about both environment dynamics and behaviours, it is independent of reward function. Because the model acts as a prior over possible futures, planning can be guided by comparatively lightweight perturbation functions $h(\\tau)$ (or even combinations of multiple perturbations) ","permalink":"https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Planning With Diffusion for Flexible Behaviour Synthesis\u003c/li\u003e\n\u003cli\u003eAuthor: Michael Janner et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 21 Dec 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Jan 30, 2023\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230130134456046\" loading=\"lazy\" src=\"/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003euse the diffusion model to learn the dynamics\u003c/li\u003e\n\u003cli\u003etight coupling of the modelling and planning\u003c/li\u003e\n\u003cli\u003eour goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eideal model-based RL\u003c/strong\u003e\u003c/p\u003e","title":"Michael_janner Planning With Diffusion for Flexible Behaviour Synthesis 2022"},{"content":"[TOC]\nTitle: Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022 Author: Publish Year: Review Date: Fri, Jan 20, 2023 Summary of paper Motivation reasoning about actions \u0026amp; changes has been widely studies in the knowledge representation community, it has recently piqued the interest of NLP and computer vision researchers. Contribution Some key terms Six most frequent types of commonsense knowledge\ntasks that involve language-based reasoning about actions\nPure language based tasks are suitable when high-level goal descriptions are to be mapped with a set of actions or for explainability purposes i.e. to provide justification about the choice of action or commonsense knowledge that is useful to make conclusions. as states become more complicated and involve many different objects, it becomes hard to convey every single detail (about object\u0026rsquo;s position, attributes such as colors, size, texture, etc) through text and require complex description to refer to objects to avoid possible ambiguity. Role of multi-modality\nmulti-modal learning aims to build models that can process and relate information from two or more modalities. Image-text multimodality has received significant interest in AI community recently as it is an important skill for humans to perform day to day tasks. Perception systems can be leveraged to identify variety of visual information and a concise way to learn through observations i.e. learn to identify or perform actions. On the other hand, language provides an effective way to exchange thoughts, communicate, query or provide justifications e.g., explaining the choice of actions while performing a task or highlighting preconditions or commonsense before performing actions. Thus multi-modal context play an important role in understanding actions and reasoning about them. The presence of multiple modalities provide natural flexibility for varied inference tasks. Instruction following\nlanguage guided image manipulation is an emerging research direction in vision+language. While a majority of dataset involve object and attribute level scene manipulations, \u0026hellip;. Another relevant task under this category is vision-and-language navigation, where an agent navigates in a visual environment to find a goal location by following linguistic instructions. All the above datasets include visuals, natural language instructions and a set of actions that can be performed to achieve desired goals. Further, ALFRED increased the complexity of level of the VLN task for agents by adding long, compositional tasks. The task comprises of dealing with longer action sequences, complex action space, and language that are closely related to real-world situations. Good things about the paper (one paragraph) Major comments Minor comments Citation\nAs a result, a significant amount of commonsense knowledge we use in our day to day life resolves around actions. reasoning about actions is important for humans as it helps us to predict if a sequence of actions will lead us to achieve the desired goal. the ability of artificial agents to perform reasoning and integrate commonsense knowledge about actions is highly desirable. In over four decades of research, the knowledge representation and reasoning (KR\u0026amp;R) community has been successful in developing promising solutions to Reasoning Action and Change (RAC) problems. ref: Sampat, Shailaja Keyur, et al. \u0026ldquo;Reasoning about actions over visual and linguistic modalities: A survey.\u0026rdquo; arXiv preprint arXiv:2207.07568 (2022). Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022\u003c/li\u003e\n\u003cli\u003eAuthor:\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Jan 20, 2023\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230120140740245\" loading=\"lazy\" src=\"/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ereasoning about actions \u0026amp; changes has been widely studies in the knowledge representation community, it has recently piqued the interest of NLP and computer vision researchers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSix most frequent types of commonsense knowledge\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg alt=\"image-20230120162054834\" loading=\"lazy\" src=\"/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/image-assets/image-20230120162054834.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003etasks that involve language-based reasoning about actions\u003c/strong\u003e\u003c/p\u003e","title":"Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022"},{"content":"[TOC]\nTitle: Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019 Author: Xin Wang et. al. Publish Year: Review Date: Wed, Jan 18, 2023 Summary of paper Motivation Visual Language Navigation (VLN) presents some unique challenges\nfirst, reasoning over images and natural language instructions can be difficult. secondly, except for strictly following expert demonstrations, the feedback is rather coarse, since the \u0026ldquo;Success\u0026rdquo; feedback is provided only when the agent reaches a target position (sparse reward) A good \u0026ldquo;instruction following\u0026rdquo; trajectory may ended up just stop before you reaching the goal state and then receive zero rewards. existing work suffer from generalisation problem. (need to retrain the agent in new environment) Implementation agent can infer which sub-instruction to focus on and where to look at. (automatic splitting long instruction) with a matching critic that evaluates an executed path by the probability of reconstructing the original instruction from the executed path. P(original instruction | past trajectory) cycle reconstruction: we have P(target trajectory | the instruction) = 1, and we want to measure P(original instruction | past trajectory) this will enhance the interpretability as now you understand how the robot was thinking about ","permalink":"https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019\u003c/li\u003e\n\u003cli\u003eAuthor: Xin Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Jan 18, 2023\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20230118095333795\" loading=\"lazy\" src=\"/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eVisual Language Navigation (VLN) presents some unique challenges\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003efirst, reasoning over images and natural language instructions can be difficult.\u003c/li\u003e\n\u003cli\u003esecondly, except for strictly following expert demonstrations, the feedback is rather coarse, since the \u0026ldquo;Success\u0026rdquo; feedback is provided only when the agent reaches a target position (sparse reward)\u003c/li\u003e\n\u003cli\u003eA good \u0026ldquo;instruction following\u0026rdquo; trajectory may ended up just stop before you reaching the goal state and then receive zero rewards.\u003c/li\u003e\n\u003cli\u003eexisting work suffer from generalisation problem. (need to retrain the agent in new environment)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"implementation\"\u003eImplementation\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eagent can infer which sub-instruction to focus on and where to look at. (automatic splitting long instruction)\u003c/li\u003e\n\u003cli\u003ewith a matching critic that evaluates an executed path by the probability of reconstructing the original instruction from the executed path. P(original instruction | past trajectory)\n\u003col\u003e\n\u003cli\u003ecycle reconstruction: we have P(target trajectory | the instruction) = 1, and we want to measure P(original instruction | past trajectory)\u003c/li\u003e\n\u003cli\u003ethis will enhance the interpretability as now you understand how the robot was thinking about\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e","title":"Xin_wang Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019"},{"content":"[TOC]\nTitle: PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning Author: Alekh Agarwal et. al. Publish Year: Review Date: Wed, Dec 28, 2022 Summary of paper Motivation The primary drawback of direct policy gradient methods is that, by being local in nature, they fail to adequately explore the environment. In contrast, while model-based approach and Q-learning directly handle exploration through the use of optimism. Contribution Policy Cover-Policy Gradient algorithm (PC-PG), a direct, model-free, policy optimisation approach which addresses exploration through the use of a learned ensemble of policies, the latter provides a policy cover over the state space. the use of a learned policy cover address exploration, and also address what is the catastrophic forgetting problem in policy gradient approaches (which use reward bonuses); the on-policy algorithm, where approximation errors due to model mispecification amplify (see [Lu et al., 2018] for discussion) Some key terms suffering from sparse reward\nThe assumptions in these works imply that the state space is already well-explored. Conversely, without such coverage (and, say, with sparse rewards), policy gradients often suffer from the vanishing gradient problem. original objective function and coverage of state space\nwider coverage objective\niterative algorithm PC-PG\nthe idea is to successively improve both the current policy $\\pi$ and the coverage distribution the algorithm starts with some policy $\\pi^0$, and works in episodes. a bonus bn in order to encourage the algorithm to find a policy πn+1 which covers a novel part of the state-action space Potential future work so exploration essentially means we need to have a good state coverage for our training trajectory so that convergence to optimum is guaranteed. ","permalink":"https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Alekh Agarwal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year:\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221228144306599\" loading=\"lazy\" src=\"/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe primary drawback of direct policy gradient methods is that, by being local in nature, they fail to adequately explore the environment.\u003c/li\u003e\n\u003cli\u003eIn contrast, while model-based approach and Q-learning directly handle exploration through the use of optimism.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePolicy Cover-Policy Gradient algorithm (PC-PG), a direct, model-free, policy optimisation approach which addresses exploration through the use of a learned ensemble of policies, the latter provides a policy cover over the state space.\n\u003cul\u003e\n\u003cli\u003ethe use of a learned policy cover address exploration, and also address what is the catastrophic forgetting problem in policy gradient approaches (which use reward bonuses);\u003c/li\u003e\n\u003cli\u003ethe on-policy algorithm, where approximation errors due to model mispecification amplify (see [Lu et al., 2018] for discussion)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003esuffering from sparse reward\u003c/strong\u003e\u003c/p\u003e","title":"Alekh_agarwal PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning 2020"},{"content":"[TOC]\nTitle: On the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020 Author: Alekh Agarwal et. al. Publish Year: 14 Oct 2020 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution and how they cope with approximation error due to using a restricted class of parametric policies. Contribution One central contribution of this work is in providing approximation guarantees that are average case - which avoid explicit worst-case dependencies on the size of state space \u0026ndash; by making a formal connection to supervised learning under distribution shift. This characterisation shows an important between estimation error, approximation error and exploration (as characterised through a precisely defined condition number) Some key terms basic theoretical convergence questions\nif and how fast they converge to a globally optimal solution (say with a sufficiently rich policy class) how they cope with approximation error due to using a restricted class of parametric policies how they cope with approximation error their finite sample behaviour. tabular policy parameterisation\nthere is one parameter per state-action pair so the policy class is complete in that it contains the optimal policy function approximation\nwe have a restricted class or parametric policies which may not contain the globally optimal policy. convergence rates (IMPORTANT)\nit depends on the optimisation measure having coverage over the state space, as measured by the distribution mismatch coefficient $D_\\infty$ $D_\\infty$ is a measure of the coverage of this initial distribution note that the convergence rate has no dependence on the number of states or the number of actions, nor does it depend on the distribution mismatch coefficient $D_\\infty$ the convergence rate analysis uses a mirror descent style of analysis in that the worst-case density ratio only depends on the state visitation distribution of an optimal policy excess risk\nthe regret the estimation error Apprixmation error\ndue to that the best linear fit using $w_\\star^{(t)}$ may not perfectly match the Q-value. non-stationary policy\npolicy might change over time Concentrability coefficient\nConcentrability ensures that the ratio between the induced state-action distribution of any non-stationary policy and the state-action distribution in the batch data is upper bounded by a constant, called the concentrability coefficient. overview of approximate methods\nthe suboptimality $V^\\star (s_0) - V^\\pi (s_0)$ after T iterations for various approximate algorithm, which use different notions of approximation error. $D_\\infty \\leq C_\\infty$ distribution mismatch coefficient is smaller than concentrability coefficient as discussed in our policy optimisation approach, the analysis of both computational and statistical complexities are straightforward, since we can leverage known statistical and computational results from stochastic approximation literature. The performance difference lemma\nThe distribution mismatch coefficient\nwe often characterise the difficulty of the exploration problem faced by our policy optimisation algorithms when maximising the objective $V^\\pi(\\mu)$ through the following notion of distribution mismatch coefficient 越大说明 explore 很差, the hardness of the exploration problem is captured through the distribution mismatch coefficient. discounted state visitation distribution $d_{s_0}^\\pi(s)$ Given a policy $\\pi$ and measures $\\rho, \\mu \\in \\Delta(S)$, we refer to $||\\frac{d_\\rho^\\pi}{\\mu}||_\\infty$, $\\rho$ distribution of starting states $\\mu$ is fitting state distribution, (initial) state distribution Potential future work we can use this suboptimality after T iterations to prove that the PPO is slowed down by due to distribution mismatch coefficient gets harder. ","permalink":"https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: On the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020\u003c/li\u003e\n\u003cli\u003eAuthor: Alekh Agarwal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 14 Oct 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221228143829438\" loading=\"lazy\" src=\"/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elittle is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution and how they cope with approximation error due to using a restricted class of parametric policies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOne central contribution of this work is in providing approximation guarantees that are average case - which avoid explicit worst-case dependencies on the size of state space \u0026ndash; by making a formal connection to supervised learning under distribution shift. This characterisation shows an important between estimation error, approximation error and exploration (as characterised through a precisely defined condition number)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ebasic theoretical convergence questions\u003c/strong\u003e\u003c/p\u003e","title":"Alekh_agarwal on the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020"},{"content":"[TOC]\nTitle: Revisiting Design Choices in Proximal Policy Optimisation Author: Chloe Ching-Yun Hsu et. al. Publish Year: 23 Sep 2020 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation Contribution on discrete action space with sparse high rewards, standard PPO often gets stuck at suboptimal actions. Why analyze the reason fort these failure modes and explain why they are not exposed by standard benchmarks In summary, our study suggests that Beta policy parameterization and KL-regularized objectives should be reconsidered for PPO, especially when alternatives improves PPO in all settings. The author proved the convergence guarantee for PPO-KL penalty version, as it inherits convergence guarantees of mirror descent for policy families that are closed under mixture Some key terms design choices\nFailure modes of standard PPO\nthe clipping mechanism effectively prevents the policy from moving further away once it is outside the trust region, but it does not bound the size of an individual policy update step. this behaviour is particularly problematic if a single reward signal can cause the policy to end up in region with low reward signal. the abruptly vanishing reward signal outside the interval [-1.0, -0.8] is the main culprit reason as soon as the $\\epsilon$ threshold is exceeded, the clipping mechanism effectively prevents the policy from moving further away in subsequent iterations. However, it does not regularize the individual policy update steps to stay inside the trust region. Thus, if update steps are large, it can cause the policy to move far away from the old policy (the learning rate issue) Good things about the paper (one paragraph) Failure mode 2 due to high dimensional discrete action space\nthe problem is that when the clipped objective see only the bad actions (reward 0) and the suboptimal actions (reward 0.5) without seeing the optimal action, it tends to increase the probability ratio of the suboptimal actions by (1 + eps), as maximally permitted by the clipping mechanism. After increasing the probability of suboptimal actions in several iteration we further note that alternative approaches such as reducing learning rate and increasing batch size can only partially mitigate the issue, while PPO with reverse KL regularisation succeeds on all 20 runs. While the high-dimensionality of action spaces is one aspect of the classical exploration-exploitation tradeoff, existing RL research around exploration mostly focuses on continuous, rather than discrete action space. Advantages of KL-regularization\nKL-regularized PPO enjoys convergence guarantees when the parameterized policy class is closed under mixture ref: Liu, Boyi, et al. \u0026ldquo;Neural proximal/trust region policy optimization attains globally optimal policy.\u0026rdquo; arXiv preprint arXiv:1906.10306 (2019). Major comments Citation\nWhen the state space S is large, sampling the full action space A on all states would require a batch size of |S| × |A|. Potential future work Failure mode 2 might be very relevant to our case\n","permalink":"https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Revisiting Design Choices in Proximal Policy Optimisation\u003c/li\u003e\n\u003cli\u003eAuthor: Chloe Ching-Yun Hsu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 23 Sep 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221228143502296\" loading=\"lazy\" src=\"/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eon discrete action space with sparse high rewards, standard PPO often gets stuck at suboptimal actions. Why analyze the reason fort these failure modes and explain why they are not exposed by standard benchmarks\u003c/li\u003e\n\u003cli\u003eIn summary, our study suggests that Beta policy parameterization and KL-regularized objectives should be reconsidered for PPO, especially when alternatives improves PPO in all settings.\u003c/li\u003e\n\u003cli\u003eThe author proved the convergence guarantee for PPO-KL penalty version, as it inherits convergence guarantees of mirror descent for policy families that are closed under mixture\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003edesign choices\u003c/strong\u003e\u003c/p\u003e","title":"Chloe_ching_yun_hsu Revisiting Design Choices in Proximal Policy Optimisation 2020"},{"content":"[TOC]\nTitle: Generalized Proximal Policy Optimisation With Sample Reuse 2021 Author: James Queeney et. al. Publish Year: 29 Oct 2021 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. Contribution in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO this motivate an off-policy version of the popular algorithm that we call GePPO. we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency Some key terms sample complexity\nRepresents the number of training-samples that it needs in order to successfully learn a target function. PPO theoretical guarantee\nthe method is motivated by a lower bound on the expected performance loss at every update, which can be approximated using sample generated by the current policy. the theoretically supported stability of these methods is very attractive but need for on-policy data and the high-variance nature of reinforcement learning often require significant data to be collected between every update, resulting in high sample complexity and slow learning reuse samples\nallow data to be reused to calculate multiple policy updates but also causes the distribution of data to shift away from the distribution generated by the current policy. this distribution shift invalidates the standard performance guarantees used in on-policy methods, and can lead to instability in the training process. popular off-policy algorithms often require various implementation tricks and extensive hyperparameter tuning to control the instability caused by off-policy data. On-policy policy improvement methods\nthe goal of monotonic policy improvement was first introduced by Kakade and Langford [14] in Conservative Policy Iteration, which maximise a lower bound on policy improvement that can be constructed using samples from the current policy. This theory of policy improvement has served as a fundamental building block in the design of on-policy deep RL methods to best of our knowledge, we are the first to directly relate the clipping mechanism in PPO to the total variation distance between policies sample efficiency with off-policy data\na common approach to improving the sample efficiency of model-free reinforcement learning is to reuse samples collected under prior policies. Popular off-policy policy gradient approaches are DDPG, SAC. these methods are not motivated by policy improvement guarantees, and do not directly control the bias introduced by off-policy data. combining on-policy and off-policy\nthis is the goal of balancing the variance of on-policy methods and bias of off-policy methods Gu et al. [10] demonstrated that the bias introduced by off-policy data is related to the KL divergence between the current policy and the behavior policy used to generate the data. Some other methods are related to the penalty term that appears in our genralized policy improvement lower bound, which can be bounded by a penalty that depends on KL divergence state visitation distribution\nPolicy improvement lower bound\nwe refer to the first term of the lower bound in lemma 1 as the surrogate objective, and the second term as the penalty term. we can guarantee policy improvement at every step of the learning process by choosing the next policy $\\pi_{k+1}$ to maximise this lower bound. because the expectations in Lemma 1 depend on the current policy $\\pi_k$, we can approximate this lower bound using samples generated by the current policy. PPO why this is called proximal\nit theoretically motivated by the policy improvement lower bound in Lemma 1. Rather than directly maximizing this lower bound, PPO considers the goal of maximizing the surrogate objective while constraint the next policy to be close to the current policy using CLIP often the number of samples must be large in order for the empirical objective to be an accurate estimator for the true objective. because these samples must be collected under the current policy between every policy update, PPO can be very sample intensive. Generalised policy improvement lower bound\nin order to retain the stability benefits of PPO while reusing samples from prior policies, we must incorporate these off-policy samples in a principled way. proof: we generalise Lemma 1 to depend on expectations with respect to any reference policy, and we apply this result M times for the past policies. Then, the convex combination determined by the distribution determined by $\\nu$ of the resulting M policy improvement lower bounds is also a lower bound. this lower bound provides theoretical support for extending PPO to include off-policy samples The penalty term is Theorem 1 suggests that we should control the expected total variance distance between the future policy $\\pi$ and the last $M$ policies. this means we can effectively control the expected performance loss at every policy update by controlling the expected total variance distance between consecutive policies. the clipping mechanism in PPO approximately accomplish this task. The expected total variation distance between the current policy $\\pi_k$ and the future policy pi\nSample efficiency analysis (IMPORTANT) in order to compare GePPO to PPO, we first must determine the appropriate choice of clipping parameter that results in the same worst-case expected performance loss at every update. this shows that we must make smaller policy updates compared to PPO in terms of total variation distance, which is a result of utilizing samples from prior policies. Assume we require N=Bn samples for the empirical objective to be a sufficiently accurate estimate of the true objective, where n is the smallest possible batch size we can collect and B is some positive integer In this setting, PPO makes one policy update per N samples collected, while GePPO leverage data from prior policies to make B updates per N samples collected as long as M \u0026gt;= B. first we see that GePPO can increase the change in total variance distance of policy throughout training compared to PPO in same training sample number. Potential future work give you some insight about how to theoretically analyse the learning efficiency\n","permalink":"https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Generalized Proximal Policy Optimisation With Sample Reuse 2021\u003c/li\u003e\n\u003cli\u003eAuthor: James Queeney et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 29 Oct 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221228140752324\" loading=\"lazy\" src=\"/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms.\u003c/li\u003e\n\u003cli\u003eWe develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO\u003c/li\u003e\n\u003cli\u003ethis motivate an off-policy version of the popular algorithm that we call GePPO.\u003c/li\u003e\n\u003cli\u003ewe demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003esample complexity\u003c/strong\u003e\u003c/p\u003e","title":"James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021"},{"content":"[TOC]\nTitle: BackdooRL Backdoor Attack Against Competitive Reinforcement Learning 2021 Author: Lun Wang et. al Publish Year: 12 Dec 2021 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation in this paper, we propose BACKDOORL, a backdoor attack targeted at two player competitive reinforcement learning systems. first the adversary agent has to lead the victim to take a series of wrong actions instead of only one to prevent it from winning. Additionally, the adversary wants to exhibit the trigger action in as few steps as possible to avoid detection. Contribution we propose backdoorl, the first backdoor attack targeted at competitive reinforcement learning systems. The trigger is the action of another agent in the environment. We propose a unified method to design fast-failing agent for different environment We prototype BACKDOORL and evaluate it in four environments. The results validate the feasibility of backdoor attacks in competitive environment We study the possible defenses for backdoorl. The results show that fine-tuning cannot completely remove the backdoor. Some key terms backdoorl workflow\nDefense\none possible defense is to fine-tune (or un-learn) the victim network by retraining with additional normal episodes. Additionally, we notice that even fine-tuning for more epochs cannot further improve the winning rate. ","permalink":"https://sino-huang.github.io/posts/lun_wang-backdoorl-backdoor-attack-against-competitive-reinforcement-learning-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  BackdooRL Backdoor Attack Against Competitive Reinforcement Learning 2021\u003c/li\u003e\n\u003cli\u003eAuthor: Lun Wang et. al\u003c/li\u003e\n\u003cli\u003ePublish Year: 12 Dec 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, we propose BACKDOORL, a backdoor attack targeted at two player competitive reinforcement learning systems.\u003c/li\u003e\n\u003cli\u003efirst the adversary agent has to lead the victim to take a series of wrong actions instead of only one to prevent it from winning.\u003c/li\u003e\n\u003cli\u003eAdditionally, the adversary wants to exhibit the trigger action in as few steps as possible to avoid detection.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose backdoorl, the first backdoor attack targeted at competitive reinforcement learning systems. The trigger is the action of another agent in the environment.\u003c/li\u003e\n\u003cli\u003eWe propose a unified method to design fast-failing agent for different environment\u003c/li\u003e\n\u003cli\u003eWe prototype BACKDOORL and evaluate it in four environments. The results validate the feasibility of backdoor attacks in competitive environment\u003c/li\u003e\n\u003cli\u003eWe study the possible defenses for backdoorl. The results show that fine-tuning cannot completely remove the backdoor.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ebackdoorl workflow\u003c/strong\u003e\u003c/p\u003e","title":"Lun_wang Backdoorl Backdoor Attack Against Competitive Reinforcement Learning 2021"},{"content":"[TOC]\nTitle: Adversarial Attacks on Neural Network Policies Author: Sandy Huang et. al. Publish Year: 8 Feb 2017 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation in this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Contribution we characterise the degree of vulnerability across tasks and training algorithm, for a subclass of adversarial example attacks in white-box and black-box settings.\nregardless of the learned tasks or training algorithm, we observed a significant drop in performance, even with small adversarial perturbations that do not interfere with human perceptions.\nmain contribution is to characterise how the efffectiveness of adversarial examples is impacted by two factors: the deep RL algorithm used to learn the policy and whether the adversary has access to the policy network\nLimitation\nThe perturbation type is only negate the sign of the signal Some key terms adversarial example crafting with the fast gradient sign method\nit is common to use a computationally cheap method of generating adversarial perturbations, even if this reduces the attack rate somewhat. Fast Gradient Sign Method (FGSM) focuses on adversarial perturbations where each pixel of the input image is changed by no more than $\\epsilon$, given a linear function $g(x) = w^T x$, the optimal adversarial perturbation $\\eta$ that satisfies $||\\eta||_\\infty \u0026lt; \\epsilon$ $\\eta = \\epsilon \\text{ sign}(w)$ In essence, FGSM is to add the noise (not random noise) whose direction is the same as the gradient of the cost function with respect to the data. sign is the direction of the gradient, not the magnitude Applying FGSM to Policies\nresults Vulnerability to white box attacks\nwe find that regardless of which game the policy is trained for or how it is trained, it is indeed possible to significantly decrease the policy\u0026rsquo;s performance through introducing relatively small perturbation. we see that policies trained with A3C, TRPO, and DQN are all susceptible to adversarial inputs. Interestingly, policies trained with DQN are more susceptible. ","permalink":"https://sino-huang.github.io/posts/sandy_huang-adversarial-attacks-on-neural-network-policies-2017/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Adversarial Attacks on Neural Network Policies\u003c/li\u003e\n\u003cli\u003eAuthor: Sandy Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 8 Feb 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Dec 28, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewe characterise the degree of vulnerability across tasks and training algorithm, for a subclass of adversarial example attacks in white-box and black-box settings.\u003c/p\u003e","title":"Sandy_huang Adversarial Attacks on Neural Network Policies 2017"},{"content":"[TOC]\nTitle: Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning Author: Yinglun Xu et. al. Publish Year: 30 May 2022 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation we study data poisoning attacks on online deep reinforcement learning (DRL) where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment. we instantiate our framework to construct several attacks which only corrupts the rewards for a small fraction of the total training timesteps and make the agent learn a low performing policy Contribution result show that the reward attack efficiently poison agent learning with a variety of SOTA DRL algorithm such as DQN, PPO our attack can work on model-free DRL algorithm for all popular learning paradigms, and only assume the learning algorithm to be efficient. large enough reward poisoning attack in the right direction is able to disrupt the DRL algorithm. limitation\nthis research assume the attack has a limited budget. But if they prove that this can disrupt the DRL, then our consistent false positive rewards can for sure disrupt the algorithm Some key terms LPE (learned policy evasion) attack\nmake all policies of good performance appear bad to the learning agent. Intuitively, policies of good performance should share similar behaviour as there is usually certain general strategy to behave well under the environment. Therefore if the attacker can make the actions correspond to such behaviour look bad. then all the good policies will appear bad to the agent. Uniformly at random (UR) attack\nit means random attack that used up all the budget. Potential future work we can get some insight about the theoretical analysis of their attack methods based on certain assumptions on the efficiency of the DRL algorithm which yields several insightful implications\n","permalink":"https://sino-huang.github.io/posts/yinglun_xu-efficient-reward-poisoning-attacks-on-online-deep-reinforcement-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Yinglun Xu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 30 May 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe study data poisoning attacks on online deep reinforcement learning (DRL) where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment.\u003c/li\u003e\n\u003cli\u003ewe instantiate our framework to construct several attacks which only corrupts the rewards for a small fraction of the total training timesteps and make the agent learn a low performing policy\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eresult show that the reward attack efficiently poison agent learning with a variety of SOTA DRL algorithm such as DQN, PPO\u003c/li\u003e\n\u003cli\u003eour attack can work on model-free DRL algorithm for all popular learning paradigms, and only assume the learning algorithm to be efficient.\u003c/li\u003e\n\u003cli\u003elarge enough reward poisoning attack in the right direction is able to disrupt the DRL algorithm.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003elimitation\u003c/strong\u003e\u003c/p\u003e","title":"Yinglun_xu Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning 2022"},{"content":"[TOC]\nTitle: Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning Author: Young Wu et. al. Publish Year: 1 Dec 2022 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Contribution unlike attacks on single-agent RL, we show that the attacker can install the target poilcy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow. This attack can be significantly cheaper than separate single-agent attacks. Limitation\nthe goal of the attacker is still wanting the agents to learn a target policy $\\pi^\\dagger$ rather than slowing down the learning speed. Some key terms susceptible\nwhile the above empirical success is encouraging, MARL algorithms are susceptible to data poisoning attacks: the agent can reach the wrong equilibria if an exogenous attacker manipulate the feedback to agents. Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/young_wu-reward-poisoning-attacks-on-offline-multi-agent-reinforcement-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Young Wu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 1 Dec 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eunlike attacks on single-agent RL, we show that the attacker can install the target poilcy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow.\u003c/li\u003e\n\u003cli\u003eThis attack can be significantly cheaper than separate single-agent attacks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation\u003c/strong\u003e\u003c/p\u003e","title":"Young_wu Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning 2022"},{"content":"[TOC]\nTitle: Robust Policy Gradient Against Strong Data Corruption Author: Xuezhou Zhang et. al. Publish Year: 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Abstract Contribution the author utilised a SVD-denoising technique to identify and remove the possible reward perturbations this approach gives a robust RL algorithm Limitation\nThis approach only solve the attack perturbation that is not consistent. (i.e. not stealthy) Some key terms Policy gradient methods\na popular class of RL methods among practitioners, as they are amenable to parametric policy classes, resilient to modelling assumption mismatches Practicability of the existing works on robust RL\nthe majority of these work focuses on the setting of tabular MDPs and cannot be applied to real-world RL problems that have large state and action space and require function approximation. policy gradient methods can be viewed as a stochastic gradient ascent method\nConceptually, policy gradient methods can be viewed as a stochastic gradient ascent method, where each iteration can be simplified as $\\theta^{(t+1)} = \\theta^{(t)} + g^{(t)} $ where $g$ is the gradient step that ideally points in the direction of fastest policy improvement. Assuming that $g^{(t)}$ is a good estimate of the gradient direction, then a simple attack strategy is try to perturb $g^{(t)}$ to point in the $-g^{(t)}$ direction, in which case the policy , rather than improving, will deteriorate as learning proceed. Major comments Citation\nWHY RL agent need robustness\nIn fact, data corruption can be a larger threat in the RL paradigm than in traditional supervised learning, because supervised learning is often applied in a controlled environment where data are collected and cleaned by highly-skilled data scientists and domain experts, whereas RL agents are developed to learn in the wild using raw feedbacks from the environment ref: Zhang, Xuezhou, et al. \u0026ldquo;Robust policy gradient against strong data corruption.\u0026rdquo; International Conference on Machine Learning. PMLR, 2021. Potential future work we can use the explanation\n","permalink":"https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Robust Policy Gradient Against Strong Data Corruption\u003c/li\u003e\n\u003cli\u003eAuthor: Xuezhou Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"abstract\"\u003eAbstract\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221227203806030\" loading=\"lazy\" src=\"/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/image-assets/image-20221227203806030.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author utilised a SVD-denoising technique to identify and remove the possible reward perturbations\u003c/li\u003e\n\u003cli\u003ethis approach gives a robust RL algorithm\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThis approach only solve the attack perturbation that is not consistent. (i.e. not stealthy)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ePolicy gradient methods\u003c/strong\u003e\u003c/p\u003e","title":"Xuezhou_zhang Robust Policy Gradient Against Strong Data Corruption 2021"},{"content":"[TOC]\nTitle: Defense Against Reward Poisoning Attacks in Reinforcement Learning Author: Kiarash Banihashem et. al. Publish Year: 20 Jun 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true unpoisoned rewards while computing their policies under the poisoned rewards. Contribution we formalise this reasoning and characterize the utility of our novel framework for designing defense policies. In summary, the key contributions include\nwe formalise the problem of finding defense policies that are effective against reward poisoning attacks that minimally modify the original reward function to achieve their goal we introduce a novel optimisation framework for designing defense policies against reward poisoning attacks \u0026ndash; this framework focuses on optimizing the agent\u0026rsquo;s worst-case utility among the set of reward functions that are plausible candidates of the true reward function we provide characterisation results that establish lower bounds on the performance of defense policies derived from our optimisation framework, and upper bounds on the suboptimality of these defense policies compared to the target policy we empirically demonstrate the effectiveness of our approach using numerical simulations to our knowledge, this is the first framework for studying this type of defenses against reward poisoning attacks that try to force a target policy at minimal cost\nit defines a novel optimisation objective to guarantee the lower bound of the performance of the defense agent under reward poisoning attacks.\nLimitation\nsample efficiency is not the focus of this study. Some key terms score of policy\n$\\mathbb E[(1-\\gamma) \\sum_{t=1}^\\infty \\gamma^{t-1} R(s_t, a_t) | \\pi, \\sigma]$\nthis is the total expected return scaled by a factor of $1 - \\gamma$ attack model\nthe proposed optmisation objective\nthis is the optimisation problem of maximising the worst case performance of the agent. ","permalink":"https://sino-huang.github.io/posts/kiarash_banihashem-defense-against-reward-poisoning-attacks-in-reinforcement-learning-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Defense Against Reward Poisoning Attacks in Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Kiarash Banihashem et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 20 Jun 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eour goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true unpoisoned rewards while computing their policies under the poisoned rewards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ewe formalise this reasoning and characterize the utility of our novel framework for designing defense policies. In summary, the key contributions include\u003c/p\u003e","title":"Kiarash_banihashem Defense Against Reward Poisoning Attacks in Reinforcement Learning 2021"},{"content":"[TOC]\nTitle: Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments Author: Amin Rakhsha et. al. Publish Year: 16 Feb 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Our attack makes minimum assumptions on the prior knowledge of the environment or the learner\u0026rsquo;s learning algorithm. most of the prior work makes strong assumptions on the knowledge of adversary \u0026ndash; it often assumed that the adversary has full knowledge of the environment or the agent\u0026rsquo;s learning algorithm or both. under such assumptions, attack strategies have been proposed that can mislead the agent to learn a nefarious policy with minimal perturbation to the rewards. Contribution We design a novel black-box attack, U2, that can provably achieve a near-matching performance to the SOTA white-box attack, demonstrating the feasibility of reward poisoning even in the most challenging black-box setting. limitation\nthis focuses on learning a nerfarious poilcy rather than slowing down the learning process. Some key terms online reward poisoning\nin online settings, reward poisoning is first introduced and studies in multi-armed bandits (ref: Data poisoning attacks in contextual bandits), where the authors show that adversarially perturbed reward can mislead standard bandit algorithm to pull a suboptimal arm or suffer large regret. poisoning attacks and teaching\npoisoning attacks is mathematically equivalent to the formulation of machine teaching with the teacher being the adversary. however, these works only consider supervised learning settings ","permalink":"https://sino-huang.github.io/posts/amin_rakhsha-reward-poisoning-in-reinforcement-learning-attacks-against-unknown-learners-in-unknown-environments-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments\u003c/li\u003e\n\u003cli\u003eAuthor: Amin Rakhsha et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year:  16 Feb 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eOur attack makes minimum assumptions on the prior knowledge of the environment or the learner\u0026rsquo;s learning algorithm.\u003c/li\u003e\n\u003cli\u003emost of the prior work makes strong assumptions on the knowledge of adversary \u0026ndash; it often assumed that the adversary has full knowledge of the environment or the agent\u0026rsquo;s learning algorithm or both.\u003c/li\u003e\n\u003cli\u003eunder such assumptions, attack strategies have been proposed that can mislead the agent to learn a nefarious policy with minimal perturbation to the rewards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe design a novel black-box attack, U2, that can provably achieve a near-matching performance to the SOTA white-box attack, demonstrating the feasibility of reward poisoning even in the most challenging black-box setting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003elimitation\u003c/strong\u003e\u003c/p\u003e","title":"Amin_rakhsha Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments 2021"},{"content":"[TOC]\nTitle: Adaptive Reward Poisoning Attacks Against Reinforcement Learning Author: Xuezhou Zhang et. al. Publish Year: 22 Jun, 2020 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$ whereas non-adaptive attacks require exponential steps. Contribution we provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe. similar to this paper, it shows that reward attack has its limit we provide a corresponding upper threshold above which the attack is feasible. we characterise conditions under which such attacks are guaranteed to fail (thus RL is safe), and vice versa in the case where attack is feasible, we provide upper bounds on the attack cost in the processing of achieving bad poliy we show that effective attacks can be found empirically using deep RL techniques. Some key terms feasible attack category\nnon-adaptive the reward attack $\\delta$ depends only on $(s_t, a_t, a_{s+1})$ or adaptive where $\\delta$ depends further on the RL agent\u0026rsquo;s learning process. attack infeasibility\nthere is a threshold such that for small reward attack, the RL agent is eventually safe. potential based reward shaping\nwe can prove that language reward shaping is a potential based reward shaping if we define $\\phi$ as a function outputting the number of completed sub-goals. potential-based reward shaping (Ng et al., 1999) has been shown able to speed up learning which preserving the optimal policy. Weak infeasibility certificate\nBoundedness of Q-learning\ngeometric series closed form Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/xuezhou_zhang-adaptive-reward-poisoning-attacks-against-reinforcement-learning-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Adaptive Reward Poisoning Attacks Against Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Xuezhou Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Jun, 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Dec 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNon-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$\u003c/li\u003e\n\u003cli\u003ewhereas non-adaptive attacks require exponential steps.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe.\n\u003cul\u003e\n\u003cli\u003esimilar to this \u003ca href=\"https://sino-huang.github.io/posts/yunhan_huang-manipulating-reinforcement-learning-stealthy-attacks-on-cost-signals-2020/\"\u003epaper\u003c/a\u003e, it shows that reward attack has its limit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewe provide a corresponding upper threshold above which the attack is feasible.\u003c/li\u003e\n\u003cli\u003ewe characterise conditions under which such attacks are guaranteed to fail (thus RL is safe), and vice versa\u003c/li\u003e\n\u003cli\u003ein the case where attack is feasible, we provide upper bounds on the attack cost in the processing of achieving bad poliy\u003c/li\u003e\n\u003cli\u003ewe show that effective attacks can be found empirically using deep RL techniques.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003efeasible attack category\u003c/strong\u003e\u003c/p\u003e","title":"Xuezhou_zhang Adaptive Reward Poisoning Attacks Against Reinforcement Learning 2020"},{"content":"[TOC]\nTitle: Reward Delay Attacks on Deep Reinforcement Learning Author: Anindya Sarkar et. al. Publish Year: 8 Sep 2022 Review Date: Mon, Dec 26, 2022 Summary of paper Motivation we present novel attacks targeting Q-learning that exploit a vulnerability entailed by this assumption by delaying the reward signal for a limited time period. We evaluate the efficacy of the proposed attacks through a series of experiments. Contribution our first observation is that reward-delay attacks are extremely effective when the goal for the adversarial is simply to minimise reward. we find that some mitigation method remains insufficient to ensure robustness to attacks that delay, but preserve the order, of rewards. Conclusion\nOur results thus suggest that even a relatively short delay in the reward signal can lead DQN learning to be entirely ineffective. Our empirical findings suggest that it is possible to induce a sub-optimal policy by strategically reshuffling the true reward sequence. Even randomly shuffling reward within relatively short time intervals is already sufficient to cause learning failure. Reward delay attack also has a disastrous effect on DQN learning, implying that the DRL process can be easily disrupted when reward channel is corrupted. Some key terms Synchrony\nOur attack exploits a common assumption of synchrony in reinforcement learning algorithms. Specifically, we assume that the adversary can delay rewards a bounded number of time steps (for example, by scheduling tasks computing a reward at time t after the task computing a reward at time t+k for some integer k \u0026gt;= 0) reward shifting attacks\nan adversary can only drop rewards, or shift these a bounded number of steps into the future. untargeted attack\nsimply aim to minimise the reward obtained by the learned policy Untargeted reward delay attacks\nwe investigate the efficacy of the untargeted reward delay attacks as we change $\\delta$, the maximum delay we can add to a reward (i.e., the maximum we can shift reward back in time relative to the rest of DQN update information) what is surprising, however, is that this improvement is extremely slight, even though we doubled the amount of time the reward can be delayed. Our results thus suggest that even a relatively short delay in the reward signal can lead DQN learning to be entirely ineffective. ","permalink":"https://sino-huang.github.io/posts/anindya_sarkar-reward-delay-attacks-on-deep-reinforcement-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward Delay Attacks on Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Anindya Sarkar et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 8 Sep 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Dec 26, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe present novel attacks targeting Q-learning that exploit a vulnerability entailed by this assumption by delaying the reward signal for a limited time period.\u003c/li\u003e\n\u003cli\u003eWe evaluate the efficacy of the proposed attacks through a series of experiments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eour first observation is that reward-delay attacks are extremely effective when the goal for the adversarial is simply to minimise reward.\u003c/li\u003e\n\u003cli\u003ewe find that some mitigation method remains insufficient to ensure robustness to attacks that delay, but preserve the order, of rewards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/p\u003e","title":"Anindya_sarkar Reward Delay Attacks on Deep Reinforcement Learning 2022"},{"content":"[TOC]\nTitle: Proximal Policy Optimisation Explained Blog Author: Xiao-Yang Liu; DI engine Publish Year: May 4, 2021 Review Date: Mon, Dec 26, 2022 Highly recommend reading this blog https://lilianweng.github.io/posts/2018-04-08-policy-gradient/ https://zhuanlan.zhihu.com/p/487754664 Difference between on-policy and off-policy\nFor on-policy algorithms, they update the policy network based on the transitions generated by the current policy network. The critic network would make a more accurate value-prediction for the current policy network in common environments. For off-policy algorithms, they allow to update the current policy network using the transitions from old policies. Thus, the old transitions could be reutilized, as shown in Fig. 1 the points are scattered on trajectories that are generated by different policies, which improves the sample efficiency and reduces the total training steps. Question: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit. PPO solves the problem of sample efficiency by utilizing surrogate objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both 1. regularizes the policy update and enables the 2. reuse of training data. Algorithm explanation Generalized advantage estimator (GAE) total PPO loss ","permalink":"https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Proximal Policy Optimisation Explained Blog\u003c/li\u003e\n\u003cli\u003eAuthor: Xiao-Yang Liu; DI engine\u003c/li\u003e\n\u003cli\u003ePublish Year: May 4, 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Dec 26, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eHighly recommend reading this blog\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\"\u003ehttps://lilianweng.github.io/posts/2018-04-08-policy-gradient/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/487754664\"\u003ehttps://zhuanlan.zhihu.com/p/487754664\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDifference between on-policy and off-policy\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221226195443427\" loading=\"lazy\" src=\"/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195443427.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor on-policy algorithms, they update the policy network based on the  transitions generated by the current policy network. The \u003cstrong\u003ecritic network\u003c/strong\u003e would make a more accurate value-prediction for the current policy  network in common environments.\u003c/li\u003e\n\u003cli\u003eFor off-policy algorithms, they allow to update the current policy  network using the transitions from old policies. Thus, the old  transitions could be \u003cstrong\u003ereutilized\u003c/strong\u003e, as shown in Fig. 1 the points are  scattered on trajectories that are generated by different policies,  which \u003cstrong\u003eimproves the sample efficiency and reduces the total training steps\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"question-is-there-a-way-to-improve-the-sample-efficiency-of-on-policy-algorithms-without-losing-their-benefit\"\u003eQuestion: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit.\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePPO solves the problem of sample efficiency by utilizing surrogate  objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both \u003cstrong\u003e1. regularizes\u003c/strong\u003e the policy update and enables the \u003cstrong\u003e2. reuse\u003c/strong\u003e of training data.\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20221226195751351\" loading=\"lazy\" src=\"/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195751351.png\"\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"image-assets/image-20221226200007957.png\" alt=\"image-20221226200007957\" style=\"width:50%;\" /\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20221226195936296\" loading=\"lazy\" src=\"/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195936296.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"algorithm\"\u003eAlgorithm\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221226200313414\" loading=\"lazy\" src=\"/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226200313414.png\"\u003e\u003c/p\u003e","title":"Proximal Policy Optimisation Explained Blog"},{"content":"[TOC]\nTitle: Reinforcement Learning With a Corrupted Reward Channel Author: Tom Everitt Publish Year: August 22, 2017 Review Date: Mon, Dec 26, 2022 Summary of paper Motivation we formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards Contribution two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed second, by using randomisation to blunt the agent\u0026rsquo;s optimisation, reward corruption can be partially managed under some assumption Limitation\nfirst solution asks for richer data, make it less data efficient second solution using randomness to blunt agent\u0026rsquo;s optimisation -\u0026gt; random exploration ? Some key terms Inverse Reinforcement learning\nIn the related framework of inverse RL (IRL) [Ng and Russell, 2000], the agent first infers a reward function from observing a human supervisor act, and then tries to optimise the cumulative reward from the inferred reward function. true reward and (possibly corrupt) observed reward\nBoard racing game example\nIn the boat racing game, the true reward may be a function of the agent\u0026rsquo;s final position in the race or the time it takes to complete the race, depending on the designers\u0026rsquo; intentions. The reward corruption function $C$ increases the observed reward on the loop the agent found. worst case regret\nthe difference in the expected cumulative true reward between $\\pi$ and an optimal (in hindsight) policy that knows $\\mu$ No Free Lunch Theorem\nthe worst case regret of any policy $\\pi$ is at least half of the regret of a worst policy $\\hat \\pi$, the maximum regret in an environment is produced by the worst policy $\\hat \\pi$ 这个最好的error也好不过最差的error的1/2 all agent will be negatively affected by the reward corruption, without additional information, the robot has no way of knowing what to do. The result is not surprising, since if all corruption function are allowed in the class $\\mathbf C$, then there is effectively no connection between he observed $\\hat R$ and true reward $\\dot R$. The result therefore encourages us to make precise in which way the observed reward is related to the true reward, and to investigate how agents might handle possible differences between true and observed rewards. this shows that general classes of CRMDPs are not learnable. We therefore suggest some natural simplifying assumptions. limited reward corruption assumption\nEasy environment assumption\nMajor comments Solution\nagents drawing from multiple sources of evidence are likely to be the safest, as they will mostly easily satisfy the conditions of Theorem 19 and 20. For example, humans simultaneously learn their values from pleasure / pain stimuli, watching other people act, listening to stories, as well as (parental) evaluation of different scenarios. Combining sources of evidence may also go some way towards managing reward corruptions beyong sensory corruption. randomness increases robustness: not all contexts allow the agent to get sufficiently rich data to overcome the reward corruption problem. the problem was that they got stuck on a particular value $\\hat r^$ of the observed reward. If unlucky, $\\hat r^$ was available in a corrupt state, in which case the CR agent may get no true reward. In other words, there were adversarial inputs where the CR agent performed poorly. a common way to protect against adversarial inputs is to use a randomised algorithm. Applied to RL and CRMDPs, this idea leads to quantilising agents \u0026ndash; these agents instead randomly choose a state from a top quantile of high-reward states. takeaways\nwithout simplifying assumptions, no agent can avoid the corrupted reward problem. Using the reward signal as evidence rather than optimisation target is no magic bullet, even under strong simplifying assumptions. Essentially, this is because the agent does not know the exact relation between the observed reward and the true reward. However, when the data enables sufficient crosschecking of rewards, agents can avoid the corrupt reward problem. Combining frameworks and providing the agent with different sources of data may often be the safest option. In other words, we need to have different reward signal sources so as to alleviate the corruption. in cases where sufficient crosschecking of rewards is not possible, quantilisation may improve robustness. Essentially, quantilisation prevents agents from overoptimising their objectives. Potential future work we can use the reward-state diagram we can take insights from the takeaways to suggest what may be the solution to alleviate language reward shaping issue ","permalink":"https://sino-huang.github.io/posts/tom_everitt-reinforcement-learning-with-a-corrupted-reward-channel-2017/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reinforcement Learning With a Corrupted Reward Channel\u003c/li\u003e\n\u003cli\u003eAuthor: Tom Everitt\u003c/li\u003e\n\u003cli\u003ePublish Year: August 22, 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Dec 26, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP\u003c/li\u003e\n\u003cli\u003eTraditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003etwo ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be \u003cstrong\u003ecompletely managed\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003esecond, by using randomisation to blunt the agent\u0026rsquo;s optimisation, reward corruption can be partially managed under some assumption\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation\u003c/strong\u003e\u003c/p\u003e","title":"Tom_everitt Reinforcement Learning With a Corrupted Reward Channel 2017"},{"content":"[TOC]\nTitle: Manipulating Reinforcement Learning Stealthy Attacks on Cost Signals Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals Author: Yunhan Huang et. al. Publish Year: 2020 Review Date: Sun, Dec 25, 2022 Summary of paper Motivation understand the impact of the falsification of cost signals on the convergence of Q-learning algorithm\nContribution In Q-learning, we show that Q-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals. and there is a robust region within which the adversarial attacks cannot achieve its objective. The robust region of the cost can be utilised by both offensive and defensive side. An RL agent can leverage the robust region to evaluate the robustness to malicious falsification. we provide conditions on the falsified cost which can mislead the agent to learn an adversary\u0026rsquo;s favoured policy. Some key terms Stealthy Attacks\nStealthy attack is a consistent reward change, which applies to our case. update of Q function under stealthy attacks\nthere are two important questions regarding the Q-learning algorithm with falsified cost (1.12): (1) Will the sequence of Qt-factors converge? (2) where will the sequence of Qt converge to. Lipschitz Continuous\nLipschitz continuous： 函数被一次函数上下夹逼 Robust region theorem\nto make the agent learn the policy $u^\\dagger$, the adversary has to manipulate the cost such that $\\tilde g$ lies outside the ball $\\mathcal B(g; (1-\\alpha)D_{Q^*}(\\mu^\\dagger))$ Good things about the paper (one paragraph) No butterfly effect theorem (1.2):\nthere exists a constant L \u0026lt; 1 such that one can conclude that falsification on cost g by a tiny perturbation does not cause significant changes in the limit point of algorithm. This is a feature known as stability, which is Major comments Citation\nthe authors investigate RL problems where agents receive false rewards from environment. Their results show that reward corruption can impede the performance of agents, and can result in disastrous consequences for highly intelligent agents. ref: Everitt, Tom, et al. \u0026ldquo;Reinforcement learning with a corrupted reward channel.\u0026rdquo; arXiv preprint arXiv:1705.08417 (2017). Potential future work we can transfer the formula into PPO ","permalink":"https://sino-huang.github.io/posts/yunhan_huang-manipulating-reinforcement-learning-stealthy-attacks-on-cost-signals-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Manipulating Reinforcement Learning Stealthy Attacks on Cost Signals\u003c/li\u003e\n\u003cli\u003eDeceptive Reinforcement Learning Under Adversarial\nManipulations on Cost Signals\u003c/li\u003e\n\u003cli\u003eAuthor: Yunhan Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Dec 25, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eunderstand the impact of the falsification of cost signals on the convergence of Q-learning algorithm\u003c/p\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn Q-learning, we show that Q-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals.\u003c/li\u003e\n\u003cli\u003eand there is a robust region within which the adversarial attacks cannot achieve its objective. The robust region of the cost can be utilised by both offensive and defensive side.\u003c/li\u003e\n\u003cli\u003eAn RL agent can leverage the robust region to evaluate the robustness to malicious falsification.\u003c/li\u003e\n\u003cli\u003ewe provide conditions on the falsified cost which can mislead the agent to learn an adversary\u0026rsquo;s favoured policy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eStealthy Attacks\u003c/strong\u003e\u003c/p\u003e","title":"Yunhan_huang Manipulating Reinforcement Learning Stealthy Attacks on Cost Signals 2020"},{"content":"[TOC]\nTitle: No-Regret Reinforcement Learning With Heavy Tailed Rewards Author: Vincent Zhuang et. al. Publish Year: 2021 Review Date: Sun, Dec 25, 2022 Summary of paper Motivation To the best of our knowledge, no prior work has considered our setting of heavy-tailed rewards in the MDP setting. Contribution We demonstrate that robust mean estimation techniques can be broadly applied to reinforcement learning algorithms (specifically confidence-based methods) in order to provably han- dle the heavy-tailed reward setting Some key terms Robust UCB algorithm\nleverage robust mean estimator such as truncated mean and median of means that have tight concentration properties. the median of means estimator is a commonly used strategy for performing robust mean estimation in heavy tailed bandit algorithms. Truncated empirical mean\nMedian-of-means\nAdaptive reward clipping\nthe reward truncation in Heavy-DQN can be viewed as an adaptive version of this kind of fixed reward clipping. the main purpose of reward clipping is to stablize the training dynamics of the neural networks, whereas this method is designed to ensure theoretically-tight reward estimation in the heavy-tailed setting for each state-action pair. Good things about the paper (one paragraph) we use this paper to get some background knowledge about handling perturbed rewards. but this paper is not very relevant to our study Minor comments good phrases for writing essay\n\u0026ldquo;In an orthogonal line of work, XXX did that\u0026rdquo; ","permalink":"https://sino-huang.github.io/posts/vincent_zhuang-no-regret-reinforcement-learning-with-heavy-tailed-rewards-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: No-Regret Reinforcement Learning With Heavy Tailed Rewards\u003c/li\u003e\n\u003cli\u003eAuthor: Vincent Zhuang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Dec 25, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTo the best of our knowledge, no prior work has considered our setting of heavy-tailed rewards in the MDP setting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe demonstrate that \u003cstrong\u003erobust mean estimation techniques\u003c/strong\u003e can be broadly applied to reinforcement learning algorithms (specifically\nconfidence-based methods) in order to provably han-\ndle the heavy-tailed reward setting\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRobust UCB algorithm\u003c/strong\u003e\u003c/p\u003e","title":"Vincent_zhuang No Regret Reinforcement Learning With Heavy Tailed Rewards 2021"},{"content":"[TOC]\nTitle: Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning Author: Wenshuai Zhao et. al. Publish Year: 2020 Review Date: Sun, Dec 25, 2022 Summary of paper Motivation we introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning we discuss on how both the different types of perturbations and how the number of agents experiencing those perturbations affect the collaborative learning effort Contribution This is, to the best of our knowledge, the first work exploring the limitation of PPO in multi-robot systems when considering that different robots might be exposed to different environment where their sensors or actuators have induced errors\nwith the conclusion of this work, we set the initial point for future work on designing and developing methods to achieve robust reinforcement learning on the presence of real-world perturbation that might differ within a multi-robot system.\nSome key terms Deep RL\nDRL algorithms work on a trial and error basis, where an agent interacts with its environment and receives a reward based on the performance. there are DRL approaches that rely on multiple agents to parallelise the learning process or explore a wider variety of experiences. PPO\nconclusion\nfor a fixed small magnitude in the perturbation, the agents still converge on a policy that works for both subsets (the original and the perturbed) Among disturbances in the model\u0026rsquo;s input (sensing, reward) and output (actuation, action) -\u0026gt; the disturbances in the ability of the robots to actuate properly have had a comparatively worse effect than those in their ability to sense to the position of object accurately limitation\nthough the empirical analysis showed that RL algorithm can still converge under the reward disturbance, the experiment is not conducted in the sparse reward environment. Major comments citation\nPPO has been identifies ad one of the most robust approaches against reward perturbation. ref: Zhao, Wenshuai, et al. \u0026ldquo;Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning.\u0026rdquo; 2020 5th International Conference on Robotics and Automation Engineering (ICRAE). IEEE, 2020. ref: Wang, Jingkang, Yang Liu, and Bo Li. \u0026ldquo;Reinforcement learning with perturbed rewards.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020. ","permalink":"https://sino-huang.github.io/posts/wenshuai_zhao-towards-closing-the-sim-to-real-gap-in-collaborative-multi-robot-deep-reinforcement-learning-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Wenshuai Zhao et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Dec 25, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning\u003c/li\u003e\n\u003cli\u003ewe discuss on how both the different types of perturbations and how the number of agents experiencing those perturbations affect the collaborative learning effort\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThis is, to the best of our knowledge, the first work exploring the limitation of PPO in multi-robot systems when considering that different robots might be exposed to different environment where their sensors or actuators have induced errors\u003c/p\u003e","title":"Wenshuai_zhao Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning 2020"},{"content":"[TOC]\nTitle: Reinforcement Learning With Stochastic Reward Machines Author: Jan Corazza et. al. Publish Year: AAAI 2022 Review Date: Sat, Dec 24, 2022 Summary of paper Motivation reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequence of actions. However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise. to overcome this practical limitation, we introduce a novel type of reward machines called stochastic reward machines, and an algorithm for learning them. Contribution Discussing the handling of noisy reward for non-markovian reward function. limitation: the solution introduces multiple sub value function models, which is different from the standard RL algorithm. The work does not emphasise on the sample efficiency of the algorithm. Some key terms Reward machine\nintuitively, one can view the role of reward machines as maintaining the sufficient amount of memory to turn the non-markovian reward function back to a ordinary, Markovian one. This results in an important feature of RMs: they enable using standard RL algorithms (which would otherwise not be usable with non-Markvoian reward functions) Stochastic reward machines\nGood things about the paper (one paragraph) Major comments citation\nthe most natural conceptualisation of the state space is the one in which the reward function depends on the history of actions that the agent has performed. (those are typically the tasks in which the agent is reward for complex behaviours over a longer period) ref: Corazza, Jan, Ivan Gavran, and Daniel Neider. \u0026ldquo;Reinforcement Learning with Stochastic Reward Machines.\u0026rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 6. 2022. an emerging tool used for reinforcement learning in environments with such non-markovian rewards are reward machines. A reward machine is an automation-like structure which augments the state space of the environment, capturing the temporal component of rewards. it has been demonstrated that Q learning can be adapted to use the benefit from reward machine ref: Corazza, Jan, Ivan Gavran, and Daniel Neider. \u0026ldquo;Reinforcement Learning with Stochastic Reward Machines.\u0026rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 6. 2022. ref: Toro Icarte, R.; Klassen, T. Q.; Valenzano, R. A.; and McIl- raith, S. A. 2018. Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning. In ICML, volume 80 of Proceedings of Machine Learning Research, 2112–2121. PMLR when the machine is not known upfront, existing learning methods prove counterproductive in the presence of noisy rewards, as there is either no reward machine consistent with the agent\u0026rsquo;s experience, or the learned reward machine explodes in size, overfitting the noise. ref: Corazza, Jan, Ivan Gavran, and Daniel Neider. \u0026ldquo;Reinforcement Learning with Stochastic Reward Machines.\u0026rdquo; Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 6. 2022. ","permalink":"https://sino-huang.github.io/posts/jan_corazza-reinforcement-learning-with-stochastic-reward-machines-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reinforcement Learning With Stochastic Reward Machines\u003c/li\u003e\n\u003cli\u003eAuthor: Jan Corazza et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: AAAI 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Dec 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ereward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequence of actions. However, existing algorithms for learning reward machines assume an overly idealized   setting where rewards have to be free of noise.\u003c/li\u003e\n\u003cli\u003eto overcome this practical limitation, we introduce a novel type of reward machines called stochastic reward machines, and an algorithm for learning them.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDiscussing the handling of noisy reward for non-markovian reward function.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003elimitation\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003ethe solution introduces multiple sub value function models, which is different from the standard RL algorithm.\u003c/li\u003e\n\u003cli\u003eThe work does not emphasise on the sample efficiency of the algorithm.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eReward machine\u003c/strong\u003e\u003c/p\u003e","title":"Jan_corazza Reinforcement Learning With Stochastic Reward Machines 2022"},{"content":"[TOC]\nTitle: Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering Author: Oguzhan Dogru et. al. Publish Year: July 2022 Review Date: Sat, Dec 24, 2022 Summary of paper Motivation this study consider a type of uncertainty, which is caused by the sensor that are utilised for reward function. When the noise is Gaussian and the system is linear Contribution this work used \u0026ldquo;particle filtering\u0026rdquo; technique to estimate the true reward function from the perturbed discrete reward sampling points. Some key terms Good things about the paper (one paragraph) Major comments Citation\ncomplex control problems today are often solved by black-box methods that are data driven. this is because the SOTA techniques have made it possible to process high-dimensional data in real time. Despite the practicality of these techniques, this data-driven era has its challenges: data reliability. ref: Dogru, Oguzhan, Ranjith Chiplunkar, and Biao Huang. \u0026ldquo;Reinforcement learning with constrained uncertain reward function through particle filtering.\u0026rdquo; IEEE Transactions on Industrial Electronics 69.7 (2021): 7491-7499. learning may be considered solving an optimisation problem with an associated reward function. ref: Dogru, Oguzhan, Ranjith Chiplunkar, and Biao Huang. \u0026ldquo;Reinforcement learning with constrained uncertain reward function through particle filtering.\u0026rdquo; IEEE Transactions on Industrial Electronics 69.7 (2021): 7491-7499. Although uncertainty in the reward has been reported to degrade model/controller performance, no empirical analysis has been conducted on the RL algorithm\u0026rsquo;s tolerance for reward perturbations. ref: J. Wang, Y. Liu, and B. Li, “Reinforcement learning with perturbed rewards,” in Proc. AAAI Conf. Artif. Intell., 2020, vol. 34, pp. 6202–6209 limitation of the experiment setting\nThis particle filtering technique is not applicable for the sparse reward signal setting. Moreover, the noise filtering technique requires further rounds of simulation steps to generate estimate of the real reward, which makes the RL training further sample inefficient. ","permalink":"https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering\u003c/li\u003e\n\u003cli\u003eAuthor: Oguzhan Dogru et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: July 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Dec 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis study consider a type of uncertainty, which is caused by the sensor that are utilised for reward function. When the noise is Gaussian and the system is linear\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221224214550390\" loading=\"lazy\" src=\"/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/image-assets/image-20221224214550390.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis work used \u0026ldquo;particle filtering\u0026rdquo; technique to estimate the true reward function from the perturbed discrete reward sampling points.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003ch2 id=\"good-things-about-the-paper-one-paragraph\"\u003eGood things about the paper (one paragraph)\u003c/h2\u003e\n\u003ch2 id=\"major-comments\"\u003eMajor comments\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCitation\u003c/strong\u003e\u003c/p\u003e","title":"Oguzhan_dogru Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering 2022"},{"content":"[TOC]\nTitle: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning Author: Inaam Ilahi et. al. Publish Year: 13 Sep 2021 Review Date: Sat, Dec 24, 2022 Summary of paper Motivation DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications. Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks. Contribution we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures we discuss the available benchmarks and metrics for the robustness of DRL finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions . Some key terms organisation of this article\nValue function\nNon-exhaustive taxonomy of major DRL schemes as proposed\nwhy use policy based RL\nadvantages better convergence properties effective in high-dimensional or continuous action spaces. When the space is large, the usage of memory and computation consumption grows rapidly, the policy based RL avoids this because the objective is to learn a set of parameters that is far less than the space count. disadvantages policy gradient methods tend to more stably converge to a good behaviour. but indeed being on-policy, makes them very sample inefficient. Evaluating a policy is typically inefficient and high variance Policy based RL has high variance, but there are techniques to reduce this variance. attacks on DRL\nreward space adding perturbations flipping the rewards adversarial training\nadversarial training includes retraining of the ML model using the adversarial examples along with the legitimate examples. This increases the robustness of the ML against adversarial examples as the model is now able to learn a better distribution. Major comments Citation\nRL algorithm also have some limitations to be utilised in practice mainly due to their slow learning process and inability to learn in complex environment. ref: Ilahi, Inaam, et al. \u0026ldquo;Challenges and countermeasures for adversarial attacks on deep reinforcement learning.\u0026rdquo; IEEE Transactions on Artificial Intelligence 3.2 (2021): 90-109. recently, DRL has been vulnerable to adversarial attacks, where an imperceptible perturbation is added to the input to the DRL schemes with a predefined goal of causing a malfunction in the working of DRL ref: Ilahi, Inaam, et al. \u0026ldquo;Challenges and countermeasures for adversarial attacks on deep reinforcement learning.\u0026rdquo; IEEE Transactions on Artificial Intelligence 3.2 (2021): 90-109. there is limited number of research focusing on the reward perturbation influence on the RL agent performance ref: Ilahi, Inaam, et al. \u0026ldquo;Challenges and countermeasures for adversarial attacks on deep reinforcement learning.\u0026rdquo; IEEE Transactions on Artificial Intelligence 3.2 (2021): 90-109. With regard to handling reward perturbation, this work utilized a neural network to estimate the actual reward of the environment and therefore detect and filter out abnormal rewards. ref: Kumar, Aashish. Enhancing performance of reinforcement learning models in the presence of noisy rewards. Diss. 2019. Dogru, Oguzhan, Ranjith Chiplunkar, and Biao Huang. \u0026ldquo;Reinforcement learning with constrained uncertain reward function through particle filtering.\u0026rdquo; IEEE Transactions on Industrial Electronics 69.7 (2021): 7491-7499. Minor comments ","permalink":"https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Inaam Ilahi et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 13 Sep 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Dec 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications.\u003c/li\u003e\n\u003cli\u003eTherefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms\u003c/li\u003e\n\u003cli\u003ewe present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures\u003c/li\u003e\n\u003cli\u003ewe discuss the available benchmarks and metrics for the robustness of DRL\u003c/li\u003e\n\u003cli\u003efinally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions .\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eorganisation of this article\u003c/strong\u003e\u003c/p\u003e","title":"Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022"},{"content":"[TOC]\nTitle: On the Robustness of Safe Reinforcement Learning Under Observational Perturbations Author: Zuxin Liu et. al. Publish Year: 3 Oct 2022 Review Date: Thu, Dec 22, 2022 Summary of paper Motivation While many recent safe RL methods with deep policies can achieve outstanding constraint satisfaction in noise-free simulation environment, such a concern regarding their vulnerability under adversarial perturbation has not been studies in the safe RL setting. Contribution we are the first to formally analyze the unique vulnerability of the optimal policy in safe RL under observational corruptions. We define the state-adversarial safe RL problem and investigate its fundamental properties. We show that optimal solutions of safe RL problems are theoretically vulnerable under observational adversarial attacks we show that existing adversarial attack algorithms focusing on minimizing agent rewards do not always work, and propose two effective attack algorithms with theoretical justifications \u0026ndash; one directly maximise the constraint violation cost, and one maximise the task reward to induce a tempting but risky policy. Surprisingly, the maximum reward attack is very strong in inducing unsafe behaviors, both in theory and practice we propose an adversarial training algorithm with the proposed attackers and show contraction properties of their Bellman operators. Extensive experiments in continuous control tasks show that our method is more robust against adversarial perturbations in terms of constraint satisfaction. Some key terms Safe reinforcement learning definition\nSRL tackles the problem by solving constrained optimisation that can maximise the task reward while satisfying certain constraints. this is usually done under the Constrained MDP framework and has shown to be effective in learning a constraint satisfaction policy in many tasks. safe RL has an additional metric that characterises the cost of constraint violations. there are some cases where sacrificing some reward is not comparable with violating the constraint because the latter may cause catastrophic consequences. make the attack stealthy\nkeep the reward as high as possible but aims to generate more constraint violations. in contrast, existing adversaries on standard RL aims to reduce the overall reward or lead to incorrect decision-making reward stealthiness\nis defined as the increased reward value under the adversary. An state-adversary v is stealthy if the reward it can obtain is higher than the original one. reward effectiveness\nthe effectiveness metric measures an adversary\u0026rsquo;s capability of attacking the safe RL agent to violate constraints. i.e., the increased cost value under the adversary adversary\nit modifies the Value estimation of the state. Good things about the paper (one paragraph) Proposition 1 for an optimal policy pi*, the Maximum Reward Attacker is guaranteed to be reward steathy and effective, given enough large perturbation set. Major comments Citation\nit has been shown that neural networks are vulnerable to adversarial attacks \u0026ndash; a small perturbation of the input data may lead to a large variance of the output ref: Gabriel Resende Machado, Eugênio Silva, and Ronaldo Ribeiro Goldschmidt. Adversarial machine learning in image classification: A survey toward the defender’s perspective. ACM Computing Surveys (CSUR), 55(1):1–38, 2021 Nikolaos Pitropakis, Emmanouil Panaousis, Thanassis Giannetsos, Eleftherios Anastasiadis, and George Loukas. A taxonomy and survey of attacks against machine learning. Computer Science Review, 34:100199, 2019. limitation\nthis paper only considered the observation perturbation instead of the reward perturbation. ","permalink":"https://sino-huang.github.io/posts/zuxin_liu-on-the-robustness-of-safe-reinforcement-learning-under-observational-perturbations-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: On the Robustness of Safe Reinforcement Learning Under Observational Perturbations\u003c/li\u003e\n\u003cli\u003eAuthor: Zuxin Liu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 3 Oct 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Dec 22, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWhile many recent safe RL methods with deep policies can achieve outstanding constraint satisfaction in noise-free simulation environment, such a concern regarding their vulnerability under adversarial perturbation has not been studies in the safe RL setting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe are the first to formally analyze the unique vulnerability of the optimal policy in safe RL under observational corruptions. We define the state-adversarial safe RL problem and investigate its fundamental properties. We show that optimal solutions of safe RL problems are theoretically vulnerable under observational adversarial attacks\u003c/li\u003e\n\u003cli\u003ewe show that existing adversarial attack algorithms focusing on minimizing agent rewards do not always work, and propose two effective attack algorithms with theoretical justifications \u0026ndash; one directly maximise the constraint violation cost, and one maximise the task reward to induce a tempting but risky policy.\n\u003cul\u003e\n\u003cli\u003eSurprisingly, the maximum reward attack is very strong in inducing unsafe behaviors, both in theory and practice\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ewe propose an adversarial training algorithm with the proposed attackers and show contraction properties of their Bellman operators. Extensive experiments in continuous control tasks show that our method is more robust against adversarial perturbations in terms of constraint satisfaction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSafe reinforcement learning definition\u003c/strong\u003e\u003c/p\u003e","title":"Zuxin_liu on the Robustness of Safe Reinforcement Learning Under Observational Perturbations 2022"},{"content":"[TOC]\nTitle: Disturbing Reinforcement Learning Agents With Corrupted Rewards Author: Ruben Majadas et. al. Publish Year: Feb 2021 Review Date: Sat, Dec 17, 2022 Summary of paper Motivation recent works have shown how the performance of RL algorithm decreases under the influence of soft changes in the reward function. However, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning learning exploration strategy. it chooses a subclass of MDPs: episodic, stochastic goal-only rewards MDPs Contribution it demonstrated that smoothly crafting adversarial rewards are able to mislead the learner the policy that is learned using low exploration probability values is more robust to corrupt rewards. (though this conclusion seems valid only for the proposed experiment setting) the agent is completely lost with attack probabilities higher that than p=0.4 Some key terms deterministic goal only reward MDP\na reward is only given if a goal state is reached. many decision task are modelled with goal-only rewards from classical control problems such as Mountain Car or the Cart Pole the goal only reward setting is a special case for sparse reward setting. goal of adversary\nproduce the maximum deterioration in the learner policy performing the minimum number of attacks. Major comments citation\nthe attack on reward function has received very little attention ref: Majadas, Rubén, Javier García, and Fernando Fernández. \u0026ldquo;Disturbing reinforcement learning agents with corrupted rewards.\u0026rdquo; arXiv preprint arXiv:2102.06587 (2021). ref: Jingkang Wang, Yang Liu, and Bo Li, ‘Reinforcement learning with perturbed rewards’, CoRR, abs/1810.01032, (2018) there are no studies about how sensitive the learning process is depending on the aggressiveness of reward perturbations and the exploration strategy. ref: Majadas, Rubén, Javier García, and Fernando Fernández. \u0026ldquo;Disturbing reinforcement learning agents with corrupted rewards.\u0026rdquo; arXiv preprint arXiv:2102.06587 (2021). limitation on the setting\nthis consider with small and low-dimensional state space lack the consideration of sparse reward setting with high dimensional large state space this perturbation is based on the probability and lack the focus on the false positive rewards. the perturbation is limited to only changing the sign of the true rewards. A fixed attack probability was used in the experiments to test how reward attacks affect agents with varying exploration rates. this experiment is like punish the good movements that lead to the goal. ","permalink":"https://sino-huang.github.io/posts/ruben_majadas-disturbing-reinforcement-learning-agents-with-corrupted-rewards-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Disturbing Reinforcement Learning Agents With Corrupted Rewards\u003c/li\u003e\n\u003cli\u003eAuthor: Ruben Majadas et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Feb 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Dec 17, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003erecent works have shown how the performance of RL algorithm decreases under the influence of soft changes in the reward function.\u003c/li\u003e\n\u003cli\u003eHowever, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning learning exploration strategy.\u003c/li\u003e\n\u003cli\u003eit chooses a subclass of MDPs: episodic, stochastic goal-only rewards MDPs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eit demonstrated that smoothly crafting adversarial rewards are able to mislead the learner\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ethe policy that is learned using low exploration probability values is more robust to corrupt rewards.\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e(though this conclusion seems valid only for the proposed experiment setting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ethe agent is completely lost with attack probabilities higher that than p=0.4\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003edeterministic goal only reward MDP\u003c/strong\u003e\u003c/p\u003e","title":"Ruben_majadas Disturbing Reinforcement Learning Agents With Corrupted Rewards 2021"},{"content":"[TOC]\nTitle: Reinforcement Learning With Perturbed Rewards Author: Jingkang Wang et. al. Publish Year: 1 Feb 2020 Review Date: Fri, Dec 16, 2022 Summary of paper Motivation this paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned. Some experiments are used to support the algorithm (i.e., estimate the confusion matrix and revert) using existing techniques from the supervised learning (and crowdsourcing) literature. Limitation reviewers had concerns over the scope / significance of this work, mostly about how the confusion matrix is learned. If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards. Specifically, the work seems to be limited in two substantial ways, both related to how confusion matrix is learned the reward function needs to be deterministic majority voting requires the number of states to be finite the significance of this work is therefore limited to finite-state problems with deterministic rewards, which is quite restricted. overall, the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work. However, the exact problem setting is not completely clear in the paper, and the limitation of the technical contribution is somewhat unclear. Contribution The SOTA PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively Some key terms reward function is often perturbed\nit is difficult to design a reward function that produce credible rewards in the presence of noise. this is because the output from any reward function is subject to multiple kinds of randomness Inherent Noise, e.g., sensors on robot may report noisy observed rewards application specific noise, when an Rl agent receives feedback/instructions, different human instructor might provide drastically different feedback that leads to biased rewards (ref: Learning something from nothing: Leveraging implicit human feedback strategies) adversarial noise: reward poisoning attack is able to mislead pretrained RL policy arbitrarily. arbitrary reward noise vs perturbed rewards\narbitrary noise is just random, which is extremely challenging, perturbed rewards, where the observed rewards are learnable. the perturbed rewards are generated via a confusion matrix that flips the true reward to another one according to a certain distribution. Unbiased estimator for true reward\nPotential future work we can use the formulation in the paper\n","permalink":"https://sino-huang.github.io/posts/jingkang_wang-reinforcement-learning-with-perturbed-rewards-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reinforcement Learning With Perturbed Rewards\u003c/li\u003e\n\u003cli\u003eAuthor: Jingkang Wang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 1 Feb 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Dec 16, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned. Some experiments are used to support the algorithm (i.e., estimate the confusion matrix and revert) using existing techniques from the supervised learning (and crowdsourcing) literature.\u003c/li\u003e\n\u003cli\u003eLimitation\n\u003cul\u003e\n\u003cli\u003ereviewers had concerns over the scope / significance of this work, mostly about how the confusion matrix is learned. If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.\u003c/li\u003e\n\u003cli\u003eSpecifically, the work seems to be limited in two substantial ways, both related to how confusion matrix is learned\n\u003cul\u003e\n\u003cli\u003ethe reward function needs to be deterministic\u003c/li\u003e\n\u003cli\u003emajority voting requires the number of states to be finite\n\u003cul\u003e\n\u003cli\u003ethe significance of this work is therefore limited to finite-state problems with deterministic rewards, which is quite restricted.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eoverall, the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work. However, the exact problem setting is not completely clear in the paper, and the limitation of the technical contribution is somewhat unclear.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe SOTA PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ereward function is often perturbed\u003c/strong\u003e\u003c/p\u003e","title":"Jingkang_wang Reinforcement Learning With Perturbed Rewards 2020"},{"content":"[TOC]\nTitle: Language Models as Agent Models Author: Jacob Andreas Publish Year: 3 Dec 2022 Review Date: Sat, Dec 10, 2022 https://arxiv.org/pdf/2212.01681.pdf\nSummary of paper Motivation during training, LMs have access only to the text of the documents, with no direct evidence of the internal states of the human agent that produce them. (kind of hidden MDP thing) this is a fact often used to argue that LMs are incapable of modelling goal-directed aspects of human language production and comprehension. The author stated that even in today\u0026rsquo;s non-robust and error-prone models \u0026ndash; LM infer and use representations of fine-grained communicative intensions and more abstract beliefs and goals. Despite that limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally. In other words, the author said that language model can be used to communicate intention of human agent, and hence it can be treated as a agent model. Contribution the author claimed that in the course of performing next-word prediction in context, current LMs sometimes infer inappropriate, partial representations of beliefs ,desires and intentions possessed by the agent that produced the context, and other agents mentioned within it. Once these representations are inferred, they are causally linked to LM prediction, and thus bear the same relation to generated text that an intentional agent\u0026rsquo;s state bears to its communicative actions. The high-level goals of this paper are twofold: first, to outline a specific sense in which idealised language models can function as models of agent belief, desires and intentions; second, to highlight a few cases in which existing models appear to approach this idealization (and describe the ways in which they still fall short) Training on text alone produces ready-made models of the map from agent states to text; these models offer a starting point for language processing systems that communicate intentionally. Some key terms Current language model is bad\noutputs from current LMs sometimes describe impossible situations, contradictory propositions, or invalid inferences. what these errors have in common is a failure to model communicative intent: they may be grammatically or even semantically acceptable, but not the sort of text that could be produced by an author with a coherent set of beliefs or goals. definition of language model\nan LM is simply a conditional distribution $p(x_i | x_1 \u0026hellip;x_{i-1})$ over next tokens xi given context x1\u0026hellip;xi-1. LMs thus learn language in a very different way from humans \u0026ndash; they lack access to the social and perceptual context that human language learners \u0026ndash; they lack access to the social and perceptual context that human language learners use to infer the relationship between utterances and speaker\u0026rsquo;s mental states also human learner is trained to act in different environments and accomplish goals beyond next-word prediction. the goodness in language model\nthe belief desire intention model\ncontext and LM and constraints\nin a collection of individually coherent documents, a context constrains the beliefs, desires, and intentions of a hypothetical author. An effective LM must learn to maintain these constraints. Event today\u0026rsquo;s largest language models make major errors involving factuality and coherence\nlimitation of training datasets solution: a small annotation about author\u0026rsquo;s beliefs and goals \u0026ndash; or at the very least, richer information about the social and perceptual context in which language is generated \u0026ndash; might improve language modelling limitation of context windows a few thousand tokens cannot infer an agent state, understood as a complete set of beliefs, desires and intentions, as this is not a small object. solution: develop new LMs that do not condition on fixed-size context windows or state vectors, but instead explicitly factorise short-term and long-term context components relevant for prediction. Limitation of LM architecture Good things about the paper (one paragraph) very good overview and discussion about the language model and how it can be treated as agent model ","permalink":"https://sino-huang.github.io/posts/jacob_andreas-language-models-as-agent-models-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language Models as Agent Models\u003c/li\u003e\n\u003cli\u003eAuthor: Jacob Andreas\u003c/li\u003e\n\u003cli\u003ePublish Year: 3 Dec 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Dec 10, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2212.01681.pdf\"\u003ehttps://arxiv.org/pdf/2212.01681.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eduring training, LMs have access only to the text of the documents, with no direct evidence of the internal states of the human agent that produce them. (kind of hidden MDP thing)\u003c/li\u003e\n\u003cli\u003ethis is a fact often used to argue that LMs are incapable of modelling goal-directed aspects of human language production and comprehension.\u003c/li\u003e\n\u003cli\u003eThe author stated that even in today\u0026rsquo;s non-robust and error-prone models \u0026ndash; LM infer and use representations of fine-grained communicative intensions and more abstract beliefs and goals. Despite that limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIn other words\u003c/strong\u003e, the author said that language model can be used to communicate intention of human agent, and hence it can be treated as a agent model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe author claimed that\n\u003cul\u003e\n\u003cli\u003ein the course of performing next-word prediction in context, current LMs sometimes infer inappropriate, partial representations of beliefs ,desires and intentions possessed by the agent that produced the context, and other agents mentioned within it.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOnce\u003c/strong\u003e these representations are \u003cstrong\u003einferred\u003c/strong\u003e, they are \u003cstrong\u003ecausally\u003c/strong\u003e \u003cstrong\u003elinked\u003c/strong\u003e to LM prediction, and thus bear the same relation to generated text that an intentional agent\u0026rsquo;s state bears to its communicative actions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe high-level goals of this paper are twofold:\n\u003cul\u003e\n\u003cli\u003efirst, to outline a specific sense in which idealised language models can function as models of agent belief, desires and intentions;\u003c/li\u003e\n\u003cli\u003esecond, to highlight a few cases in which existing models appear to approach this idealization (and describe the ways in which they still fall short)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTraining on text alone produces ready-made models of the map from agent states to text; these models offer a starting point for language processing systems that communicate intentionally.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCurrent language model is bad\u003c/strong\u003e\u003c/p\u003e","title":"Jacob_andreas Language Models as Agent Models 2022"},{"content":"[TOC]\nTitle: Context Aware Language Modeling for Goal Oriented Dialogue Systems Author: Charlie Snell et. al. Publish Year: 22 Apr 2022 Review Date: Sun, Nov 20, 2022 Summary of paper Motivation while supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. how can we scalably and effectively introduce the mechanisms of goal-directed decision making into end-to-end language models, to steer language generation toward completing specific dialogue tasks rather than simply generating probable responses. they aim to directly finetune language models in a task-aware manner such that they can maximise a give utility function. Contribution it seems like the manipulation of training dataset and also the auxiliary objective are the two main \u0026ldquo;innovations\u0026rdquo; of the model. Some key terms Dialogue\ndialogue can also be viewed as a sequential decision making process, which is well-suited to planning and reinforcement learning (RL) algorithm Training such a system with active human-in-the-loop interaction quickly becomes expensive and cumbersome, making it desirable to develop techniques for goal-directed training of dialogue system that can effectively leverage offline data Most systems take a pipelined approach, where an abstract representation of state and actions is designed by hand and then combined with RL, rather than generating dialogue end-to-end. These pipelined approaches rely on a manually designed decomposition of the dialogue task, which may be domain specific and more importantly, may not enjoy all the benefits of tightly integrating low level text generation with the overall goals of the task. Formulation\nlarge language models can already be formulated within a MDP as capturing both the dynamics and policy for a decision the dynamics of a MDP define how states, obervations and reward are generated at each time step. dialogue history serves as state, and the agent\u0026rsquo;s utterances serve as actions. they propose a conditional imitation learning strategy coupled with a novel task relabelling approach that can finetune language model from offline data. (imitation?) in the end, the model still represents the joint distribution over dialogues, but tilts this distribution towards dialogues with high rewards Dialogue Task relabelling\nContext aware fine-tuning\nWhile these high-level actions are theoretically learnable from correlations between the dialogue and the given context, in general, we find that learning these correlations corresponds to a relatively small decrease in dialogue entropy under the model. As a result, the model is less incentivized to learn these correlations relevant tot he task than the form of the dialogue this is the trade off between fluency and the goal-oriented ISSUE: the primary high-level action involved in Air Dialogue is the decision of which flight table entry, if any, to recommend to the user. They therefore implement the auxiliary objective as a classification head on top of the language model, trained to predict the flight table entry that meets the customer\u0026rsquo;s request. BUT this is specific to the AirFlight dataset. ","permalink":"https://sino-huang.github.io/posts/charlie_snell-context-aware-language-modeling-for-goal-oriented-dialogue-systems-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Context Aware Language Modeling for Goal Oriented Dialogue Systems\u003c/li\u003e\n\u003cli\u003eAuthor: Charlie Snell et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Apr 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Nov 20, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewhile supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question.\u003c/li\u003e\n\u003cli\u003ehow can we scalably and effectively introduce the mechanisms of goal-directed decision making into end-to-end language models, to steer language generation toward completing specific dialogue tasks rather than simply generating probable responses.\u003c/li\u003e\n\u003cli\u003ethey aim to directly finetune language models in a task-aware manner such that they can maximise a give \u003cem\u003eutility function\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cimg src=\"image-assets/image-20221120180500086.png\" alt=\"image-20221120180500086\" style=\"width:50%;\" /\u003e\n\u003cul\u003e\n\u003cli\u003eit seems like the manipulation of training dataset and also the auxiliary objective are the two main \u0026ldquo;innovations\u0026rdquo; of the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDialogue\u003c/strong\u003e\u003c/p\u003e","title":"Charlie_snell Context Aware Language Modeling for Goal Oriented Dialogue Systems 2022"},{"content":"[TOC]\nTitle: Building Goal Oriented Dialogue Systems With Situated Visual Context 2021 Author: Sanchit Agarwal et. al. Publish Year: 22 Nov 2021 Review Date: Sun, Nov 20, 2022 Summary of paper Motivation with the surge of virtual assistants with screen, the next generation of agents are required to also understand screen context in order to provide a proper interactive experience, and better understand users\u0026rsquo; goals. So in this paper, they propose a novel multimodal conversational framework, where the agent\u0026rsquo;s next action and their arguments are derived jointly conditioned on the conversational and the visual context. The model can recognise visual features such as color and shape as well as the metadata based features such as price or star rating associated with a visual entity. Contribution propose a novel multimodal conversational system that considers screen context, in addition to dialogue context, while deciding the agent\u0026rsquo;s next action The proposed visual grounding model takes both metadata and images as input allowing it to reason over metadata and visual information Our solution encodes the user query and each visual entities and then compute the similarity between them. to improve the visual entity encoding, they introduced query guided attention and entity self-attention layers. collect the MTurk survey and also create a multimodal dialogue simulator Architecture ","permalink":"https://sino-huang.github.io/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Building Goal Oriented Dialogue Systems With Situated Visual Context 2021\u003c/li\u003e\n\u003cli\u003eAuthor: Sanchit Agarwal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Nov 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Nov 20, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewith the surge of virtual assistants with screen, the next generation of agents are required to also understand screen context in order to provide a proper interactive experience, and better understand users\u0026rsquo; goals.\u003c/li\u003e\n\u003cli\u003eSo in this paper, they propose a novel multimodal conversational framework, where the agent\u0026rsquo;s next action and their arguments are derived jointly conditioned on the conversational and the visual context.\u003c/li\u003e\n\u003cli\u003eThe model can recognise visual features such as color and shape as well as the metadata based features such as price or star rating associated with a visual entity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003epropose a novel multimodal conversational system that considers screen context, in addition to dialogue context, while deciding the agent\u0026rsquo;s next action\u003c/li\u003e\n\u003cli\u003eThe proposed visual grounding model takes both metadata and images as input allowing it to reason over metadata and visual information\u003c/li\u003e\n\u003cli\u003eOur solution encodes the user query and each visual entities and then compute the similarity between them. to improve the visual entity encoding, they introduced query guided attention and entity self-attention layers.\u003c/li\u003e\n\u003cli\u003ecollect the MTurk survey and also create a multimodal dialogue simulator\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"architecture\"\u003eArchitecture\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221120232504688\" loading=\"lazy\" src=\"/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/image-assets/image-20221120232504688.png\"\u003e\u003c/p\u003e","title":"Sanchit_agarwal Building Goal Oriented Dialogue Systems With Situated Visual Context 2021"},{"content":"[TOC]\nTitle: DANLI: Deliberative Agent for Following Natural Language Instructions Author: Yichi Zhang Publish Year: 22 Oct, 2022 Review Date: Sun, Nov 20, 2022 Summary of paper Motivation reactive agent simply learn and imitate behaviours encountered in the training data these reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from the past experience. Contribution We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark Some key terms Natural language instruction following with embodied AI agents\nparent class of language reward shaping AI agent. where an agent must interpret human language commands to perform actions in the physical world and achieve a goal. especially challenging is the hierarchical nature of everyday tasks, which often require reasoning about subgoals and reconciling them with the world state and overall goal. however, despite recent progress, past approaches are typical reactive in their execution of actions: conditioned on the rich, multimodal inputs from the environment, they perform actions directly without using an explicit representation of the world to facilitate grounded reasoning and planning. Such an approach is inefficient, as natural language instructions often omit trivial steps that a human may be assumed to already know Besides, the lack of any explicit symbolic component makes such approaches hard to interpret, especially when the agent make errors DANLi architecture\nbuild a uniquely rich semantic spatial representation, acquired online from the surrounding environment and language descriptions to capture symbolic information about object instances and their physical states. to capture the highest level of hierarchy in tasks, we propose a neural task monitor that learns to extract symbolic information about task progress and upcoming subgoals from the dialog and action history. Using these elements as a planning algorithm to plan low-level actions for subgoals in the environment, taking advantage of DANLI\u0026rsquo;s transparent reasoning and planning pipeline to detect and recover from errors. Architecture diagram\n\\\n","permalink":"https://sino-huang.github.io/posts/yichi_zhang-danli-deliberative-agent-for-following-natural-language-instructions-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: DANLI: Deliberative Agent for Following Natural Language Instructions\u003c/li\u003e\n\u003cli\u003eAuthor: Yichi Zhang\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Oct, 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Nov 20, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ereactive agent simply learn and imitate behaviours encountered in the training data\u003c/li\u003e\n\u003cli\u003ethese reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from the past experience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWe show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eNatural language instruction following with embodied AI agents\u003c/strong\u003e\u003c/p\u003e","title":"Yichi_zhang Danli Deliberative Agent for Following Natural Language Instructions 2022"},{"content":"[TOC]\nTitle: Diffusion-LM Improves Controllable Text Generation Author: Xiang Lisa Li Publish Year: May 2022 Review Date: Mon, Nov 14, 2022 https://arxiv.org/pdf/2205.14217.pdf\nSummary of paper Motivation can language tokens be represented as floating number? they develop a new non-autoregressive language model based on continuous diffusion Diffusion LM iteratively denoises as sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variable. how to convert from continuous embeddings back to words they used rounding and many other tricks to stabilise the training process Contribution they tried diffusion model for Language Model Incomprehension Not sure if the model is good at text generation.\n","permalink":"https://sino-huang.github.io/posts/xiang_li-diffusion_lm_improves_controllable_text_generation_2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Diffusion-LM Improves Controllable Text Generation\u003c/li\u003e\n\u003cli\u003eAuthor: Xiang Lisa Li\u003c/li\u003e\n\u003cli\u003ePublish Year: May 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Nov 14, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2205.14217.pdf\"\u003ehttps://arxiv.org/pdf/2205.14217.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecan language tokens be represented as floating number?\u003c/li\u003e\n\u003cli\u003ethey develop a new non-autoregressive language model based on continuous diffusion\u003c/li\u003e\n\u003cli\u003eDiffusion LM iteratively denoises as sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variable.\u003c/li\u003e\n\u003cli\u003ehow to convert from continuous embeddings back to words\n\u003cul\u003e\n\u003cli\u003ethey used rounding and many other tricks to stabilise the training process\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethey tried diffusion model for Language Model\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"incomprehension\"\u003eIncomprehension\u003c/h2\u003e\n\u003cp\u003eNot sure if the model is good at text generation.\u003c/p\u003e","title":"Xiang_li Diffusion-LM Improves Controllable Text Generation 2022"},{"content":"You need password to access to the content, go to Slack *#phdsukai to find more.\nPart of this article is encrypted with password:\n--- DON'T MODIFY THIS LINE --- [TOC]\nExisting work Goyal\u0026rsquo;s language reward shaping module + PPO RL agent\nLimitation of existing work The Goyal\u0026rsquo;s model used only sequence of past actions + instruction, and thus can only address the action constraints in the instruction However, an instruction can contain either action, object(state) or temporal constraints, and thus the Goyal\u0026rsquo;s language reward shaping binary classifier cannot handle the others and thus it is inaccurate. (This issue is specific to Montezuma\u0026rsquo;s Revenge environment (MR env) ) In MR env, some actions may become invalid in some cases and hence may not give you the expected outcome (e.g., left or right action become invalid when you are on the ladder). Human players can notice this by looking at the game screen but Goyal\u0026rsquo;s reward shaping module cannot, thus leading to huge error. Eventually Goyal\u0026rsquo;s language reward shaping RL model cannot solve room one in MR env. Summary of our work We proposed an advanced language reward shaping module that can make more accurate matched/mismatched prediction\nSpecifically, we trained a video autoencoder to encode video data into latent vectors that has smaller dimension but still capture the semantic meaning. we used video\u0026rsquo;s relative offset ($img_t - img_{t-1}$) to predict avatar\u0026rsquo;s valid actions, this avoids the invalid action issue in MR env. Goyal provided about 5000 video and description (matched) pairs as training dataset. But in order to train the language reward binary classifier, we need to create our own negatives samples. On top of the in-batch negatives, we used phrase replacement method on both the verb phrases and noun phrases to create hard negatives. (Not our main focus) we used image augmentation and also change of the video speed method on the video data to increase the robustness of the video encoding model In the end we used a Transformer architecture that applies cross attention between the sentence embeddings, sequence of image embeddings (video) and sequence of action embeddings -\u0026gt; cross attention can capture the interrelationship of the sequential data and thus can implicitly capture the temporal information In the RL env section, we experimented different settings of how the instructions are given to the RL agent\nSpecificity: specific vs. vague instructions e.g., \u0026ldquo;Climb down the middle ladder to the conveyor belt and jump right to the yellow rope on the right of the screen\u0026rdquo; vs \u0026ldquo;Move to the platform, then move to the rope\u0026rdquo; Composition of goals and time coverage: single goal vs. composition of multi goals so one can cover 1 sec vs one can cover 3 secs) (3 sec is the length of video in Goyal\u0026rsquo;s training dataset) e.g., \u0026ldquo;Go down the ladder\u0026rdquo; and \u0026ldquo;jump right on the yellow string\u0026rdquo; two separate single-goal instructions, each covers about 1 sec vs. \u0026ldquo;Go down the ladder and jump right on the yellow string\u0026rdquo; one composition of multiple goals and can cover 3 secs Missing of intermediate steps e.g., [ \u0026ldquo;Go down the middle ladder\u0026rdquo;, \u0026ldquo;Jump right to the rope\u0026rdquo;, \u0026ldquo;climb down the right ladder\u0026rdquo;] vs. [ \u0026ldquo;Go down the middle ladder\u0026rdquo;, \u0026ldquo;climb down the right ladder\u0026rdquo;] Some changes to prevent agent from repeatedly get language rewards form the same instruction\nChange immediate language reward formula to $immediate_rew_{t} = lang_rew(obs_t, sentence ) - \\max(lang_rew_history)$ Intrinsic reward when encountering unseen state. We also tested different strictness level for the binary classifier to give reward\nStrictness (confidence threshold): the reward module only gives reward when its prediction probability is above a certain value. E.g., if we set confidence threshold to be 70%, it means that the language reward module only gives reward when the prediction probability of being matched is greater than 0.7 We found that the RL agent is very sensitive to this hyper-parameter. Our contribution We proposed a language reward shaping model that can address the action, object and temporal constraints in the walkthrough instructions. And finally make the language reward shaping method practical for helping to solve complex tasks such as MR\u0026rsquo;s room one. We proposed the phrase replacement method to create hard negatives training data and make the model more effective in handling action and object constraints. And this method seems to fit more naturally than other hard negative generation methods (e.g., top K retrieval) in this language reward shaping circumstance. It is because this method creates the hard negatives of action and object constraints in the instructions. Moreover, this also alleviates the data scarcity issue and also decrease the possibility of generating false negatives compared to Top-K retrieval method. Although there are works on using language reward shaping methods to solve AI problems, non attempted to measure the impact of different settings of walkthrough instructions on the language reward shaping agent. How does the agent performance influenced by 1) the specificity of the instructions, 2) the length and quantity of the instruction sentences. 3) the strictness of the language reward module? Understanding this can better help us to utilise language reward shaping method practically. Result* Those results are not strong enough, more experiments needs to be conducted to confirm the results\nour improvements on the language reward shaping module eventually leads to the accuracy increase of the mismatched/matched binary classification prediction\nwe still need a better validation dataset for this (I can create this by hand) Also I am not satisfy with the current performance of the language reward shaping module (see the result from the report, we can continue to improve the module by Having object detection module Other pre-training and training method to improve accuracy stated in the report In general for the RL section, we found out that\nan inappropriate configuration may cause the RL agent to fall into local minima and cannot solve the problem, these local minima are contributed by the error from the language reward module With the use of language reward shaping, it may postpone the first time the agent reach the goal (i.e., get the key and escape from room one) because to some extent the language reward prevent agent from exploring more. Nevertheless it appears that the language reward shaping method can help the agent to increase the frequency of getting out of the first room. In terms of different settings of walkthrough instruction\nthe specificity of the instructions, false positive and false negative gets high when we make the instruction too specific vague instructions are more preferred when you have limited training dataset and also the vague instruction works a hint and the agent can find the way by itself by exploration. the length and quantity of the instruction sentences. Increasing the quantity of the instruction sentence may make the training unstable due to the error accumulation from the language reward shaping module. but having more specific instruction may help the agent to reach the goal earlier, but after that the agent may fall into the local minima very quickly. Length (composition of multiple goals): It seems better if we can let the instruction to have suitable amount of goals that can cover the same time duration in the training dataset. the strictness of the language reward module it turns out that if we decrease the strictness level, the agent may easily fall into the local minima, the agent can easily learn unexpected tricks to maximise the language reward and then the language reward prevents agent from exploring more. Thus it may lead to worse performance compared to a vanilla RL agent. Future work Connectionist Temporal Classification (CTC) handle temporal constraints -\u0026gt; help to recognise when the event start and end -\u0026gt; our current model is bad at telling this. change the style of reward shaping module \u0026ndash; from binary classifier to video - text generator help interpretability of the model (BTW, this is what I really want to do next because I just do not know why sometimes the model cannot recognise some instructions and sometimes it is too confident to some instructions) ","permalink":"https://sino-huang.github.io/posts/paper_proposal_nov_2022/","summary":"\u003cp\u003eYou need password to access to the content, go to Slack *#phdsukai to find more.\u003c/p\u003e","title":"Consider incremental publication of results Nov, 2022"},{"content":"[TOC]\nTitle: Can Language Models Be Specific? How? Author: Jie Huang et. al. Publish Year: 11 Oct 2022 Review Date: Tue, Nov 8, 2022 Summary of paper Motivation they propose to measure how specific the language of pre-trained language models (PLM) is, To achieve this, they introduced a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. for instance given \u0026ldquo;J.K. Rowling was born in [MASK]\u0026rdquo;, we want to test whether a more specific answer will be better filled by PLMs. e.g., Yate instead of England it is known that if the prediction is more specific, we can retrieve more fine-grained information from language models, and further acquire more information. viewer\u0026rsquo;s opinion: we are not saying that summarisation is easy or having less useful information, there are cases that abstract info is more useful Contribution although there are works on measuring how much knowledge is stored in PLMs or improving the correctness of the predictions, non attempted to measure or improve the specificity of prediction made by PLMs. Understanding how specific the language of PLMs is can help us better understand the behaviour of language models and facilitate downstream applications such as question answering etc. setup a dataset benchmark for specificity, The quality of the benchmark is high, where the judgment on which answer is more specific is ∼ 97% consistent with humans. Discovery in general, PLMs prefer less specific answers without subjects given, and they only have a weak ability to differentiate coarse-grained/fine-grained objects by measuring their (cosine) similarities to subjects. the results indicate that specificity was neglected by existing research on language models Improving specificity of the prediction few-shot prompting\nwhere demonstrations with more specific answers are provided to guide the models to produce more specific answers Cascade Prompting\nwhere \u0026ldquo;which\u0026rdquo; clauses are added as suffixes to bias the predictions to be more specific. Terms Specificity\nis a semantic feature of language to describe things specifically in a given context. or instance, to extract the answer (object) for relation (Toronto, location, X), we convert the query to a masked token prediction task using prompts, e.g., “Toronto is located in [MASK].” and let PLMs predict the masked token. The answer here can be a coarse-grained one, e.g., Canada, or a fine-grained one, e.g., Ontario. Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work the term \u0026ldquo;specificity\u0026rdquo; is related to the idea we mentioned before \u0026ldquo;the abstraction level of language information\u0026rdquo; this author, however, focus on how to increase the specificity of the PLM\u0026rsquo;s output. ","permalink":"https://sino-huang.github.io/posts/jie_huang-can-language-models-be-specific-how-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Can Language Models Be Specific? How?\u003c/li\u003e\n\u003cli\u003eAuthor: Jie Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 11 Oct 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Nov 8, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethey propose to measure how specific the language of pre-trained language models (PLM) is, To achieve this, they introduced a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts.\u003c/li\u003e\n\u003cli\u003efor instance given \u0026ldquo;J.K. Rowling was born in [MASK]\u0026rdquo;, we want to test whether a more specific answer will be better filled by PLMs. e.g., Yate instead of England\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"image-20221108211541780image-assetsimage-20221108211541780png\"\u003e\u003cimg alt=\"image-20221108211541780\" loading=\"lazy\" src=\"/posts/jie_huang-can-language-models-be-specific-how-2022/image-assets/image-20221108211541780.png\"\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eit is known that if the prediction is more specific, we can retrieve more fine-grained information from language models, and further acquire more information.\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eviewer\u0026rsquo;s opinion\u003c/em\u003e: we are not saying that summarisation is easy or having less useful information, there are cases that abstract info is more useful\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ealthough there are works on measuring how much knowledge is stored in PLMs or improving the correctness of the predictions, non attempted to measure or improve the specificity of prediction made by PLMs.\u003c/li\u003e\n\u003cli\u003eUnderstanding how specific the language of PLMs is can help us better understand the behaviour of language models and facilitate downstream applications such as question answering etc.\u003c/li\u003e\n\u003cli\u003esetup a dataset benchmark for specificity,  The quality of the benchmark is high, where the judgment on which answer is more specific is ∼ 97% consistent with humans.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"discovery\"\u003eDiscovery\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ein general, PLMs prefer less specific answers without subjects given, and they only have a weak ability to differentiate coarse-grained/fine-grained objects by measuring their (cosine) similarities to subjects.\u003c/li\u003e\n\u003cli\u003ethe results indicate that specificity was neglected by existing research on language models\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"improving-specificity-of-the-prediction\"\u003eImproving specificity of the prediction\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003efew-shot prompting\u003c/strong\u003e\u003c/p\u003e","title":"Jie_huang Can Language Models Be Specific How 2022"},{"content":"[TOC]\nTitle: Semantic-Aligned Fusion Transformer for One Shot Object Detection Author: Yizhou Zhao et. al. Publish Year: 2022 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/2203.09093v2.pdf\nSummary of paper Motivation with extreme data scarcity, current approaches, explore various feature fusions to obtain directly transferable meta-knowledge in this paper, they, attribute the previous limitation to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structure and scale variances. ","permalink":"https://sino-huang.github.io/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Semantic-Aligned Fusion Transformer for One Shot Object Detection\u003c/li\u003e\n\u003cli\u003eAuthor: Yizhou Zhao et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Oct 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2203.09093v2.pdf\"\u003ehttps://arxiv.org/pdf/2203.09093v2.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewith extreme data scarcity, current approaches, explore various feature fusions to obtain directly transferable meta-knowledge\u003c/li\u003e\n\u003cli\u003ein this paper, they, attribute the previous limitation to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structure and scale variances.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025164925086\" loading=\"lazy\" src=\"/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/image-assets/image-20221025164925086.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025165002340\" loading=\"lazy\" src=\"/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/image-assets/image-20221025165002340.png\"\u003e\u003c/p\u003e","title":"Yizhou_zhao Semantic Aligned Fusion Transformer for One Shot Object Detection 2022"},{"content":"[TOC]\nTitle: One-Shot Object Detection With Co-Attention and Co-Excitation Author: Ting-I Hsieh et. al. Publish Year: Nov 2019 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/1911.12529.pdf\nSummary of paper Motivation this paper aims to tackle the challenging problem of one-shot object detection, Given a query image patch whose class label is not included in the training data, To this end, they developed a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects first, use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasise correlated feature channels to help uncover relevant object proposals and eventually the target objects third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen training. https://github.com/timy90022/One-Shot-Object-Detection\n","permalink":"https://sino-huang.github.io/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: One-Shot Object Detection With Co-Attention and Co-Excitation\u003c/li\u003e\n\u003cli\u003eAuthor: Ting-I Hsieh et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Nov 2019\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Oct 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1911.12529.pdf\"\u003ehttps://arxiv.org/pdf/1911.12529.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025160104142\" loading=\"lazy\" src=\"/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/image-assets/image-20221025160104142.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis paper aims to tackle the challenging problem of one-shot object detection, Given a query image patch whose class label is not included in the training data,\u003c/li\u003e\n\u003cli\u003eTo this end, they developed a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects\n\u003cul\u003e\n\u003cli\u003efirst, use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation.\u003c/li\u003e\n\u003cli\u003esecond, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasise correlated feature channels to help uncover relevant object proposals and eventually the target objects\u003c/li\u003e\n\u003cli\u003ethird, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen training.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"heading\"\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025162638405\" loading=\"lazy\" src=\"/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/image-assets/image-20221025162638405.png\"\u003e\u003c/p\u003e","title":"Ting_i_hsieh One Shot Object Detection With Co Attention and Co Excitation 2019"},{"content":"[TOC]\nTitle: A Deep-One Shot Network for Query-Based Logo Retrieval Author: Ayan Kumar Bhunia et. al. Publish Year: Jul 2019 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/1811.01395.pdf\nSummary of paper Motivation Existing general purpose just cannot handle unseen new logos (not labelled logos) in this work, they developed an easy-to-implement query based logo detection and localisation system by employing a one-shot learning technique using off-the-shelf neural network components. Limitation of current work Deep-learning based framework are largely data-driven, contrary to logo-dataset that have several image classes but few images. need to be robust to new unseen logos, the model should be designed to satisfy the incremental demands for logo classes, contrary to existing methods which are limited to a set of seen logos and are not. Contribution propose a scalable solution for the logo detection problem, they present a query-based logo search and detection system by employing a simple fully differentiable one-shot learning framework which can be used for new logo classes without further training the whole network. to deal with the logos of varying sizes, we propose a novel one-shot framework through multi-scale conditioning that is specially designed to learn the similarity between the query image and target image at multiple scales and resolutions. Architecture Conditioning module\nthe logo image is converted into a multichannel feature vector of unit spatial dimension (1 x 1 x 512) Multi-scale conditioning\nthe Tile module helps to scale the 1x1x512 vector to target WxHx512 tile: Constructs a tensor by repeating the elements of input Minor comments they still need training data ","permalink":"https://sino-huang.github.io/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: A Deep-One Shot Network for Query-Based Logo Retrieval\u003c/li\u003e\n\u003cli\u003eAuthor: Ayan Kumar Bhunia et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Jul 2019\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Oct 24, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/1811.01395.pdf\"\u003ehttps://arxiv.org/pdf/1811.01395.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eExisting general purpose just cannot handle unseen new logos (not labelled logos)\u003c/li\u003e\n\u003cli\u003ein this work, they developed an easy-to-implement query based logo detection and localisation system by employing a one-shot learning technique using off-the-shelf neural network components.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025130007528\" loading=\"lazy\" src=\"/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/image-assets/image-20221025130007528.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"limitation-of-current-work\"\u003eLimitation of current work\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDeep-learning based framework are largely data-driven, contrary to logo-dataset that have several image classes but \u003cstrong\u003efew images\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eneed to be robust to new unseen logos, the model should be designed to satisfy the incremental demands for logo classes, contrary to existing methods which are limited to a set of seen logos and are not.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003epropose a scalable solution for the logo detection problem, they present a query-based logo search and detection system by employing a simple fully differentiable one-shot learning framework which can be used for new logo classes without further training the whole network.\u003c/li\u003e\n\u003cli\u003eto deal with the logos of varying sizes, we propose a novel one-shot framework through multi-scale conditioning that is specially designed to learn the similarity between the query image and target image at multiple scales and resolutions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"architecture\"\u003eArchitecture\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221025151708843\" loading=\"lazy\" src=\"/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/image-assets/image-20221025151708843.png\"\u003e\u003c/p\u003e","title":"Ayan_kumar_bhunia a Deep One Shot Network for Query Based Logo Retrieval 2019"},{"content":"[TOC]\nTitle: An Efficient Spatio-Temporal Pyramid Transformer for Action Detection Author: Yuetian Weng et. al. Publish Year: Jul 2022 Review Date: Thu, Oct 20, 2022 Summary of paper Motivation the task of action detection aims at deducing both the action category and localisation of the start and end moment for each action instance in a long, untrimmed video. it is non-trivial to design an efficient architecture for action detection due to the prohibitively expensive self-attentions over a long sequence of video clips To this end, they present an efficient hierarchical spatial temporal transformer for action detection Building upon the fact that the early self-attention layer in Transformer still focus on local patterns. Background to date, the majority of action detection methods are driven by 3D convolutional neural networks (CNNs), e.g., C3D, I3D, to encode video segment features from video RGB frames and optical flows however, the limited receptive field hinders the CNN-based models to capture long-term spatio-temporal dependencies. alternatively, vision transformers have shown the advantage of capturing global dependencies via the self-attention mechanism. Hierarchical ViTs divide Transformer blocks into several stages and progressively reduce the spatial size of feature maps when the network goes deeper. but having self-attention over a sequence of images is expensive also they found out that the global attention in the early layers actually only encodes local visual pattens (i.e., it only attends to its nearby tokens in adjacent frames while rarely interacting with tokens in distance frames) Efficient Spatio-temporal Pyramid Transformer some issues about redundancy\ntarget motions across adjacent frames are subtle, which implies large temporal redundancy when encoding video representations. they observed that the self-attention in the shallow layers mainly focuses on neighbouring tokens in a small spatial area and adjacent frames, rarely attending to other tokens in distant frames. encourage locality inductive bias\nfrom theoretical perspective, locality inductive bias suppresses the negative Hessian eigenvalues, thus assisting in optimisation by convexifying the loss landscape [Park, N., Kim, S.: How do vision transformers work? In: ICLR (2022)] jointly learn spatio-temporal representation\nthe model used Conv3D and attention to do this jointly rather than learn separately Some key terms Temporal feature pyramid network\nprogressively reduce the spatial and temporal dimension and enlarge the receptive field into different scales. the multi-scale spatio-temporal feature representation are further utilised to predict the temporal boundaries and categories via an anchor-free prediction and refinement module MViT\nprevious work lacks hierarchical structure or model spatio-temporal dependencies separately, which may not be sufficient for the task of action detection targeting these issues, MViT presents a hierarchical Transformer to progressively thrink the spatio-temporal resolution of feature maps while expanding the channels as the network goes deeper. Relation to existing video Transformers\nwhile others are based on separate space-time attention factorization, this method can encode the target motions by jointly aggregating spatio-temporal relations, without loss of spatio-temporal correspondence. apply local self-attention -\u0026gt; lower computational cost LSTA is data-dependent and flexible in terms of window size Additional Illustration of LSTA\nTemporal Feature Pyramid given an untrimmed video, action detection aims to find the temporal boundaries and categories of action instances, with annotation denoted by ${\\psi_n = (t_n^s, t_n^e, c_n)}_{n=1}^{N}$ the 3D feature maps are then fed to TFPN to obtain multi-scale temporal feature maps motivation multi-scale feature maps contribute to tackle the variation of action duration specifically, we construct an M-level temporal feature pyramid ${f_m}_{m=1}^M$, where $f_m \\in \\mathbb R^{T_m \\times C\u0026rsquo;}$ and $T_m$ is the temporal dimension of the m-th level. TFPN contains two 3D convolution layers followed by 1D Conv layer to progressively forms a feature hierarchy Refinement\nafter we predict class label $\\hat y_i^C$ and boundary distances $(\\hat b_i^s, \\hat b_i^e)$, they further predict an offset $(\\Delta \\hat b_i^s, \\Delta \\hat b_i^e)$ and the refinement action category label $\\hat y _i^R$\nIn the final prediction, we have the following form\nPotential future work we may use this to construct lang rew module\n","permalink":"https://sino-huang.github.io/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: An Efficient Spatio-Temporal Pyramid Transformer for Action Detection\u003c/li\u003e\n\u003cli\u003eAuthor: Yuetian Weng et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Jul 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Oct 20, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe task of action detection aims at deducing both the action category and localisation of the start and end moment for each action instance in a long, untrimmed video.\u003c/li\u003e\n\u003cli\u003eit is non-trivial to design an efficient architecture for action detection due to the prohibitively \u003cstrong\u003eexpensive\u003c/strong\u003e self-attentions over a long sequence of video clips\u003c/li\u003e\n\u003cli\u003eTo this end, they present an efficient hierarchical spatial temporal transformer for action detection\u003c/li\u003e\n\u003cli\u003eBuilding upon the fact that the early self-attention layer in Transformer still focus on local patterns.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eto date, the majority of action detection methods are driven by 3D convolutional neural networks (CNNs), e.g., C3D, I3D, to encode video segment features from video RGB frames and optical flows\n\u003cul\u003e\n\u003cli\u003ehowever, the limited receptive field hinders the CNN-based models to capture long-term spatio-temporal dependencies. alternatively, vision transformers have shown the advantage of capturing global dependencies via the self-attention mechanism.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHierarchical ViTs divide Transformer blocks into several stages and progressively reduce the spatial size of feature maps when the network goes deeper.\n\u003cul\u003e\n\u003cli\u003ebut having self-attention over a sequence of images is expensive\u003c/li\u003e\n\u003cli\u003ealso they found out that the \u003cem\u003eglobal attention\u003c/em\u003e in the early layers actually only encodes local visual pattens (i.e., \u003cu\u003eit only attends to its nearby tokens in adjacent frames\u003c/u\u003e while rarely interacting with tokens in distance frames)\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20221021185742980\" loading=\"lazy\" src=\"/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/image-assets/image-20221021185742980.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"efficient-spatio-temporal-pyramid-transformer\"\u003eEfficient Spatio-temporal Pyramid Transformer\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"image-20221022182202983\" loading=\"lazy\" src=\"/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/image-assets/image-20221022182202983.png\"\u003e\u003c/p\u003e","title":"Yuetian_weng an Efficient Spatio Temporal Pyramid Transformer for Action Detection 2022"},{"content":"[TOC]\nTitle: Human Level Atari 200x Faster Author: Steven Kapturowski et. al. DeepMind Publish Year: September 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2209.07550.pdf\nMotivation Agent 57 came at the cost of poor data-efficiency , requiring nearly 80,000 million frames of experience to achieve. this one can achieve the same performance in 390 million frames Contribution Some key terms NFNet - Normalisation Free Network\nhttps://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee Batch normalisation \u0026ndash; the bad it is expensive batch normalisation breaks the assumption of data independence NFNet applies 3 different techniques: Modified residual branches and convolutions with Scaled Weight standardisation Adaptive Gradient Clipping Architecture optimisation for improved accuracy and training speed. https://github.com/vballoli/nfnets-pytorch Previous Non-Image features\nNew features\nA1. Bootstrapping with online network\ntarget networks are frequently used in conjunction with value-based agents due to their stabilising effect, but this design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. but if we update the target more frequently, then it is no more stable so they use online network bootstrapping, and they stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute the loss. The trust region masks out the loss at any timestep for which both the following condition hold: this is similar to PPO A2. Target computation with tolerance\nAgent57 uses Rtrace (Similar to Vtrace) to compute return estimate from off-policy data, but they observed that it tends to cut traces too aggressively when using e-greedy policy thus slowing down the propagation of information into the value function so the new return estimater is B1. Loss and priority normalisation\n**B2. Cross-mixture training. **\ntrain all the policy network (distributed RL) rather than single policy network to increase efficiency C1. Normalizer-free torso network\nuse NFNet architecture C2. shared torso with combined loss\nintrinsic and extrinsic components are going to use shared network D. Robustifying behaviour via policy distillation\nthey proposed to train an explicit policy head $\\pi_{\\text{dist}}$ via policy distillation to match the e-greedy policy induced by the Q-function (since it is value-based RL so the policy is just a greedy policy) ","permalink":"https://sino-huang.github.io/posts/steven_kapturowski-human-level-atari-200x-faster-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Human Level Atari 200x Faster\u003c/li\u003e\n\u003cli\u003eAuthor: Steven Kapturowski et. al. DeepMind\u003c/li\u003e\n\u003cli\u003ePublish Year: September 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Oct 5, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2209.07550.pdf\"\u003ehttps://arxiv.org/pdf/2209.07550.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAgent 57 came at the cost of \u003cu\u003epoor data-efficiency\u003c/u\u003e , requiring nearly 80,000 million frames of experience to achieve.\u003c/li\u003e\n\u003cli\u003ethis one can achieve the same performance in 390 million frames\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eNFNet - Normalisation Free Network\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee\"\u003ehttps://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eBatch normalisation \u0026ndash; the bad\n\u003cul\u003e\n\u003cli\u003eit is expensive\u003c/li\u003e\n\u003cli\u003ebatch normalisation breaks the assumption of data independence\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNFNet applies 3 different techniques:\n\u003cul\u003e\n\u003cli\u003eModified residual branches and convolutions with Scaled Weight standardisation\u003c/li\u003e\n\u003cli\u003eAdaptive Gradient Clipping\u003c/li\u003e\n\u003cli\u003eArchitecture optimisation for improved accuracy and training speed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/vballoli/nfnets-pytorch\"\u003ehttps://github.com/vballoli/nfnets-pytorch\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePrevious Non-Image features\u003c/strong\u003e\u003c/p\u003e","title":"Steven_kapturowski Human Level Atari 200x Faster 2022"},{"content":"[TOC]\nTitle: CoBERL Contrastive BERT for Reinforcement Learning Author: Andrea Banino et. al. DeepMind Publish Year: Feb 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2107.05431.pdf\nMotivation Contribution Some key terms Representation learning in reinforcement learning\nmotivation: if state information could be effectively extracted from raw observations it may then be possible to learn from there as fast as from states. however, given the often sparse reward signal coming from the environment, learning representations in RL has to be achieved with little to no supervision. approach types class 1: auxiliary self-supervised losses to accelerate the learning speed in model-free RL algorithm class 2: learn a world model and use this to collect imagined rollouts, which then act as extra data to train the RL algorithm reducing the samples required from the environment CoBERL is in class 1 ​\tit uses both masked language modelling and contrastive learning RL using BERT architecture \u0026ndash; RELIC\nunlike BERT where the input is a discrete vocabulary for language learning and targets are available, in RL inputs consist of images, rewards and actions, so they construct proxy targets and the corresponding proxy tasks to solve. They used contrastive learning, RELIC Instead we rely on the sequential nature of our input data to create the necessary groupings of similar and dissimilar points needed for contrastive learning. Auxiliary loss\n15% MASK and recover CoBERL architecture\nLSTM is more efficient means 2 layer (one GRU one LSTM) Minor comments They measure performance on all 57 Atari games after running for 200 million frames (steps)\u0026hellip; ","permalink":"https://sino-huang.github.io/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: CoBERL Contrastive BERT for Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Andrea Banino et. al. DeepMind\u003c/li\u003e\n\u003cli\u003ePublish Year: Feb 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Wed, Oct 5, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2107.05431.pdf\"\u003ehttps://arxiv.org/pdf/2107.05431.pdf\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eRepresentation learning in reinforcement learning\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emotivation:\n\u003cul\u003e\n\u003cli\u003eif state information could be effectively extracted from raw observations it may then be possible to learn from there as fast as from states.\u003c/li\u003e\n\u003cli\u003ehowever, given the often sparse reward signal coming from the environment, learning representations in RL has to be achieved with little to no supervision.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eapproach types\n\u003cul\u003e\n\u003cli\u003eclass 1: auxiliary self-supervised losses to accelerate the learning speed in model-free RL algorithm\u003c/li\u003e\n\u003cli\u003eclass 2: learn a world model and use this to collect imagined rollouts, which then act as extra data to train the RL algorithm reducing the samples required from the environment\u003c/li\u003e\n\u003cli\u003eCoBERL is in class 1\n\u003cul\u003e\n\u003cli\u003e​\tit uses both masked language modelling and contrastive learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eRL using BERT architecture\u003c/strong\u003e \u0026ndash; RELIC\u003c/p\u003e","title":"Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022"},{"content":"[TOC]\nTitle: Sample Factory: Asynchronous Rl at Very High FPS Author: Alex Petrenko Publish Year: Oct, 2020 Review Date: Sun, Sep 25, 2022 Summary of paper Motivation Identifying performance bottlenecks\nRL involves three workloads:\nenvironment simulation inference backpropagation overall performance depends on the lowest workload In existing methods (A2C/PPO/IMPALA) the computational workloads are dependent -\u0026gt; under-utilisation of the system resources. Existing high-throughput methods focus on distributed training, therefore introducing a lot of overhead such as networking serialisation, etc.\ne.g., (Ray \u0026amp; RLLib \u0026lt;==\u0026gt; Redis/Plasma, Seed RL \u0026lt;==\u0026gt; GRPC, Catalyst \u0026lt;==\u0026gt; Mongo DB) Contribution Some key terms Double-buffered sampling\nwith double-buffered approach, environments simulators never wait Resolving bottleneck # 2 (communication)\nRL training process is based on static data structures: allocate all memory in the beginning of training, and then only send pointers around (shared memory suits for single server) Good things about the paper (one paragraph) Major comments Minor comments Incomprehension Potential future work ","permalink":"https://sino-huang.github.io/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Sample Factory: Asynchronous Rl at Very High FPS\u003c/li\u003e\n\u003cli\u003eAuthor: Alex Petrenko\u003c/li\u003e\n\u003cli\u003ePublish Year: Oct, 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Sun, Sep 25, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eIdentifying performance bottlenecks\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eRL involves three workloads:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eenvironment simulation\u003c/li\u003e\n\u003cli\u003einference\u003c/li\u003e\n\u003cli\u003ebackpropagation\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eoverall performance depends on the \u003cstrong\u003elowest\u003c/strong\u003e workload\u003c/li\u003e\n\u003cli\u003eIn existing methods (A2C/PPO/IMPALA) the computational workloads are dependent -\u0026gt; under-utilisation of the system resources.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExisting high-throughput methods focus on distributed training, therefore introducing a lot of overhead such as networking serialisation, etc.\u003c/p\u003e","title":"Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020"},{"content":"[TOC]\nTitle: Google Video Diffusion Models Author: Jonathan Ho et. al. Publish Year: 22 Jun 2022 Review Date: Thu, Sep 22, 2022 Summary of paper Motivation proposing a diffusion model for video generation that shows very promising initial results Contribution this is the extension of image diffusion model they introduce a new conditional sampling technique for spatial and temporal video extension that performs better. Some key terms Diffusion model\nA diffusion model specified in continuous time is a generative model with latents Training diffusion model\nlearning to reverse the forward process for generation can be reduced to learning to denoise $z_t \\sim q(z_t|x)$ into an estimate $\\hat x_\\theta (z_t, \\lambda_t) \\approx x$ for all $t$ (we will drop the dependence on $\\lambda_t$) to simplify notation. We train this denoising model $\\hat x_\\theta$ using a weighted MSE loss this reduction of generation to denoising can be justified as optimising a weighted variational lower bound on the data log likelihood under the diffusion model. Effective sampling with the new method for conditional generation \u0026ndash; predictor-corrector sampler\nin the conditional generation setting, the data $x$ is equipped with a conditional signal $c$, which may represent a text caption. To train a diffusion model to fit $p(x|c)$, the only modification that needs to be made is to provide $c$ to the model as $\\hat x_\\theta (z_t, c)$ Improvements to sample quality by classifier-free guidance\nVideo diffusion model Architecture and condition\nthe standard diffusion model is a U-Net which is a neural network architecture constructed as a spatial downsampling pass followed by a spatial upsampling pass with skip connections to the downsampling pass activations. The network is built from layer of 2D convolutional residual blocks, and each ConV block is followed by a spatial attention block. Conditioning information, such as $c$, is provided to the network in the form of an embedding vector (sentence embedding https://huggingface.co/sentence-transformers/clip-ViT-L-14), added into each residual block (they find it helpful to process these embedding vectors using several MLP layers before adding) For video data, we use a particular type of 3D U-Net that is factorised over space and time. space-only 3D convolution for instance, we change each 3x3 convolution into a 1x3x3 convolution (the first axis indexes video frames, the second and third index the spatial height and width) the attention in each spatial attention block remains as attention over space. (i.e., the first axis is treated as a batch axis) after each spatial attention block, we further insert a temporal attention block that perform attention over the first axis and treats the spatial axes as batch axes. this separation is good for computational efficiency Text-conditioned video generation\nhyperparameters\nPotential future work Maybe we can also use this training method and architecture to pretrain our image-action-text multimodal model\nwe can combine this with the latent diffusion model to increase computational efficiency.\n","permalink":"https://sino-huang.github.io/posts/jonathan_ho-video-diffusion-models-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Google Video Diffusion Models\u003c/li\u003e\n\u003cli\u003eAuthor: Jonathan Ho et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 22 Jun 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Sep 22, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eproposing a diffusion model for video generation that shows very promising initial results\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethis is the extension of image diffusion model\u003c/li\u003e\n\u003cli\u003ethey introduce a new conditional sampling technique for spatial and temporal video extension that performs better.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDiffusion model\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA diffusion model specified in continuous time is a generative model with latents\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20220923173758941\" loading=\"lazy\" src=\"/posts/jonathan_ho-video-diffusion-models-2022/image-assets/image-20220923173758941.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eTraining diffusion model\u003c/strong\u003e\u003c/p\u003e","title":"Jonathan_ho Video Diffusion Models 2022"},{"content":"[TOC]\nTitle: Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text Based Games Author: Dongwon Kelvin Ryu et. al. Publish Year: ACL 2022 Review Date: Thu, Sep 22, 2022 Summary of paper Motivation Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action space. A fundamental challenges in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. So, we want to inject external commonsense knowledge into the agent during training when the agent is most uncertain about its next action. Contribution In addition to performance increase, the produced trajectory of actions exhibit lower perplexity, when tested with a pre-trained LM, indicating better closeness to human language. Some key terms Exploration efficiency\nexisting RL agent are far away from solving TGs due to their combinatorially large action spaces that hinders efficient exploration Prior work on using commonsense knowledge graph (CSKG)\nprior works with commonsense focused on completing belief knowledge graph (BKG) using pre-defined CSKG or dynamic LM commonsense transformer-generated commonsense inferences. Nonetheless, there is no work on explicitly using commonsense as an inductive bias in the context of exploration for TGs Methodology Overview\nthey proposed commonsense exploration (COMMEXPL) which constructs a CSKG dynamically, using COMET, based on the state of the observations per step. Then, the natural language actions are scored with the COMET and agent, to re-rank the policy distributions. They refer to this as applying commonsense conditioning Incomprehension The author assumed that readers understand what are CSKG (commonsense knowledge graph) and COMeT model, which is not applicable to me.\n","permalink":"https://sino-huang.github.io/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text Based Games\u003c/li\u003e\n\u003cli\u003eAuthor: Dongwon Kelvin Ryu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: ACL 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Sep 22, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eText-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action space.\u003c/li\u003e\n\u003cli\u003eA fundamental challenges in TGs is the \u003cu\u003eefficient exploration of the large action space\u003c/u\u003e when the agent has not yet acquired \u003cu\u003eenough knowledge\u003c/u\u003e about the environment.\u003c/li\u003e\n\u003cli\u003eSo, we want to inject external commonsense knowledge into the agent during training when the agent is most uncertain about its next action.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eIn addition to performance increase, the produced trajectory of actions exhibit lower perplexity, when tested with a pre-trained LM, indicating better closeness to human language.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eExploration efficiency\u003c/strong\u003e\u003c/p\u003e","title":"Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022"},{"content":"[TOC]\nTitle: Language Models as Zero Shot Planners: Extracting Actionable Knowledge for Embodied Agents Author: Wenlong Huang et. al. Publish Year: Mar 2022 Review Date: Mon, Sep 19, 2022 Summary of paper Motivation Large language models are learning general commonsense world knowledge. so this paper, the author investigate the possibility of grounding high-level tasks, expressed as natural language (e.g., \u0026ldquo;make breakfast\u0026rdquo;) to a chosen set of action steps (\u0026ldquo;open fridge\u0026rdquo;). Contribution they found out that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. they proposed several tools to improve executability of the model generation without invasive probing or modifications to the model. Some key terms What is prompt learning\nMethodology they will have 2 steps\nin prompt learning way, convert the high level tasks into mid-level plans convert the mid-level plans into admissible actions loop admissible action planning by semantic translation Many reasons cause the failure of mapping from free-form language to unambiguous actionable steps the output does not follow pre-defined mapping of any atomic actions e.g., \u0026ldquo;I first walk to the bedroom\u0026rdquo; is not of the format \u0026ldquo;walk to \u0026lt;PLACE\u0026gt;\u0026rdquo; the output may refer to atomic action and objects using words unrecognisable by the environment e.g., \u0026ldquo;microwave the chocolate milk\u0026rdquo; where \u0026ldquo;microwave\u0026rdquo; and \u0026ldquo;chocolate milk\u0026rdquo; cannot be mapped to precise action and objects. the output contains lexically ambiguous words e.g., open TV vs switch on TV SOLUTION: cosine similarity of the language embeddings of the action phrases \u0026hellip; ","permalink":"https://sino-huang.github.io/posts/wenlong_huang-language-models-as-zero-shot-planners-extracting-actionable-knowledge-for-embodied-agents-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language Models as Zero Shot Planners: Extracting Actionable Knowledge for Embodied Agents\u003c/li\u003e\n\u003cli\u003eAuthor: Wenlong Huang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Mar 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mon, Sep 19, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLarge language models are learning general commonsense world knowledge.\u003c/li\u003e\n\u003cli\u003eso this paper, the author investigate the possibility of grounding high-level tasks, expressed as natural language (e.g., \u0026ldquo;make breakfast\u0026rdquo;) to a chosen set of action steps (\u0026ldquo;open fridge\u0026rdquo;).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethey found out that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training.\u003c/li\u003e\n\u003cli\u003ethey proposed several tools to improve executability of the model generation without invasive probing or modifications to the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is prompt learning\u003c/strong\u003e\u003c/p\u003e","title":"Wenlong_huang Language Models as Zero Shot Planners Extracting Actionable Knowledge for Embodied Agents 2022"},{"content":"[TOC]\nTitle: VinVL: Revisiting Visual Representations in Vision Language Models Author: Pengchuan Zhang et. al. Publish Year: 10 Mar 2021 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model Oscar.\nAnd utilise an improved approach OSCAR + to pretrain the VL model\nContribution has a bigger Object Detection model with larger amount of training data, called \u0026ldquo;ResNeXt-152 C4\u0026rdquo; Some key terms Vision Language Pretraining\nit often consists of two stages an object detection model is pre-trained to encode an image and the visual objects in the image to feature vectors, and a cross-modal fusion model is pre-trained to blend text and visual features. this paper focuses on improving the object-centric visual representations and present a comprehensive empirical study to demonstrate that visual features matter in VL model. Vision Language models typically consists of two modules\nDeep learning-based VL models typically consists of two modules: an image understanding module Vision and a cross-modal understanding module VL Training object detection module\nenhance visual concepts of tail classes, we perform class-aware sampling to get at least 2000 instances per class. balance the contribution of each dataset Unify the object vocabularies in the end they obtained 1848 classes C4 object detection architecture is better than FPN architecture OSCAR+ pretraining method $$ \\mathcal{L}{\\text{Pre-training}} = \\mathcal{L}{\\text{MTL}} + \\mathcal{L}_{\\text{CL3}} $$\nMTL is the masked Token loss by masking text and tag tokens by 15% probability. CL3 takes into account two types of training sample x: the {caption, image-tags, image-features} triplets of the image captioning and image tagging data, and the {question, answer, image-features} triplets of the VQA data contains 50% matched triples, 25% w-polluted triples, and 25% q- polluted triples. Result the proposed 3-way contrastive loss transfers well to both tasks. (text-image retrieval task and VQA task) Good things about the paper (one paragraph) Github Page: https://github.com/pzzhang/VinVL\nPotential future work transfer the triplet loss to our work\n","permalink":"https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: VinVL: Revisiting Visual Representations in Vision Language Models\u003c/li\u003e\n\u003cli\u003eAuthor: Pengchuan Zhang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 10 Mar 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Sep 3, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIn our experiments we feed the visual features generated by \u003cstrong\u003ethe new object detection model\u003c/strong\u003e into a Transformer-based VL fusion model Oscar.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAnd utilise an improved approach OSCAR + to pretrain the VL model\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003ehas a bigger Object Detection model with larger amount of training data, called \u0026ldquo;ResNeXt-152 C4\u0026rdquo;\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eVision Language Pretraining\u003c/strong\u003e\u003c/p\u003e","title":"Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021"},{"content":"[TOC]\nTitle: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks Author: Xiujun Li et. al. Publish Year: 26 Jul 2020 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation Existing method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.\nthe lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.\nMoreover, visual regions are often over-sampled, noisy and ambiguous, which makes the task even more challenging.\nin this paper, they proposed a method that uses object tag detected in images as anchor points to significantly ease the learning of alignments.\nMotivation\nthe salient objects in an image can be accurately detected and are often mentioned in the text. Some key terms how self-attention transformer learn cross-modal contextualised representation\nit takes visual region features $v = {v_1, \u0026hellip;, v_k}$ and word embedding $w = {w_1, \u0026hellip;, w_T}$ of its paired text as input, and relies on the self-attention mechanism to learn image-text alignments to produce cross-modal contextual representation. What things VLP method suffers\nambiguity, the visual region features are usually extracted from over-sampled regions, which inevitably results in overlaps among image regions at different positions. This renders ambiguities for the extracted visual embeddings. i.e., overlap of objects in the image lack of grounding. there is no explicit label alignments between regions or objects in an image and words or phrases in text. therefore we may want to summarised the image further so that we can match with the abstract words. OSCAR architecture\nword and tag are all in BERT text embeddings images is the set of region vectors of the image. Preprocess Region feature\nGiven an image with K regions of objects (normally over-sampled and noisy), Faster R-CNN is used to extract the visual semantics of each region as (v\u0026rsquo;, z), where v\u0026rsquo; is the region feature and z is the region position. They concatenate $v\u0026rsquo;$ and $z$ to form a position-sensitive region feature vector, which is further transformed into $v$ using a linear projection to ensure that it has the same vector dimension as that of word embeddings. Pre-training objective\n15% Maksed Token Loss\nContrastive loss\napply a fully-connected (FC) layer on the top of the special token [CLS] as a binary classifier f(.) to predict whether the pair contains the original image representation or any polluted ones. $\\mathcal{L}{\\text{Pre-training}} = \\mathcal{L}{\\text{MTL}} + \\mathcal{L}_C$\nResults feature visualisation\nWe observe small distances between text and image features of the same objects; some of them are perfectly aligned, as demonstrated by the overlapping regions. Good things about the paper (one paragraph) The code and pre-trained models are released: https://github.com/microsoft/ Oscar\n","permalink":"https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks\u003c/li\u003e\n\u003cli\u003eAuthor: Xiujun Li et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 26 Jul 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Sep 3, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eExisting method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ethe lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.\u003c/p\u003e","title":"Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020"},{"content":"[TOC]\nTitle: DiffCSE: Difference Based Contrastive Learning for Sentence Embeddings Author: Yung-Sung Chuang et. al. Publish Year: 21 Apr 2022 Review Date: Sat, Aug 27, 2022 Summary of paper Motivation DiffCSE learns sentences that are sensitive to the difference between the original sentence and and edited sentence. Contribution we propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings Some key terms DiffCSE\nthis is an unsupervsied contrastive learning framework rather than model architecture Contrastive learning in single modality data\nuse multiple augmentations on a single datum to construct positive pairs whose representations are trained to be more similar to one another than negative pairs e.g., random cropping, color jitter, rotation for vision models dropout for NLP Contrastive learning encourages representations to be insensitive to these transformations i.e. the encoder is trained to be invariant to a set of manually chosen transformations Limitation of existing contrastive learning data augmentation for language data\nGao et. al. find that constructing positive pairs via a simple dropout-based augmentation works much better than more complex augmentations such as word deletions or replacements based on synonyms or masked language models. Hindsight behind direct augmentation in language\nwhile the training objective in contrastive learning encourages representations to be invariant to augmentation transformations, direct augmentations on the input (e.g., deletion, replacement) often change the meaning of the sentence. Methodology we propose to learn sentence representations that are aware of, but not necessarily invariant to, such direct surface-level augmentations\nwe operationalise equivariant contrastive learning on sentences by using dropout-based augmentation as the insensitive transformation and MLM-based word replacement as the sensitive transformation SimCSE\nMeaning of $x_i^+ = x_i$\nthis means that the sentence that is going to augmented is the original sentence.\nThey output $h_i^+$ just by using the random dropout layer.\nPotential future work we may directly use the sentence embedding from this model\ngithub page: https://github.com/voidism/DiffCSE\n1 model_bert_trans = DiffCSE(\u0026#34;voidism/diffcse-bert-base-uncased-trans\u0026#34;) However, this sentence embedding is not caring about the binary classification\nSo if we want to know the binary classification Transformer stuffs, please read: Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval\n","permalink":"https://sino-huang.github.io/posts/yung_sung_chuang-diffcse-difference-based-contrastive-learning-for-sentence-embeddings-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: DiffCSE: Difference Based Contrastive Learning for Sentence Embeddings\u003c/li\u003e\n\u003cli\u003eAuthor: Yung-Sung Chuang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 21 Apr 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Aug 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDiffCSE learns sentences that are sensitive to the difference between the original sentence and and \u003cstrong\u003eedited sentence\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ewe propose DiffCSE, an unsupervised contrastive learning framework for learning \u003cstrong\u003esentence embeddings\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDiffCSE\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethis is an unsupervsied contrastive learning framework rather than model architecture\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eContrastive learning in single modality data\u003c/strong\u003e\u003c/p\u003e","title":"Yung_sung_chuang Diffcse Difference Based Contrastive Learning for Sentence Embeddings 2022"},{"content":"[TOC]\nTitle: Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval Author: Gregor Geigle et. al. Publish Year: 19 Feb, 2022 Review Date: Sat, Aug 27, 2022 Summary of paper Motivation they want to combine the cross encoder and the bi encoder advantages and have a more efficient cross-modal search and retrieval\nefficiency and simplicity of BE approach based on twin network expressiveness and cutting-edge performance of CE methods. Contribution We propose a novel joint Cross Encoding and Binary Encoding model (Joint-Coop), which is trained to simultaneously cross-encode and embed multi-modal input; it achieves the highest scores overall while maintaining retrieval efficiency\nSome key terms Bi-encoder approach\nencodes images and text separately and then induces a shared high-dimensional multi-modal feature space. very common\ncross attention based approach\napply a cross attention mechanism between examples from the two modalities to compute their similarity scores\nlimitation of cross attention approach they have extremely high search latency may results in inflated and misleading evaluation performance when using small benchmarks Methodology Pretraining part\nSimilar to masked language modelling (MLM) in the text domain, multi-modal Transformer models are trained with self-supervised objectives. For pretraining, image-caption datasets (i.e., MSCOCO, Flickr30k, Conceptual Captions and SBU) are utilised The pretrained multi-modal model is subsequently fine-tuned with multi-modal downstream task data. cross-encoder training\ntraining A pretrained model receives as input positive and negative pairs of image and captions A binary classification head is placed on top of the Transformer model, where the contexualized embedding of the [CLS] token is passed into the classification head. The weight of classifier head together with the Transformer are fully fine-tuned using a binary cross-entropy (BCE) loss bi-encoding training\ntraining the objective of the twin network is to place positive training instances (image, caption) (i,c) closely in the shared multi-modal space, while unrelated instance should be placed farther apart. This is formulated through a standard triplet loss function. It leverages (i,c,c\u0026rsquo;) and (i,i\u0026rsquo;, c) triplets, where (i,c) are positive image-caption pairs from the training corpus, while c\u0026rsquo; and i\u0026rsquo; are negative examples sampled from the same corpus such at (i, c\u0026rsquo;) and (i\u0026rsquo;, c) do not occur in the corpus, the triplet loss is then bi-encoding retrieval\nthe BE approach enables pre-encoding of all items for efficient retrieval loop-up and thus this approach can scale to even billions of images Joint Coop process\nUse Bi-Encoder to retrieve top K first and then use Cross Encoder to rank based on the Sigmoid classification score [0, 1] Training setup and hyperparameters Good things about the paper (one paragraph) Github page: https://github.com/UKPLab/MMT-Retrieval\nMajor comments Minor comments Incomprehension Potential future work Try this to get similarity score for multi-modality data\nAlso try DeepNet to solve gradient vanishing problem\n","permalink":"https://sino-huang.github.io/posts/gregor_geigle-retrieve-fast-rerank-smart-coorperative-and-joint-approaches-for-improved-cross-modal-retrieval-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval\u003c/li\u003e\n\u003cli\u003eAuthor: Gregor Geigle et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 19 Feb, 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Sat, Aug 27, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003ethey want to combine the cross encoder and the bi encoder advantages and have a more efficient cross-modal search and retrieval\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eefficiency and simplicity of BE approach based on twin network\u003c/li\u003e\n\u003cli\u003eexpressiveness and cutting-edge performance of CE methods.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cp\u003eWe propose a novel joint Cross Encoding and Binary Encoding model (Joint-Coop), which is trained to simultaneously cross-encode and embed multi-modal input; it achieves the highest scores overall while maintaining retrieval efficiency\u003c/p\u003e","title":"Gregor_geigle Retrieve Fast Rerank Smart Cooperative and Joint Approaches for Improved Cross Modal Retrieval 2022"},{"content":"[TOC]\nTitle: MPNet: Masked and Permuted Pre-training for Language Understanding Author: Kaitao Song et. al. Publish Year: 2020 Review Date: Thu, Aug 25, 2022 Summary of paper Motivation BERT adopts masked language modelling (MLM) for pre-training and is one of the most successful pre-training models.\nSince BERT is all attention block and the positional embedding is the only info that care about the ordering, BERT neglects dependency among predicted tokens\nXLNet introduces permuted language modelling (PLM) to address dependency among the predicted tokens. So in addition to XLNet, MPNet takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy Contribution Obtain better results than BERT and XLNet Some key terms Comparison between BERT and XLNet\nLimitation (Output dependency) in Autoencoding (BERT)\nMLM assumes the masked token are independent with each other and predicts them separately, which is not sufficient to model the complicated context dependency in natural language\ne.g., [MASK] [MASK] is a city -\u0026gt; New York is a city (MLM autoencoding) the dependency between New and York are not captured In contrast, PLM factorizes the predicted tokens with the product rule in any permuted order, which avoids the independence assumption in MLM and can better model dependency among predicted tokens.\nLimitation (position discrepancy) in random shuffling autoregression (XLNet)\nSince in downstream tasks, a model can see the full input sentence to ensure the consistency between the pre-training and fine-tuning, the model should see as much as information as possible of the full sentence during pre-training. In MLM, their position information are available to the model to (partially) represent the information of full sentence (how many tokens in a sentence i.e., the sentence length).\nHowever, each predicted tokens in PLM can only see its preceding tokens in a permuted sentence but does not know the position information of the full sentence during the autoregressive pretraining, which brings discrepancy between pre-training and fine-tuning. so as long as the downstream task is not text generation, then autoregression method has this limitation. Proposed method Architecture\nTwo-stream diagram (not clear actually)\nResults Good things about the paper (one paragraph) quite good and combine all the stuffs together\nPotential future work We may use this model to get temporal information for video input\n","permalink":"https://sino-huang.github.io/posts/kaitao_song-mpnet-masked-and-permuted-retrain-for-language-understanding-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: MPNet: Masked and Permuted Pre-training for Language Understanding\u003c/li\u003e\n\u003cli\u003eAuthor: Kaitao Song et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Aug 25, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBERT adopts masked language modelling (MLM) for pre-training and is one of the most successful pre-training models.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSince BERT is all attention block and the positional embedding is the only info that care about the ordering, BERT neglects dependency among predicted tokens\u003c/p\u003e","title":"Kaitao_song Mpnet Masked and Permuted Retrain for Language Understanding 2020"},{"content":"[TOC]\nTitle: Vision Language Models Towards Multimodal Deep Learning Author: Sergios Karagiannakos Publish Year: 03 Mar 2022 Review Date: Tue, Aug 9, 2022 https://theaisummer.com/vision-language-models/\n","permalink":"https://sino-huang.github.io/posts/sergios_karagiannakos-vision-language-models-towards-multimodal-dl-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Vision Language Models Towards Multimodal Deep Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Sergios Karagiannakos\u003c/li\u003e\n\u003cli\u003ePublish Year: 03 Mar 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Aug 9, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://theaisummer.com/vision-language-models/\"\u003ehttps://theaisummer.com/vision-language-models/\u003c/a\u003e\u003c/p\u003e","title":"Sergios_karagiannakos Vision Language Models Towards Multimodal Dl 2022"},{"content":"[TOC]\nTitle: Multi-modal Alignment Using Representation Codebook Author: Jiali Duan, Liqun Chen et. al. Publish Year: 2022 CVPR Review Date: Tue, Aug 9, 2022 Summary of paper Motivation aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion. since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. Contribution in this paper, we treat image and text as two \u0026ldquo;views\u0026rdquo; of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook). to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. Some key terms Types of Vision language pre-training tasks\nmultimodal alignment: aligning the feature spaces of different modalities late fusion approaches such as CLIP and ALIGN focus on this cross-modal fusion: capturing the interaction across modalities. early fusion approaches such as OSCAR, VinVL and VilLT focus on this **momentum distillation, **\nfor each of the image, text and fusion encoder, there is a corresponding encoder that is updated through moving average without gradient back propagation. These momentum encoder serve as teachers to guide the self-supervised learning process. In this paper, we use the teachers to guide codebook learning as well as for the cross-modal and intra-modal alignment codebook\ncodebook is a d-by-K matrix used as projector to project image and text feature into a common space.\nMethod in this work, features from image and text modalities were first aligned and then fused using a transformer encoder. the main focus of the work is on the feature alignment stage -\u0026gt; make it more efficient the main contribution of this method is: using a codebook that quantizes the common text-image feature into codewords (cluster centre). this cluster centre provide a more stable means for contrastive reasoning compared to individual text or visual features. Inspiration from SwAV\nTwo augmented versions (views) of the same input image were passed through a deep network for feature extraction. visual embedding was learned by optimising an objective function that enforces the consistency between the feature from one and the assigned cluster from the other view (different view leads to the same entity and that entity is represented as cluster (or codeword in this paper))\nEffectively, visual and text features are lined up via aligning with the common codewords during training. Overview of the framework\nOptimal Transport\nhttp://alexhwilliams.info/itsneuronalblog/2020/10/09/optimal-transport/\nthis allows the feature vector to be similar to one of the codeword cluster centre\nOptimal Transport is a little bit complex, we may want to use alternative way to implement this.\nEssentially the idea is that we want to have an intermediate cluster centre vector so that both image feature and text feature can take projection on this.\nPotential future work Use the alignment loss in this paper to train our model\nAlthough this model does not consider the temporal order / sequence alignment\n","permalink":"https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Multi-modal Alignment Using Representation Codebook\u003c/li\u003e\n\u003cli\u003eAuthor: Jiali Duan, Liqun Chen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022 CVPR\u003c/li\u003e\n\u003cli\u003eReview Date: Tue, Aug 9, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ealigning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion.\u003c/li\u003e\n\u003cli\u003esince image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ein this paper, we treat image and text as two \u0026ldquo;views\u0026rdquo; of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook).\u003c/li\u003e\n\u003cli\u003eto further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTypes of Vision language pre-training tasks\u003c/strong\u003e\u003c/p\u003e","title":"Jiali_duan Multimodal Alignment Using Representation Codebook 2022"},{"content":"This page is not completed yet\nYou need password to access to the content, go to Slack *#phdsukai to find more.\nPart of this article is encrypted with password:\n--- DON'T MODIFY THIS LINE --- [TOC]\nMotivation of the research We may lack a solid justification why bother developing the natural language understanding ability for learning-based agent when the agent can eventually find its way solving tasks by a large amount of training steps. Moreover, although people are saying that natural language understanding ability helps agents to increase sample efficiency and generalisation ability, we lack a systematic experiment to demonstrate that. Therefore I think it is worth investigating how natural language understanding ability really influences the learning process of agent.\nInspiration from how people learn physics Claim: All of the incoming discussions are only applicable for agents which use neural network to construct their policy module (brain) and all the of the incoming discussions only make sense if we believe that the weights of the neural network somehow represent the \u0026ldquo;intelligence\u0026rdquo; of agents.\nThe following figure shows the relationship of various physics theories\nOk, it is clearly known that people learn classical physics before they learn the modern physics theories that are more accurate to explain the physical phenomena in the universe.\nWhy not directly learn modern physics? The reason is straightforward: modern physics is complex and difficult to be understood. People will experience a steep learning curve and in the end they may spend a large amount of time and still fail to understand the modern physics theories. In contrast, people can build a foundation of knowledge by learning the classical physics, and \u0026ldquo;by standing upon the shoulders of classical physics\u0026rdquo;, we can understand the modern physics more easily.\nSo, what is the take-away lesson\nwhen learning something difficult, it might be more (time) efficient to learn several simpler but related things first so as to build some foundations, before learning the difficult one. This is equivalent to the following statement:\nwhen learning to solve a difficult task, it might be more (time) efficient to ask the learning based agent to learn to solve simpler tasks as the intermediate step to build a foundation of related knowledge before learning to solve the difficult task. So, here is the research title: \u0026ldquo;using instruction following as a intermediate training step towards a general learning-based agent\u0026rdquo; so we want to ask the learning based agent to learn to solve instruction following tasks as the intermediate step to build a foundation of related knowledge general intelligence (since it is natural language understanding) before learning to solve the difficult task.\nExperiment setting Given a shared environment, we will examine on three learning styles\ndirectly learn the problem solving task through the interaction of the environment learn some simple instruction following tasks in the same environment, after that, learn to solve the target task through the interaction with the environment provided with the associated walkthrough info learn some simple instruction following tasks in the same environment, after that, learn to solve the target task through the interaction without any auxiliary instruction info. So, the result of this experiment should visualise the relationships of the following dimension:\ntask difficulty total learning time agent performance learning style If the AI agent learning is similar to human learning, then we expected to see the following diagram\nFigure No Figure 1 2 3 Well, in fact, the green line may just indicates that agents who can comprehend the language walkthrough info is able to solve difficult tasks because the difficult task become easy if we provide walkthrough and all the intermediate learning steps are the prerequisite for agent to comprehend the walkthrough.\nWhat would be a surprise is the blue line in Figure 3, it means that the intermediate instruction following tasks indeed helps to form a foundation of general intelligence which will help the agent to solve difficult task. (the paper can wikipeida help offline reinforcement learning may imply this)\nIf that\u0026rsquo;s the case (the blue line), it is also worth conducting experiments on how the quality of instruction following task would affect the final performance. Can the agent even learn solve the final task more quickly by having intermediate instruction following tasks with bad instructions (bad means that the behaviour is toxic to the final task, e.g., ask the agent to jump into a snake), I expect that if we ensure the diversity of the instruction following task, intermediate instruction following tasks with bad instruction (bad for the final task) will also be helpful.\nI also tried to explain why agent can utilise toxic demonstration in HER section 3.3 using the concept of compositionality, which is much clearer than the term \u0026ldquo;general intelligence\u0026rdquo;, which is the following:\ntoxic demonstrations can be utilised by understanding the compositionality of the task with the help of natural language descriptions. For example, we have a toxic demonstration about an agent who jumped into a snake and died. This demonstration can then be decomposed in two dimensions – 1). objects “snake” and 2). action “jump into”. The agent turns out to learn two neutral knowledge through the decomposition process. After extensive training, we provide the agent with the correct instruction ”you should jump over a snake” and the agent is expected to composite knowledge of “snake” and knowledge of “jump over” together to form a desirable action plan. So the following statements are equivalent:\nNatural language understanding ability helps learning based agent in decision-making process The learning process of solving instruction following tasks help to build a foundation of general intelligence which will help agent to solve the difficult task. The learning process of solving instruction following tasks helps the agent to understand the compositionality of complex tasks and thus decompose the complex task into familiar subtasks. ","permalink":"https://sino-huang.github.io/posts/instruction-following-as-a-path-to-general-problem-solving-agent-aug-2022/","summary":"\u003cp\u003e\u003cstrong\u003e\u003cu\u003eThis page is not completed yet\u003c/u\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou need password to access to the content, go to Slack *#phdsukai to find more.\u003c/p\u003e","title":"A preliminary idea about using instruction following as a intermediate training step towards a general learning-based agent"},{"content":"You need password to access to the content, go to Slack *#phdsukai to find more.\nPart of this article is encrypted with password:\n--- DON'T MODIFY THIS LINE --- [TOC]\nThesis Structure Plan In our research, we start with an investigation of the extent to which the task-dependent natural language information can assist in the decision making process of learning-based agents. Specifically, we will present several methods to improve utilisation of the language information based on the existing approaches. Improving utilisation means we want to improve learning agent\u0026rsquo;s efficiency and generalisation performance with the use of language information. (This will become one chapter (Chapter A) in my PhD thesis)\nAfter we present techniques for improving language-assisted learning agents to surpass baseline (i.e., Goyal\u0026rsquo;s model) in terms of efficiency and generalisation performance (in Chapter A), I will continue to conduct an empirical study to investigate how natural language understanding ability influences the learning process of agent. And we expect that this investigation will give us a solid justification why we want to have a natural language understanding agent for solving complex tasks, which I think we currently lack such a justification. The preliminary ideas can be found here. (This will become one chapter (Chapter B) in my PhD thesis)\nChapter A will treat the language information as having a unified level of abstraction. So in a new chapter (Chapter C), we will investigate the effect of various granularity of language information in training agents (we expect some observations:\nModels that are good at handling low-level language information (these models will be the products of Chapter A) may face difficulty handling abstract language information (agent may require longer training time or even fail to converge). Agents that are trained to utilise abstract language may have better the generalisation performance. After the above investigation, we will investigate the approaches that will decrease the agent\u0026rsquo;s training time when utilising abstract language data. Moreover, we will present a model that can automatically handle both low level and high level language information (e.g., includes general guides in the Manual book and low-level walkthrough for specific game level) to solve the game task. The preliminary ideas can be found here.\nBut for Chapter A stuffs, I want to give you my own opinions about what are types of ways to improve existing approaches. It is because later I will categorise my proposed methods of improving existing language assisted learning agents into these types.\nIn a broader perspective, what are the types of ways to improve existing approaches Re-design Model Architecture Aim: capture the inductive bias of the input in a better way E.g., Why CNN (e.g., VGG net) is good for image classification task Reason: translation invariance property of image classification is captured by CNN model. What is translation invariance: class of object won\u0026rsquo;t change no matter the object is at which position of the image. Preprocess the input Aim: pre-processing of input is a concrete approach based on divide and conquer principle \u0026ndash; we let the model to focus on fewer tasks rather than all-in-one, and by doing this, the model (neural network) can converge faster E.g., Preprocessing of the text tokens into semantic embeddings using pre-trained model is helpful for downstream NLP tasks Modify the training process Aim: leads to a more effective learning E.g., Contrastive learning technique is about to generate lots of different types of negative samples to train the model Consequence: the model is able to learn such an embedding space in which dissimilar sample pairs are far apart. And this embedding space is more effective to captures the semantic meaning Include data augmentation and pre-trained model Aim: generate more training data and also transfer sharable knowledge from pre-trained model. This is a task-agnostic and model-agnostic technique to improve the performance. Later I will categorise my proposed methods into these types. But before we do that, let\u0026rsquo;s recall the existing agent structure that handles language information\nStructure of language assisted learning agent Let\u0026rsquo;s recall the structure of language assisted learning agent\nHere are some delimitations:\nWe express the problem in MDP format. Reason: we just want to stick to the convention because games in OpenAI gym environment, Atari Learning environment and also NetHack learning environment are expressed in MDP format. the goal of learning agent is to Maximise the accumulated reward it can get in one episode If the goal state of the game is stated, then the agent should reach goal state of the game using minimum steps Reason: there is no goal state required in vanilla MDP expression, but in video games there is usually a goal (e.g., escape the current level) In our study (Chapter A), we will focus on three components of the agent\nreward shaping module\nThe role of reward shaping module: give rewards according to whether the past observations meet the expectation.\nTo be more specific, it will give rewards if the agent follows the instruction or obeys the guides\n(Advanced) it will also give rewards if the agent concretises abstract information into a good decision (a good decision means expert player will also deduce this decision based on the message) (will also be investigated in Chapter C)\ne.g., in NetHack guidebook Weapon section, it says\n\u0026ldquo;You need weapons for self-defense. Monk characters are an exception; they normally do more damage with bare (or gloved) hands than they do with weapons.\u0026rdquo;\nReward should be given if agent decide not to wield a weapon when they play Monk character.\nfusion module\nThe role of fusion module: fuse observation and language into a unified multimodal representation common expectations of the fusion: condense -\u0026gt; smaller multimodal space dimension to avoid the curse of dimensionality robust -\u0026gt; is able to capture the semantic meaning even if some associated unimodal information is masked/missing. accurate -\u0026gt; the multimodal representation is able to capture accurate semantic meaning of the original unimodal data. policy module\nThe role of policy module: train a policy network that can correctly handle the incoming observations and reward signals so as to output a sequence of actions that solves the task. Now let\u0026rsquo;s see what directions we have to improve the agent based on these three components and their roles.\nDirections of improvement. To what extent the task-dependent natural language information can assist in the decision making process of learning-based agents? To answer this question, we form the following sub-questions based on the roles of reward shaping module, fusion module and policy network module in the language-assisted learning agent and then propose methods for the questions:\nReward shaping part\nHow to give more accurate and informative rewards when the agent follows the instruction or obeys the guides?\nRe-design Model Architecture\nState-aware reward shaping module (See Section 3.1 in confirmation report) Motivation: past observation (state) information is better than past actions in terms of inferring the agent\u0026rsquo;s behaviour. Therefore, a state-aware reward shaping module is capable of making more accurate prediction on whether the agent followed the instruction. Pre-process the input\nConverting image pixel into latent vector using Intra-cross attention masked learning (similar to BERT) (See Section 3.1.1 in confirmation report) Motivation: allowing the image feature vector to capture the temporal ordering information Converting image pixel into latent vector representation from pretrained object recognition model (soft-discretisation process) (See Section 3.1.2 in confirmation report) Motivation: allowing the reward shaping module to conduct reasoning process on the object level rather than pixel level. Converting language instruction into Linear Temporal Logic expression (See Section 3.4 in confirmation report) Motivation: explicitly showing the temporal constraints in the natural language and thus the reward shaping module becomes more sensitive to the temporal relationship of events. Modify the training process\nCurrent method: input: two unimodal representations of trajectory and language instruction, output: binary classification output about whether trajectory and instruction are matched. Proposed method 1.1 (not mentioned in the confirmation report) : title: treat unimodal data as the projection of the corresponding event onto that modality\u0026rsquo;s feature space Aim: we treat trajectory modality and language instruction modality as different views from an event. Therefore, we train two separate unimodal encoders that will project trajectory and language instruction onto the same event embedding space and use contrastive learning to force the cross-modal encoding output of the same event to be similar (cross-modal similarity) but decrease the intra-modal similarity among different event. Finally, the reward is measured by the similarity score of the encoding vector of the trajectory data and the encoding vector of the language data. (this is inspired from paper multimodal alignment using representation codebook) Having an explicit event vector might be helpful to compare the similarity of events: e.g., \u0026ldquo;avatar killed a skull in room A\u0026rdquo; vs \u0026ldquo;avatar killed a skull in room B\u0026rdquo; even though two observations might be highly different due to different room decorations, the event vector might be similar. Motivation 1: introduce the idea of intra-modal similarity score, may increase the robustness Motivation 2: we obtain an explicit event vector, which can be directly treated as the fused multimodal representation (which will link to Early fusion part Proposed method 2.1, Thus, the multimodal alignment model and the multimodal fusion model can be the same) Proposed method 1.2 (not mentioned in the confirmation report) : design negative rewards based on language information (e.g., agent\u0026rsquo;s behaviour meets some constraints stated in the walkthrough) Motivation: Having more informative reward signal may better help the agent to learn. However, this requires further annotation work. Include data augmentation and pre-trained model (See Section 3.1.4 and Section 3.1.5 in confirmation report)\nMotivation: generating more training data which will help the deep learning model to converge Motivation2: transfer knowledge from other domain will also help the model to converge faster. Early fusion part\nHow to achieve a condensed, robust and accurate multimodal representation from multiple unimodal data?\nRe-design Model Architecture\nChange Attention based fusion model to lightweight fusion model (See Section 3.1.3 in confirmation report) Motivation: Attention based fusion model is data-hungry, and we need to avoid that due to lack of annotated training data Modify the training process\nProposed method 2.1 (not mentioned in the confirmation report) : title: pre-train the fusion model using masked auto encoding learning (i.e., learn to recover missing data), the fusion model will become the encoder in this setting. Aim: we can obtain a multimodal representation vector by the autoencoder learning in order to increase the robustness, we can gradually mask out portions of the input The autoencoder learning can also train the agent to deduce and induce (which is related to handling abstraction levels of language information (Chapter C), which is not our focus for now) Motivation: this will link back to Proposed method 1.1 . Having this pre-training of the multimodal representation will help to reduce the burden on the policy network. Policy network part\nHow to effectively and efficiently train a policy network that can correctly handle the varieties of incoming observations and reward signals so as to output a sequence of actions that solves the task.\nRe-design Model Architecture\nOption MDP (See Section 3.5 in confirmation report) Motivation: Option MDP may fit the language instruction info quite well Memory module (See Section 3.5 in confirmation report) Motivation: memory module helps to integrate agent’s observations over time. Therefore, we also plan to construct an extra episodic memory module so as to deal with alignment between long language instruction and long trajectories. Modify the training process\nHindsight Experience Relabelling (See Section 3.3 in confirmation report) HER require to translate agent behaviour into natural language descriptions, which we can use Connectionist Temporal Classification (See Section 3.2 in confirmation report) Motivation: this helps to train the policy even if the training data is imbalanced (i.e., mainly bad demonstrations) Some thoughts about learning based agent\u0026rsquo;s natural language understanding ability Statement: the agent will not obtain the natural language understanding ability if the language information only affect the reward signal during training.\nJustification: If the natural language only influence the rewards signal, then in the perspective of agent\u0026rsquo;s policy network, it will only think that the environment is starting to provide more frequent reward signal and the policy network itself has nothing to do with the natural language information.\nTherefore, if we want to train agent\u0026rsquo;s natural language understanding ability, we have to apply the early fusion setting\nIn fact, for a learning agent, we can apply both reward shaping and early fusion simultaneously, and this can be achieved once we apply proposed method 1.1 (title: treat unimodal data as the projection of the corresponding event onto that modality\u0026rsquo;s feature space) and proposed method 2.1 (title: pre-train the fusion model using masked auto encoding learning) because we can use one module to represent both multimodal fusion module and reward shaping module.\nBut why we want to train the natural language understanding ability for learning based agent? this will be answered in the thesis Chapter B The preliminary ideas can be found here.\n","permalink":"https://sino-huang.github.io/posts/supplementary-notes-for-mindmap-aug-2022/","summary":"\u003cp\u003eYou need password to access to the content, go to Slack *#phdsukai to find more.\u003c/p\u003e","title":"Supplementary explanations for proposed methods and PhD thesis structure"},{"content":"[TOC]\nTitle: Masked World Models for Visual Control 2022 Author: Younggyo Seo et. al. Publish Year: 2022 Review Date: Fri, Jul 1, 2022 https://arxiv.org/abs/2206.14244?context=cs.AI\nhttps://sites.google.com/view/mwm-rl\nSummary of paper Motivation TL:DR: Masked autoencoders (MAE) has emerged as a scalable and effective self-supervised learning technique. Can MAE be also effective for visual model-based RL? Yes! with the recipe of convolutional feature masking and reward prediction to capture fine-grained and task-relevant information.\nSome key terms Decouple visual representation learning and dynamics learning\ncompared to DreamerV2, the new model try to decouple visual learning and dynamics learning in model based RL.\nspecifically, for visual learning, we train a autoencoder with convolutional layers and vision transformer (ViT) to reconstruct pixels given masked convolutional features. we also introduce auxiliary reward prediction objective for the observation autoencoder for model based RL part, we learn a latent dynamics model that operates on the representation from the autoencoder. Early convolutional layers and masking out convolutional features instead of pixel patches\nthis approach enables the world model to capture fine-grained visual details from complex visual observations. we compare convolutional feature masking with pixel masking (i.e., MAE), which shows that convolutional feature masking significantly outperforms pixel masking. some possible reasons: raw pixel is noisy and less focused conv features is more robust Potential future work We can use this model on our project\nConv feature masking Dynamics learning ","permalink":"https://sino-huang.github.io/posts/younggyo_seo-masked-world-models-for-visual-control-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle:  Masked World Models for Visual Control 2022\u003c/li\u003e\n\u003cli\u003eAuthor: Younggyo Seo et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Fri, Jul 1, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2206.14244?context=cs.AI\"\u003ehttps://arxiv.org/abs/2206.14244?context=cs.AI\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://sites.google.com/view/mwm-rl\"\u003ehttps://sites.google.com/view/mwm-rl\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://lh3.googleusercontent.com/owoi-mhzvc1Jb8T_jENqF3jzsCYlizdoTPCJQYp0cNmv6AM5nZWqPUi2juwMuDYJrZ4Z6Pnsi5TF7J56GvL6CEyJTZF5AQBqSw-1njMf4Jy9El-Uck_iscK1PU1Y5gC_1w=w1280\"\u003e\u003c/p\u003e\n\u003cp\u003eTL:DR: \u003ca href=\"https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2111.06377\u0026amp;sa=D\u0026amp;sntz=1\u0026amp;usg=AOvVaw1IDxCsGFfXTiNsfRJw8iat\"\u003eMasked autoencoders (MAE)\u003c/a\u003e has emerged as a scalable and effective self-supervised learning technique. Can MAE be also effective for visual model-based RL? Yes! with the recipe of convolutional feature masking and reward prediction to capture fine-grained and task-relevant information.\u003c/p\u003e\n\u003ch3 id=\"some-key-terms\"\u003eSome key terms\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eDecouple visual representation learning and dynamics learning\u003c/strong\u003e\u003c/p\u003e","title":"Younggyo_seo Masked World Models for Visual Control 2022"},{"content":"[TOC]\nTitle: Prioritised Experience Replay Author: Neuralnet.ai Publish Year: 25 Feb, 2016 Review Date: Thu, Jun 2, 2022 https://www.neuralnet.ai/a-brief-overview-of-rank-based-prioritized-experience-replay/\nReplay memory is essential in RL Replay memory has been successfully deployed in both value based and policy gradient based reinforcement learning algorithms, to great success. The reasons for this success cut right to the heart of reinforcement learning. In particular, replay memory simultaneously solves two outstanding problems with the field.\nwe shuffle the dataset and sample historic experience at random, we can obtain independent and uncorrelated inputs, which is important for deep neural network training. This is precisely what underpins the Markov property of the system. (In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process.) we revisited and make attention to the historic experience with the hope that the agent learns something generalisable. Improvement Direction we can improve on how we sample the agent\u0026rsquo;s memories. The default is to simply sample them at random, which works, but leaves much to be desired.\nInput aliasing to be improved One issue that can be improved upon is that neural networks introduce a sort of aliasing into the problem.\nImages that may be completely distinct from a human perspective could very well turn out to be nearly identical to the neural network. (i.e., the semantic meaning is completely different for a little bit changes in visuals). And this is a sort of aliasing of the input So the question become \u0026ldquo;would the agent learn more from sampling totally distinct experiences\u0026rdquo;\npossible solution: introduce the idea of priority. We can assign some sort of priority to our memories, and then sample them according to that priority. A natural candidate for this priority is the temporal difference error. so the idea is: we prioritise learning on the things we don\u0026rsquo;t understand $$ \\delta_t = r_t + \\gamma Q_{target}(S_{t+1}, argmax_a Q(S_{t+1}, a)) - Q(S_t, a_t) $$\ndrawback: this only really holds in the case that the rewards from the environment aren’t particularly noisy. For \u0026ldquo;hard lessons\u0026rdquo;, these TD errors shrink slowly over time. This means that the agent is bound to sample the same memories over and over, which leads to overfitting. ","permalink":"https://sino-huang.github.io/posts/a-brief-overview-of-rank-based-prioritized-experience-replay-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Prioritised Experience Replay\u003c/li\u003e\n\u003cli\u003eAuthor: Neuralnet.ai\u003c/li\u003e\n\u003cli\u003ePublish Year:  25 Feb, 2016\u003c/li\u003e\n\u003cli\u003eReview Date: Thu, Jun 2, 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://www.neuralnet.ai/a-brief-overview-of-rank-based-prioritized-experience-replay/\"\u003ehttps://www.neuralnet.ai/a-brief-overview-of-rank-based-prioritized-experience-replay/\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"replay-memory-is-essential-in-rl\"\u003eReplay memory is essential in RL\u003c/h2\u003e\n\u003cp\u003eReplay memory has been successfully deployed in both value based and  policy gradient based reinforcement learning algorithms, to great success. The reasons for this success cut right to the heart of  reinforcement learning. In particular, replay memory simultaneously solves two outstanding problems with the field.\u003c/p\u003e","title":"A Brief Overview of Rank Based Prioritized Experience Replay 2016"},{"content":"[TOC]\nTitle: Flamingo: a Visual Language Model for Few-Shot Learning Author: Jean-Baptiste Alayrac et. al. Publish Year: Apr 2022 Review Date: May 2022 Summary of paper Flamingo architecture Pretrained vision encoder: from pixels to features\nthe model\u0026rsquo;s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)\nthey pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper \u0026ldquo;Learning Transferable Visual Models From Natural Language Supervision\u0026rdquo;\nHaving contrastive objective means that the visual encoder model is already stay in the same latent space of the BERT language model.\nThe output of the final stage is a 2D spatial grid of feature $X_f$, later 2D feature will be flattened to 1D\ntrainable Perceiver Resampler: from varying-size large feature maps to few visual tokens.\nthe Perceiver Resampler module connects the vision encoder to the frozen language model\ninput: a variable number of image features\noutput: a fixed number of visual outputs (in practical 64)\nmotivation: significantly reduce the computational complexity of vision text cross attention. (particularly important when dealing with multiple long videos)\nwhat do they need: in order to get the fixed number of output, we need to have a fixed number of query tokens\nso, we learn a predefined number of latent input queries. these latent queries are fed to a transformer stack and cross attend to the flattened visual features $X_f$\nThe keys and values computed from the learnt latents are concatenated to the keys and values obtained from $X_f$, which we found to perform slightly better.\nfrozen language model\nthe pretrained text-only model is a decoder-only model.\nbut in order to let the frozen language model fit the current situation, they introduced a gated xatten-dense layer\nthey also apply layer normalisation to the keys, values and queries\nmulti-image attention\nThey limit the number of visual tokens that a certain text token sees.\nTypically, they allow each token to attend to the tokens of the image that appeared just before it in the interleaved sequence. (this means we have temporal matching when we do cross attention between images and text)\nAlthough the model can only directly attend to a single image at any given point, there is still a causal dependency on all previous images in the sequence via causal self-attention in the text decoder.\nHow to train the model\u0026rsquo;s parameters They train the models by minimizing a weighted sum of dataset specific expected negative log likelihood of text given some visual inputs.\nIn practice, at each step of optimisation we go over each dataset D𝑚 in turn, sample a batch of size 𝐵𝑚 of visual language sequences from it, compute the gradient of the loss with respect to the minibatch and weight it by 𝜆𝑚. We then accumulate the gradients over all 𝑀 datasets before triggering an update step. We found this gradient accumulation strategy to be crucial for high performance compared to a round-robin approach. (how to deal with multiple training dataset)\nbut how to flow the gradient given that we need to freeze some modules\nactually if we set requires_grad = False, everything still works ok\nhttps://discuss.pytorch.org/t/will-freezing-an-intermediate-block-influence-the-gradient-flow/88859/4\nPotential future work yes, we can use this model for any multi-modal tasks.\n","permalink":"https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Flamingo: a Visual Language Model for Few-Shot Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Jean-Baptiste Alayrac et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Apr 2022\u003c/li\u003e\n\u003cli\u003eReview Date: May 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"flamingo-architecture\"\u003eFlamingo architecture\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003ePretrained\u003c/strong\u003e \u003cstrong\u003evision encoder: from pixels to features\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ethe model\u0026rsquo;s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)\u003c/p\u003e\n\u003cp\u003ethey pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper \u0026ldquo;Learning Transferable Visual Models From Natural Language Supervision\u0026rdquo;\u003c/p\u003e","title":"Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022"},{"content":"[TOC]\nTitle: Augmenting Transformers with KNN-based composite memory for dialog Author: Angela Fan et. al. Publish Year: 2021 Review Date: Apr 2022 Summary of paper Motivation The author proposed augmenting generative Transformer neural network with KNN based Information Fetching module\nEach KIF module learns a read operation to access fix external knowledge (e.g., WIKI)\nThe author demonstrated the effectiveness of this approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images and human-written dialog utterances.\ndrawback of previous work\nmany existing approaches focus on using attention over the memory slots, which is computationally intensive and becomes less effective as the size of the memory grows.\nAdvantages of KNN read operation\nIn the proposed method, KNN search is computationally efficient and scalable.\nwe can thus scale easily to larger memories by learning only the KNN-based read operation to identify relevant information from the memory\nThe procedure\nfirst find the k nearest elements to f_E(M\u0026rsquo;(x_i)) in M(E), based on KNN search with inner product, concretely, f is a multiplayer perceptron with ReLU activator. when we get the relevant elements e_j identified by KIF, we re-encode it by M' since e_j and x_i may have variable length, we average across the length dimension to produce a fixed sided representation to conduct KNN the external knowledge elements {e_j, e_j+1\u0026hellip;} are weighted by their normalised KNN score and then summed. Subsequently, we concatenate M\u0026rsquo;(x_i) to form the final encoder output finally, different sources of information may not be required for every prediction and some information sources E can be more important than others. To allow the model to make more fine-grained decisions about what information to use from what source and how much of it, we add a gating mechanism using a sigmoid (0-1) function around each weighted sum of KNN representations KIF_1, KIF_2, from E_1, E_2 so the final output is Some key terms dialog modelling\nthis is a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversations.\ntraining after stabilised\nthe model also try to pass backpropagation gradient to encoding module after certain training steps.\nif we directly pass the gradient at the early stage. the noisy loss will mess up the pre-trained encoder module. to solve this, first of all, we separate M encoder into two parts. part 1 M is to fixed and encode the external knowledge part 2 M\u0026rsquo; encode the dialog text, which is also dynamic and get updated every iteration they learn a mapping operator f_E(M\u0026rsquo;(x_i)) that trains to map elements of the model\u0026rsquo;s representation of X, into additional information representation space M(E) so f learns representations of an output close to the corresponding projection of X into E. This can be interpreted as learning a read operation on a fixed external memory. as the model changes significantly during training process, the nonlinear mapping capability of f_E(M\u0026rsquo;(x_I)) is essential to be able to identify the correct knowledge E from input X. Minor comments library faiss allow KNN to easily used on GPUs\nPotential future work good to try for NetHack environment\n","permalink":"https://sino-huang.github.io/posts/angela_fan-augmenting-transformer-with-knn-composite-memory-for-dialog-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Augmenting Transformers with KNN-based composite memory for dialog\u003c/li\u003e\n\u003cli\u003eAuthor: Angela Fan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Apr 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eThe author proposed augmenting generative Transformer neural network with KNN based Information Fetching module\u003c/p\u003e\n\u003cp\u003eEach KIF module learns a read operation to access fix external knowledge (e.g., WIKI)\u003c/p\u003e\n\u003cp\u003eThe author demonstrated the effectiveness of this approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images and human-written dialog utterances.\u003c/p\u003e","title":"Angela_fan Augmenting Transformer With Knn Composite Memory for Dialog 2021"},{"content":"[TOC]\nTitle: Generalisable episodic memory for Deep Reinforcement Learning Author: Hao Hu et. al. Publish Year: Jun 2021 Review Date: April 2022 Summary of paper Motivation The author proposed Generalisable Episodic Memory (GEM), which effectively organises the state-action values of episodic memory in a generalisable manner and supports implicit planning on memorised trajectories.\nso compared to traditional memory table, GEM learns a virtual memory table memorized by deep neural networks to aggregate similar state-action pairs that essentially have the same nature.\nGEM is able to do implicit planning by performing value propagation along trajectories saved in the memory and calculating the best sequence over all possible real and counterfactual combinatorial trajectories.\nSome key terms Traditional discrete model-free episodic control\nThe key idea if to store good past experience in a tabular based non parametric memory and rapidly latch onto past successful policies when encountering similar states, instead of waiting for many steps of optimization.\nWhen different experiences meet up at the same state-action pair (s,a), model-free episodic control aggregates the values of different trajectories by taking the maximum return R among all these rollouts starting from the moment (s, a)\nAt the execution time, we select the action according to the maximum Q-value of the current state, if there is no exact match of the state, MFEC performs a KNN loop up to estimate the state-action Q values\nGeneralisable episodic memory\nSomehow it is just like critic network to measure the Q value of the state action pair.\nMajor comments Maybe this paper just want to say that for Actor Critic model, the Critic network that estimate the Q value can be treated as Generalisable Episodic Memory table.\n","permalink":"https://sino-huang.github.io/posts/hao_hu-generalisable-episodic-memory-for-drl-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Generalisable episodic memory for Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Hao Hu et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Jun 2021\u003c/li\u003e\n\u003cli\u003eReview Date: April 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eThe author proposed Generalisable Episodic Memory (GEM), which effectively organises the state-action values of episodic memory in a generalisable manner and supports implicit planning on memorised trajectories.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eso compared to traditional memory table\u003c/strong\u003e, GEM learns a virtual memory table memorized by deep neural networks to aggregate similar state-action pairs that essentially have the same nature.\u003c/p\u003e","title":"Hao_hu Generalisable Episodic Memory for Drl 2021"},{"content":"[TOC]\nTitle: Offline Reinforcement Learning with Implicit Q-learning Author:Ilya Kostrikov et. al. Publish Year: 2021 Review Date: Mar 2022 Summary of paper Motivation conflict in offline reinforcement learning\noffline reinforcement learning requires reconciling two conflicting aims:\nlearning a policy that improves over the behaviour policy (old policy) that collected the dataset while at the same time minimizing the deviation from the behaviour policy so as to avoid errors due to distributional shift (e.g., obtain out of distribution actions) -\u0026gt; the challenge is how to constrain those unseen actions to be in-distribution. (meaning there is no explicit Q-function for actions, and thus the issue of unseen action is gone) all the previous solutions like 1. limit how far the new policy deviates from the behaviour policy and 2. assign low value to out of distribution actions impose a trade-off between how much the policy improve and how vulnerable it is to misestimation due to distributional shift.\nSo, what we want is to never query or estimate values for actions that were not seen in the data.\nlimitation of \u0026ldquo;single-step\u0026rdquo; approach:\nsingle-step means they either use no value function at all, or lean the value function of the behaviour policy.\nand this methods perform very poorly on more complex datasets that require combining parts of suboptimal trajectories.\nexpectile regression\ntheir aim is not to estimate the distribution of values that results from stochastic transitions, but rather estimate expectiles of the state value function with respect to random actions.\nso this aim is not to determine how Q-value can vary with different future outcomes, but how the Q-value can vary with different actions while averaging together future outcomes due to stochastic dynamics.\nwhat is expectile regression\nwhile quantile regression can be seen as a generalisation of median regression, expectile as alternative are a generalised form of mean regression.\nanyway, expectile regression is a generalisation version of MSE\nwhy out of distribution action is bad for offline RL\nout of distribution action a\u0026rsquo; can produce erroneous values for Q_theta(s\u0026rsquo;, a\u0026rsquo;) in the temporal different error objective, often leading to overestimation as the policy is defined to maximise the (estimated) Q-value.\nSARSA style optimal Q function\n$Q_\\hat \\theta$ is the target network (not trainable, controlled by behaviour policy)\n$\\pi_\\beta$ is the behaviour policy\nand this avoid issues with out-of-distribution actions because $\\pi_\\beta$ does not event choose out-of distribution actions.\nHow to improve, just do not consider out of distribution action when calculating Q values\nMoreover, apply expectile regression objective\nrequire additional policy extraction step\nwhile this modified TD learning procedure learns an approximation to the optimal Q-function, it does not explicitly represent the corresponding policy, and therefore requires a separate policy step\nas before, we aim to avoid using out of samples actions. therefore, we extract the policy using advantage weighted regression\nfinal algorithm\nNote that the policy does not influence the value function in any way, and therefore extraction could be performed either concurrently or after TD learning. Concurrent learning provides a way to use IQL with online fine-tuning.\nSome key terms distributional shift\ndistributional shift refers to the situation where training data distribution is not the same as the testing data distribution.\nQ function\nQ(s,a) is a measure of the overall expected reward assuming the Agent is in state s and perform action a, and then continues playing until the end of the episode following some policy pi\nSARSA vs Q-learning\nSARSA is more conservative than Q learning\nQ learning vs Dynamic programming\nDynamic programming creates optimal policies based on an already given model of its environment. Opposed to that is Q-Learning. It creates policies solely based on the rewards it receives by interacting with its environment.\nContribution the algorithm is computationally efficient: can perform 1M updates on one GTX1080 GPU in less than 20 minutes. simple to implement, requiring only minor modifications over a standard SARSA-like TD algorithm, and performing policy extraction with a simple weighted behaviour cloning procedure resembling supervised learning. ","permalink":"https://sino-huang.github.io/posts/ilya_kostrikov-offline-rl-with-implicit-q-learning-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Offline Reinforcement Learning with Implicit Q-learning\u003c/li\u003e\n\u003cli\u003eAuthor:Ilya Kostrikov et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003econflict in offline reinforcement learning\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eoffline reinforcement learning requires reconciling two conflicting aims:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003elearning a policy that improves over the behaviour policy (old policy) that collected the dataset\u003c/li\u003e\n\u003cli\u003ewhile at the same time minimizing the deviation from the behaviour policy so as to avoid errors due to distributional shift (e.g., obtain out of distribution actions) -\u0026gt; the challenge is how to constrain those unseen actions to be in-distribution. (meaning there is no explicit Q-function for actions, and thus the issue of unseen action is gone)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eall the previous solutions like 1. limit how far the new policy deviates from the behaviour policy and 2. assign low value to out of distribution actions impose a trade-off between how much the policy improve and how vulnerable it is to misestimation due to distributional shift.\u003c/p\u003e","title":"Ilya_kostrikov Offline Rl With Implicit Q Learning 2021"},{"content":"[TOC]\nTitle: Online Decision Transformer Author: Qinqing Zheng Publish Year: Feb 2022 Review Date: Mar 2022 Summary of paper Motivation the author proposed online Decision transformer (ODT), an RL algorithm based on sequence modelling that blends offline pretraining with online fine-tuning in a unified framework.\nODT builds on the decision transformer architecture previously introduced for offline RL\nquantify exploration\ncompared to DT, they shifted from deterministic to stochastic policies for defining exploration objectives during the online phase. They quantify exploration via the entropy of the policy similar to max-ent RL frameworks.\nbehaviour cloning term\nadding a behaviour cloning term to offline RL methods allows the porting of off-policy RL algorithm to the offline setting with minimal changes.\noffline learning and online fine-tuning\nthe policy is extracted via a behaviour cloning step that avoid of out of distribution actions.\nsome improvements on the offline-online settings\nLee et al. (2021) tackles the offline-online setting with a balanced replay scheme and an ensemble of Q functions to maintain conservatism during offline training. Lu et al. (2021) improves upon AWAC (Nair et al., 2020), which exhibits collapse during the online fine tuning stage, by incorporating positive sampling and exploration during the online stage. the author claimed that positive sampling and exploration are naturally embedded in ODT method why offline trajectories has limitations\noffline trajectories might not have high return and cover only a limited part of the state space.\nmodifications from decision transformer\nlearn a stochastic policy (a Gaussian multivariate distribution with a diagonal covariance matrix to model the action distribution conditioned on states and RTGs) quantify exploration via the policy entropy Algorithm\nSome key terms offline RL\nan agent is trained to autoregressively maximize the likelihood of trajectories in the offline dataset.\npolicies learned via offline RL are limited by the quality of the training dataset and need to be finetuned to the task of interest via online interactions.\ntransformer for RL\nit focuses on predictive modelling of action sequences conditioned on a task specification (target goal or returns) as opposed to explicitly learning Q-functions or policy gradients.\noff-policy vs on-policy vs offline reinforcement learning\nthe process of reinforcement learning involves iteratively collecting data by interacting with the environment. this data is also referred as experiences.\nall these methods fundamentally differ in how this data (collection of experiences) is generated\nOn-policy RL\ntypically the experience are collected using the latest learned policy, and then using that experience to improve the policy. the policy pi_k is updated with data collected by pi_k itself example: SARSA, PPO, TRPO Off-policy RL\nin the classical off-policy setting, the agent\u0026rsquo;s experience is appended to a data buffer (also called replay buffer) and each policy pi_k collects additional data, such that the replay buffer is composed of sample from pi_0, pi_1,\u0026hellip; to pi_k, and all of this data is used to train an updated new policy pi_k+1. Offline RL\noffline RL: those utilise previously collected data, without additional online data collection. bootstrap method\nThe bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\noff-policy bootstrapping error accumulation\nhttps://arxiv.org/pdf/1906.00949.pdf\nOff-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution (out of distribution actions), and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, the author study the setting where the off-policy experience is fixed and there is no further interaction with the environment. the author identified bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator.\nreturn to go (RTG)\nreturn to go of a trajectory $\\tau$ at timestep t,\nis the sum of future reward from that timestep.\n","permalink":"https://sino-huang.github.io/posts/qinqing_zheng-online-decision-transformer-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Online Decision Transformer\u003c/li\u003e\n\u003cli\u003eAuthor: Qinqing Zheng\u003c/li\u003e\n\u003cli\u003ePublish Year: Feb 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003ethe author proposed online Decision transformer (ODT), an RL algorithm based on sequence modelling that blends offline pretraining with online fine-tuning in a unified framework.\u003c/p\u003e\n\u003cp\u003eODT builds on the decision transformer architecture previously introduced for offline RL\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003equantify exploration\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ecompared to DT, they shifted from deterministic to stochastic policies for defining exploration objectives during the online phase. They quantify exploration via the \u003cstrong\u003eentropy\u003c/strong\u003e of the policy similar to max-ent RL frameworks.\u003c/p\u003e","title":"Qinqing_zheng Online Decision Transformer 2022"},{"content":"[TOC]\nTitle: Improving language models by retrieving from trillions of tokens Author: Sebastian Borgeaud et. al. Publish Year: Feb 2022 Review Date: Mar 2022 Summary of paper Motivation in order to decrease the size of language model, this work suggested retrieval from a large text database as a complementary path to scaling language models.\nthey equip models with the ability to directly access a large dataset to perform prediction \u0026ndash; a semi-parametric approach.\nhow do they do\nfirst, construct a key-value database, where values store raw chunks of text tokens and key are frozen BERT embeddings.\nthen we use a frozen model (not trainable) to avoid having to periodically re-compute embeddings over the entire database during training. second, each training sequence input is split into chunks, which are augmented with their k-nearest neighbours retrieved from the database.\ne.g., so a chunk C1 will have several value neighbours from the database. finally, a encoder-decoder architecture integrates retrieval chunks into the models\u0026rsquo;s predictions.\ncheck the CCA architecture, this preserves autoregressivity, the later token depends on the previous tokens. FFW is the fully connected layer ATTN is self attention module Algorithm\nSome key terms retrieval database\na key-value database\nwhere values store raw chunks of text tokens and key are frozen BERT embeddings query keys two main approaches are matching words in the query against the database index (keyword searching) and traversing the database using hypertext or hypermedia links in this work, the value of the database is some information sentences Potential future work looks like we do have such a very large database\nalso the database input and output are both text sequences, which may not be useful for language assisted RL\n","permalink":"https://sino-huang.github.io/posts/sebastian_borgeaud-improving-language-models-by-retrieving-from-trillions-of-tokens-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Improving language models by retrieving from trillions of tokens\u003c/li\u003e\n\u003cli\u003eAuthor: Sebastian Borgeaud et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Feb 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003ein order to decrease the size of language model, this work suggested retrieval from a large text database as a complementary path to scaling language models.\u003c/p\u003e\n\u003cp\u003ethey equip models with the ability to directly access a large dataset to perform prediction \u0026ndash; a \u003cstrong\u003esemi-parametric\u003c/strong\u003e approach.\u003c/p\u003e","title":"Sebastian_borgeaud Improving Language Models by Retrieving From Trillions of Tokens 2022"},{"content":"[TOC]\nTitle: Can Wikipedia Help Offline Reinforcement Learning Author: Machel Reid et. al. Publish Year: Mar 2022 Review Date: Mar 2022 Summary of paper Motivation Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments.\nMoreover, when the model is trained from scratch, it suffers from slow convergence speeds\nIn this paper, they look to take advantage of this formulation of reinforcement learning as sequence modelling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when fine tuned on offline RL tasks (control, games).\nHow do they do\nencouraging similarity between language representations and offline RL input representations\nthey add one term in the objective named L_cos\nthis objective wants that each input representation should at least be corresponded to one word. (this is wired\u0026hellip;)\nbut they said that they tested mean pooling and they found out that the model cannot converge.\nI is the input (either reward, action or state), E is the word embedding\nSome key terms the zero-shot performance of transformer based language models\noffline RL and sequence modelling\noffline reinforcement learning (RL) has been seen as analogous to sequence modelling, framed as simply supervised learning to fit return-augmented trajectories in an offline dataset.\noffline RL model\nthis paper wants to adapt pre-trained language model (from Wikipedia) to offline RL (in continuous control and games)\noffline reinforcement learning\nin offline RL, the objective remains the same, but has to be optimised with no interactive data collection on a fixed set of trajectory $\\tau_i$ $$ \\tau = (r_1,s_1,a_1,r_2,s_2,a_2,\u0026hellip;,r_N,s_N,a_N) $$\nGood things about the paper (one paragraph) Major comments Minor comments decision transformer\nIn the Abstract it said \u0026ldquo;recent work has looked at tackling offline RL from the perspective of sequence modelling with improved results as result of the introduction of the Transformer architecture\u0026rdquo;\nIncomprehension I don\u0026rsquo;t why this is a good way to encourage similarity between language representations and offline RL input representations.\nPotential future work Is there a better way to encourage RL input embedding and word embeddings stay in the same latent space?\n","permalink":"https://sino-huang.github.io/posts/machel_reid-can-wikipedia-help-offline-rl-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Can Wikipedia Help Offline Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Machel Reid et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Mar 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eFine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments.\u003c/p\u003e\n\u003cp\u003eMoreover, when the model is trained from scratch, it suffers from slow convergence speeds\u003c/p\u003e\n\u003cp\u003eIn this paper, they look to take advantage of this formulation of reinforcement learning as \u003cstrong\u003esequence modelling\u003c/strong\u003e and investigate the transferability of pre-trained sequence models on other domains (vision, language) when fine tuned on offline RL tasks (control,  games).\u003c/p\u003e","title":"Machel_reid Can Wikipedia Help Offline Rl 2022"},{"content":"[TOC]\nTitle: Generalised Domain Model Acquisition from Action Traces (LOCM2) Author: Stephen Cresswell et. al. Publish Year: 2013 Review Date: Mar 2022 Summary of paper Motivation One approach to the problem of formulating domain models for planning is to learn the models from example action sequences.\nThis work extended LOCM by allowing multiple parameterised state machine to represent a single object.\nIn other words, it is possible to automatically infer the underlying transition system from sample action sequences of the domain. Using such an approach removes the necessity for the domain expert to also be an expert at modelling transition systems.\nimprovement from LOCM to LOCM2\nSpecifically, they allowed the separate aspects of an object\u0026rsquo;s behaviour to be represented by separate state machines.\nwhat is the essential strategy for LOCM2\nThe strategy is to include a state machine with all of the transitions for the sort (as in LOCM), but to additionally check for holes.\nGiven that holes indicate behaviour missed in the LOCM analysis, further analysis is performed with the aim of selecting valid subsets\nSome key terms holes\nThey refer to a transition pair not observed in the example sequence, but filled in by generalisation (unification) process.\nwell formed transition matrix definition\nsort\nrefers to a collection of states of one object\ne.g.,\nIncomprehension need to read LOCM paper to understand the whole thing.\n","permalink":"https://sino-huang.github.io/posts/stephen_cresswell-generalised-domain-model-acquisition-from-action-traces-2013/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Generalised Domain Model Acquisition from Action Traces (LOCM2)\u003c/li\u003e\n\u003cli\u003eAuthor: Stephen Cresswell et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2013\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eOne approach to the problem of formulating domain models for planning is to learn the models from example action sequences.\u003c/p\u003e\n\u003cp\u003eThis work extended LOCM by allowing multiple parameterised state machine to represent a single object.\u003c/p\u003e\n\u003cp\u003eIn other words, it is possible to automatically infer the underlying transition system from \u003cem\u003esample action sequences of the domain\u003c/em\u003e. Using such an approach removes the necessity for the domain expert to also be an expert at modelling transition systems.\u003c/p\u003e","title":"Stephen_cresswell Generalised Domain Model Acquisition From Action Traces 2013"},{"content":"[TOC]\nTitle: Extracting Action Sequences from Texts Based on Deep Reinforcement Learning Author: Wenfeng Feng et. al. Publish Year: Mar 2018 Review Date: Mar 2022 Summary of paper Motivation the author want to build a model that learns to directly extract action sequences without external tools like POS tagging and dependency parsing results\u0026hellip;\nAnnotation dataset structure\nexample\nModel\nthey exploit the framework to learn two models to predict action names and arguments respectively.\nWhy this approach can be treated as Reinforcement Learning problem\nInitially we have word vector and operations pairs with operations all being NULL as starting point.\nwhy the author wants to stick this method to RL schema\u0026hellip;\nRewards signals\nbasic rewards signal additional rewards signal this is used as momentum and buffer based on the overview of the training data issue: we don\u0026rsquo;t have item rate for testing texts because we don\u0026rsquo;t have labels. Potential future work When we extend to unseen corpus, we may use word association to assist to extract action and action arguments.\n","permalink":"https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Extracting Action Sequences from Texts Based on Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Wenfeng Feng et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Mar 2018\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003ethe author want to build a model that learns to directly extract action sequences without external tools like POS tagging and dependency parsing results\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAnnotation dataset structure\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220315161910319\" loading=\"lazy\" src=\"/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/image-assets/image-20220315161910319.png\"\u003e\u003c/p\u003e\n\u003cp\u003eexample\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220315162057150\" loading=\"lazy\" src=\"/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/image-assets/image-20220315162057150.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ethey exploit the framework to learn two models to predict action names and arguments respectively.\u003c/p\u003e","title":"Wenfeng_feng Extracting Action Sequences From Texts by Rl"},{"content":"[TOC]\nTitle: NLtoPDDL: One-Shot Learning of PDDL Models from Natural Language Process Manuals Author: Shivam Miglani et. al. Publish Year: 2020 Review Date: Mar 2022 Summary of paper Motivation pipeline\nPipeline architecture\nPhase 1 we have a DQN that learns to extract words that represent action name, action arguments, and the sequence of actions present in annotated NL process manuals. (why only action name, do we need to extract other information???) Again, why this is called DQN RL? is it just normal supervised learning\u0026hellip; (Check EASDRL paper to understand Phase 1)\nPhase 2 we extract the action sequences by feeding the unseen process manual to Phase 1\u0026rsquo;s trained DQN. From this extracted sequences,\nLOCM2 algorithm learns a partial PDDL model in one shot the model can be completed with human input in an interactive fashion they named it as interactive-LOCM (iLOCM) EASDRL How it works\nIt represents action sequences extract as a RL problem and uses two DQNs. Each DQN can perform only RL actions: select or reject a word.\nthe first DQN only select action words but the second DQN will based on the action words to select arguments words\nsome pre-processing methods\nthe author replaced pronouns with nouns in the unseen process manual to reduce ambiguity using a neural based technique using Huggingface library (neuralcoref https://github.com/huggingface/neuralcoref)\nSome key terms LOCM2 what it is\nthis method tried to cluster similar looking actions and arguments under one template, resulting in higher PDDL models with precise action sets\nsome limitations for the proposed model Since the DQNs were trained to extract single words, the learned model extracts multiple words for the same argument and also misses out on implicit arguments.\ne.g., The learned model for tea domain assigned correct duration to the actions but missed out preconditions and effects related to \u0026ldquo;mug\u0026rdquo; argument and static \u0026ldquo;hand\u0026rdquo; argument. Also extracting words with adjectives such as \u0026ldquo;cold milk\u0026rdquo; or compound nouns such as \u0026ldquo;training personnel\u0026rdquo; would be better (??!!! word association!)\nMinor comments Flair library for word embeddings might be useful\nhttps://github.com/flairNLP/flair\nAnnotated dataset\nFor the training of the DRL models, the pre-annotated datasets are taken from Feng, Zhuo, and Kambhampati’s repository1for three real-world domains\nMicrosoft Windows Help and Support (WinHelp) documents CookingTutorial (Cooking) WikiHow Home and Garden (WikiHG) Some examples for Phase 1 action extraction\nLearned state machine via iLOCM\nPotential future work I see, there is no special method to find arguments or adjectives\nlet\u0026rsquo;s see if we can use some word association technique to do that so as to enhance the better argument score.\n","permalink":"https://sino-huang.github.io/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: NLtoPDDL: One-Shot Learning of PDDL Models from Natural Language Process Manuals\u003c/li\u003e\n\u003cli\u003eAuthor: Shivam Miglani et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003epipeline\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220314153211927\" loading=\"lazy\" src=\"/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/image-assets/image-20220314153211927.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePipeline architecture\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220314165337096\" loading=\"lazy\" src=\"/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/image-assets/image-20220314165337096.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePhase 1\u003c/strong\u003e we have a DQN that learns to extract words that represent action name, action arguments, and the sequence of actions present in annotated NL process manuals. (why only action name, do we need to extract other information???) Again, why this is called DQN RL? is it just normal supervised learning\u0026hellip;  (Check EASDRL paper to understand Phase 1)\u003c/p\u003e","title":"Shivam_miglani Nltopddl Learning From Nlp Manuals 2020"},{"content":"[TOC]\nTitle: Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf Restraining Specification Author: Giuseppe De Giacomo et. al. Publish Year: 2019 Review Date: Mar 2022 Summary of paper The author investigated the concept of \u0026ldquo;restraining bolt\u0026rdquo; that can control the behaviour of learning agents.\nEssentially, the way to control a RL agent is that the bolt provides additional rewards to the agent\nAlthough this method is essentially the same as reward shaping (providing additional rewards to the agent), the contribution of this paper is\nprovide theoretical support for combining MDP and Linear Temporal Logic provide a deterministic finite state automata to provide rewards signals from LTL. Some key terms Restraining Bolts\nRestraining bolts were small, cylindrical devices that could be affixed to a droid in order to limit its functions and enforce its obedience.\ntwo distinct representation of the world one for the agent, by the designer of the agent one for the restraining bolt, by the authority imposing the bolt. are these to representation related to each other? NO: the agent designer and the authority imposing the bolt are not aligned (why should they!) YES: the agent and the bolt act in the same real world. so the glue between two representation is the world itself. But can restraining bolt exist at all? YES: for example based on RL The learning agents and the restraining bolts are very decoupled.\nYou can put a specific restraining bolt to the agent and then this agent can play new games (because now the behaviour of the agent is changed).\nMinor comments check this ICAPS 2019 video about this paper\nhttps://www.youtube.com/watch?v=qGLRxfKD40s\n[^]:\n","permalink":"https://sino-huang.github.io/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf Restraining Specification\u003c/li\u003e\n\u003cli\u003eAuthor: Giuseppe De Giacomo et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2019\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe author investigated the concept of \u0026ldquo;restraining bolt\u0026rdquo; that can control the behaviour of learning agents.\u003c/p\u003e\n\u003cp\u003eEssentially, the way to control a RL agent is that the bolt provides additional rewards to the agent\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220309181427110\" loading=\"lazy\" src=\"/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/image-assets/image-20220309181427110.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAlthough this method is essentially the same as reward shaping (providing additional rewards to the agent), the contribution of this paper is\u003c/p\u003e","title":"Giuseppe_de_giacomo Foundations for Retraining Bolts Rl With Ltl 2019"},{"content":"please modify the following\n[TOC]\nTitle: Collaborative Planning with Encoding of Users\u0026rsquo; High-level Strategies Author: Joseph Kim et. al. Publish Year: 2017 Review Date: Mar 2022 Summary of paper Motivation Automatic planning is computationally expensive. Greedy search heuristics often yield low-quality plans that can result in wasted resources; also, even in the event that an adequate plan is generated, users may have difficulty interpreting the reason why the plan performs well and trusting it.\nSuch challenges motivate the development of a form of collaborative planning, where instead of treating the planner as \u0026ldquo;black box\u0026rdquo;, users are actively involved in the plan generation process.\nhow to let humans to encode LTL operators, since LTL operators are hard to be handled by laymen\nHowever, the encoding of user-provided high-level strategies for scheduling problems is an underexplored area.\nSome key terms Preferences\nPDDL 3.0\nWhile soft goals are defined at the final state, soft trajectory constraints assert that conditions that must be met by the entire sequence of states visited during plan execution.\ntrajectory constraints\nsoft trajectory constraints assert that conditions that must be met by the entire sequence of states visited during plan execution.\nThese constraints are expressed using temporal modal operators over first-order logic involving state variables.\nBasic operators include always, sometimes, at-most-once, at-end\nPrecedence relationships are set using sometime-after, sometime-before, imply\nOperators such as within, hold-after, hold-during represent deadlines with lower and upper bounds.\ne.g.,\nUnlike hard constraints\npreferences cannot be directly used to prune the search space; however, they can serve as forward search heuristics wherein several techniques use related planning graphs and sum the layers containing preference facts. These are then used to estimate goal distances and preference satis-faction potential.\nCollaborative planning procedure\nManually encoding\nThe user-provided strategies were manually encoded as preferences by two experimenters.\n\u0026hellip; ok, this is different from the annotation tool method. We must have domain experts to annotate \u0026hellip;.\nWell, we can automate the process later\u0026hellip;\nExamples of strategies and their preference encod-ings\n","permalink":"https://sino-huang.github.io/posts/joseph_kim-collaborative-planning-with-encoding-of-high-level-strategies-2017/","summary":"\u003cp\u003eplease modify the following\u003c/p\u003e\n\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Collaborative Planning with Encoding of Users\u0026rsquo; High-level Strategies\u003c/li\u003e\n\u003cli\u003eAuthor: Joseph Kim et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eAutomatic planning is computationally expensive. Greedy search heuristics often yield low-quality plans that can result in wasted resources; also, even in the event that an adequate plan is generated, users may have difficulty interpreting the reason why the plan performs well and trusting it.\u003c/p\u003e","title":"Joseph_kim Collaborative Planning With Encoding of High Level Strategies 2017"},{"content":"[TOC]\nTitle: MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research Author: Mikayel Samvelyan et. al. Publish Year: Nov 2021 Review Date: Mar 2022 Summary of paper They presented MiniHack, an easy-to-use framework for creating rich and varied RL environments, as well as a suite of tasks developed using this framework. Built upon NLE and the des-file format, MiniHack enables the use of rich entities and dynamics from the game of NetHack to create a large variety of RL environments for targeted experimentation, while also allowing painless scaling-up of the difficulty of existing environments. MiniHack’s environments are procedurally generated by default, ensuring the evaluation of systematic generalization of RL agents.\nLimitation of existing RL benchmark benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. lack the ability to test specific components or open problems of RL methods in well-controlled proof-of-concept test cases Systematically extending such environments and gradually dropping simplifying assumptions can require arduous engineering and excessive time commitment, while opting for more challenging benchmarks des-file The des-file format is a domain-specific language created by the developers of NetHack for describing the levels of the game. des-files are human-readable specifications of levels: distributions of grid layouts together with monsters, objects on the floor, environment features (e.g.walls, water, lava), etc.\nPython operation we can also use python code to construct the environment\nTypes of tasks Navigation tasks\nMiniHack’s navigation tasks challenge the agent to reach the goal position by overcoming various difficulties on their way, such as fighting monsters in corridors, crossing a river by pushing boulders into it, navigating through complex or procedurally generated mazes.\nSkill Acquisition Tasks\nThe nature of commands in NetHack requires the agent to perform a sequence of actions so that the initial action, which is meant for interaction with an object, has an effect. The exact sequence of subsequent can be inferred by the in-game message bar prompts.\nFor example, when located in the same grid with an apple lying on the floor, choosing the Eat action will not be enough for the agentto eat it. In this case, the message bar will ask the following question: \u0026ldquo;There is an apple here; eat it? [y n q] (n)\u0026rdquo;. Choosing the Y action at the next time step will cause the initial EAT action to take effect, and the agent will eat the apple. Choosing the N action (or MORE action since N is the default choice) will decline the previous EAT action prompt. The rest of the actions will not progress the in-game timer and the agent will stay in the same state. We refer to this skill as Confirmation.\n","permalink":"https://sino-huang.github.io/posts/mikayel_samvelyan-minihack-the-planet-a-sandbox-for-open-ended-rl-research-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research\u003c/li\u003e\n\u003cli\u003eAuthor: Mikayel Samvelyan et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Nov 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThey presented MiniHack, an easy-to-use framework for creating rich and varied RL environments, as well as a suite of tasks developed using this framework. Built upon NLE and the \u003ccode\u003edes-file\u003c/code\u003e format, MiniHack enables the use of rich entities and dynamics from the game of NetHack to create a large variety of RL environments for targeted experimentation, while also allowing painless scaling-up of the difficulty of existing environments.  MiniHack’s environments are procedurally generated by default, ensuring the evaluation of systematic generalization of RL agents.\u003c/p\u003e","title":"Mikayel_samvelyan Minihack the Planet a Sandbox for Open Ended Rl Research 2021"},{"content":"[TOC]\nTitle: Constrained Language models yield few-shot semantic parsers Author: Richard Shin et. al. Publish Year: Nov 2021 Review Date: Mar 2022 Summary of paper Motivation The author wanted to explore the use of large pretrained language models as few-shot semantic parsers\nHowever, language models are trained to generate natural language. To bridge the gap, they used language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. (using synchronous context-free grammar SCFG)\nSuch a grammar maps between canonical natural language forms and domain-specific meaning representations, so that a separate LM-based system can focus entirely on mapping an unconstrained utterance $u$ to a canonical (but still natural) form $c$\nSome key terms Synchronous context-free grammar (SCFG)\nSCFG for Semantic Parsing\n","permalink":"https://sino-huang.github.io/posts/richard_shin-constrained-language-models-yield-few-shot-semantic-parsers-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Constrained Language models yield few-shot semantic parsers\u003c/li\u003e\n\u003cli\u003eAuthor: Richard Shin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Nov 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eThe author wanted to explore the use of large pretrained language models as few-shot semantic parsers\u003c/p\u003e\n\u003cp\u003eHowever, language models are trained to generate natural language. To bridge the gap, they used language models to paraphrase inputs into a \u003cem\u003e\u003cstrong\u003econtrolled\u003c/strong\u003e\u003c/em\u003e sublanguage resembling English that can be automatically mapped to a target meaning representation. (using synchronous context-free grammar SCFG)\u003c/p\u003e","title":"Richard_shin Constrained Language Models Yield Few Shot Semantic Parsers 2021"},{"content":"[TOC]\nTitle: The NetHack Learning Environment Author: Heinrich Kuttler et. al. Publish Year: Dec 2020 Review Date: Mar 2022 Summary of paper The NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack.\nNetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience.\nLimitations of existing environment and methods\nGo-Explore, overfits to specific properties of ALE and Montezuma\u0026rsquo;s Revenge. While Go-Explore is an impressive solution for Montezuma\u0026rsquo;s Revenge, it exploits the determinism of environment transitions, allowing it to memorize sequences of actions that lead to previously visited states from which the agent can continue to explore\nNetHack, however, can surpass the limits of deterministic or repetitive settings because its environment is procedurally generated.\nMany aspects of the game are procedurally generated and follow stochastic dynamics.\nThe procedurally generated content of each level makes it highly unlikely that a player will ever experience the exact same situation more than once.\nLong game and sparse rewards\nNetHack is an extremely long game. Successful expert episodes usually last tens of thousands of turns, while average successful runs can easily last hundreds of thousands of turns, spawning multiple days of play-time. Compared to testbeds with long episode horizons such as StarCraft and Dota 2, NetHack’s “episodes” are one or two orders of magnitude longer, and they wildly vary depending on the policy.\nNetHack and Natural Language resources\nNetHack’s popularity has attracted a larger number of contributors to its community. Consequently, there exists a comprehensive game wiki and many so-called spoilers that provide advice to players. Due to the randomized nature of NetHack, this advice is general in nature (e.g., explaining the behavior of various entities) and not a step-by-step guide. These texts could be used for language-assisted RL. Lastly, there is also a large public repository of human replay data (over five million games) hosted on the NetHack Alt.org (NAO) servers, with hundreds of finished games per day on average. This extensive dataset could spur research advances in imitation learning, inverse RL, and learning from demonstrations.\nIn addition, the extensive documentation about NetHack can enable research on using prior (natural language) knowledge for learning, which could lead to improvements in generalization and sample efficiency\nTasks\nTo demonstrate that NetHack is a suitable testbed for advancing RL, they have a set of initial tasks for tractable subgoals in the game:\nnavigating to a staircase down to the next level, navigating to a staircase while being accompanied by a pet, locating and eating edibles, collecting gold, maximizing in-game score, scouting to discover unseen parts of the dungeon, and finding the oracle. ","permalink":"https://sino-huang.github.io/posts/heinrich_kuttler-the-nethack-learning-environment-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: The NetHack Learning Environment\u003c/li\u003e\n\u003cli\u003eAuthor: Heinrich Kuttler et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Dec 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Mar 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack.\u003c/p\u003e\n\u003cp\u003eNetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience.\u003c/p\u003e","title":"Heinrich_kuttler the Nethack Learning Environment 2020"},{"content":"please modify the following\n[TOC]\nTitle: LTL2Action: Generalizing LTL Instructions for Multi-Task RL Author: Pashootan Vaezipoor et. al. Publish Year: 2021 Review Date: March 2022 Summary of paper Motivation they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language \u0026ndash; linear temporal logic (LTL)\nLimitation of the vanilla MDP\ntemporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards.\nContribution\nproposed a novel approach for teaching RL agents to follow LTL instructions that has theoretical advantages over other methods encode LTL instruction via a neural architecture equipped with LTL progression (GNN) use environment-agnostic LTL pre-training From LTL Instructions to Rewards\nwe have a labelling function L(s,a) assigns truth values to the propositions in P given the current state s (or state history) of the environment and the action $a \\in A$ (also known as event detector)\nFormally, given an LTL instruction $\\varphi$ over $P$ and a labelling function $L : S \\times A \\rarr 2^P$\nit just means that if all the trajectory follows the instruction, then we can have such reward\nSome key terms Multitask learning\nin order to instruct RL using language, the first step is to agree upon a common vocabulary between different tasks.\nPropositional symbols\npropositional logic -\u0026gt; deals with propositions (which can be true or false) and relations between propositions,\nproposition -\u0026gt; is the meaning of a declarative sentence. \u0026ldquo;meaning\u0026rdquo; is understood to be a non-linguistic entity which is shared by all sentences with the same meaning.\nLinear Temporal Logic\ndefinition of formula: formula, is a finite sequence of symbols from a given alphabet that is part of a formal language\nthe unary connectives have higher precedence than the binary connectives\nA until B means A will hold continuously until B comes up and A will stop to hold\nExample\n\u0026ldquo;infinitely often\u0026rdquo; means it happens multiple times\nLift Example\nFormal semantics\nMinigrid example\nLet\u0026rsquo;s say that the set of propositions $P$ includes $R, G$ and $B$, which are true iff the agent is standing on red/green/blue square (respectively) in the current time step\nHow to analyse the LTL formula and how to make the progress Markovian again\nwhat I mean is to check if LTL formula is satisfied or not\nsolution: LTL progression\nwe can see the progression will decompose the instructions gradually.\ni.e., Progress towards completion of the task is reflected in diminished remaining instructions.\nall in all, as we consume the instruction $\\varphi$ , we can then neglect the trajectory and make the policy Markovian\ncheck the transition distribution definition\nMinor comments check LTL introduction videos\nhttps://www.youtube.com/watch?v=a9fo3dUly8A\u0026list=PLMBx8HjvK7672qEl6bdnXdzYEbLP_lWPw\nPotential future work I can see that LTL is also care about the low level instructions rather than strategies.\n","permalink":"https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/","summary":"\u003cp\u003eplease modify the following\u003c/p\u003e\n\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: LTL2Action: Generalizing LTL Instructions for Multi-Task RL\u003c/li\u003e\n\u003cli\u003eAuthor: Pashootan Vaezipoor et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: March 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003ethey addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language \u0026ndash; linear temporal logic (LTL)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLimitation of the vanilla MDP\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003etemporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards.\u003c/p\u003e","title":"Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021"},{"content":"[TOC]\nTitle: Learning to Ground Language to Temporal Logical Form Author: Roma Patel et. al. Publish Year: 2019 Review Date: Feb 2022 Summary of paper Motivation natural language commands often exhibits sequential (temporal) constraints e.g., \u0026ldquo;go through the kitchen and then into the living room\u0026rdquo;.\nBut this constraints cannot be expressed in the reward of Markov Decision Process setting. (see this paper)\nTherefore, they proposed to ground language to Linear Temporal logic (LTL) and after that continue to map from LTL expressions to action sequences.\nWeak supervision\nThe author believed that annotating natural language into logical forms is expensive. Instead, they would collect many demonstrations (action sequences) performed by the human players who followed the language instructions. After that,\nthey build a generative model that generate logical forms candidates from language inputs filter out invalid candidates by having a LTL Program checker Planning: DQN with LTL\nto handle temporal constraints, the author used a non-Markovian reward decision process that incorporates the LTL expression into the environment MDP. the reward function is defined over state history (otherwise the constraints cannot be expressed)\nSome key terms Linear Temporal Logic\nMajor comments We can see that LTL can only describe the temporal (causal) constraints, how can we express other constraints from the natural instructions (spatial constraints (maybe something related to the observation analysis), action constraints etc)\nIf we consider the instruction information as constraints, then we should segment it into different types of constraints\u0026hellip; (check the book)\nPotential future work maybe we can combine this work (especially the LTL checker) with the annotation work Guided K-best Selection for Semantic Parsing Annotation\n","permalink":"https://sino-huang.github.io/posts/roma_patel-learning-to-ground-language-temporal-logical-form-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Learning to Ground Language to Temporal Logical Form\u003c/li\u003e\n\u003cli\u003eAuthor: Roma Patel et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2019\u003c/li\u003e\n\u003cli\u003eReview Date: Feb 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003enatural language commands often exhibits sequential (temporal) constraints e.g., \u0026ldquo;go through the kitchen and then into the living room\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eBut this constraints cannot be expressed in the reward of Markov Decision Process setting. (see \u003ca href=\"https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/\"\u003ethis paper\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eTherefore, they proposed to ground language to Linear Temporal logic (LTL) and after that continue to map from LTL expressions to action sequences.\u003c/p\u003e","title":"Roma_patel Learning to Ground Language Temporal Logical Form 2019"},{"content":"[TOC]\nTitle: Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks? Author: Thang M. Pham Publish Year: Jul 2021 Review Date: Feb 2022 Summary of paper The author found out that BERT-based models trained on GLUE have low sensitivity to word orders.\nThe research questions are the following\nDo BERT-based models trained on GLUE care about the order of words in a sentence? ANS: NO, except one task named CoLA, which is to detecting grammatically incorrect sentences. Surprisingly, for the rest of the 5 out of 6 binary-classification tasks (i.e. except CoLA), between75% and 90% of the originally correct predictions remain constant after 1-grams are randomly re-ordered Are SOTA BERT-based models using word order information when solving NLU tasks? If not, what cues do they rely on? ANS: they heavily rely on the word itself rather than the ordering. The results showed that if the top - 1 most important word measured by LIME has a positive meaning, then there is 100% probability that the sentence\u0026rsquo;s label is \u0026ldquo;positive\u0026rdquo; Results\nSome key terms STS-B (Cer et al.,2017)\na regression task of predicting the semantic similarity of two sentences\nRandom shuffling methods\nthe author experimented with three tests: randomly shuffling n-grams where n = {1,2,3}\n(Sukai\u0026rsquo;s opinion: randomly shuffling looks weird, can we shuffle based on the syntax so that the grammar is still correct while the semantic meaning is different?)\nLIME\ncomputes a score between [−1,1] for each token in the input denoting how much its presence contributes for or against the network’s predicted label. The importance score per word w is intuitively the mean confidence-score drop over a set of randomly-masked versions of the input when w is masked out.\nPotential future work The vanilla BERT model trained by GLUE is not sensitive to the word ordering and thus may not be very effective in solving the causality in the sentence. Thus it may not be very helpful to directly conduct end to end natural utterance to action sequence mapping for our project.\n","permalink":"https://sino-huang.github.io/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?\u003c/li\u003e\n\u003cli\u003eAuthor: Thang M. Pham\u003c/li\u003e\n\u003cli\u003ePublish Year: Jul 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Feb 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe author found out that BERT-based models trained on GLUE have low sensitivity to word orders.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220228205000220\" loading=\"lazy\" src=\"/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/image-assets/image-20220228205000220.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe research questions are the following\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDo BERT-based models trained on GLUE care about the order of words in a sentence?\n\u003cul\u003e\n\u003cli\u003eANS: NO, except one task named CoLA, which is to detecting grammatically incorrect sentences. Surprisingly, for the rest of the 5 out of 6 binary-classification tasks (i.e.  except CoLA), between75% and 90% of the originally correct predictions remain constant after  1-grams are randomly re-ordered\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAre SOTA BERT-based models using word order information when solving NLU tasks? If not, what cues do they rely on?\n\u003cul\u003e\n\u003cli\u003eANS: they heavily rely on the word itself rather than the ordering. The results showed that if the top - 1 most important word measured by LIME has a positive meaning, then there is 100% probability that the sentence\u0026rsquo;s label is \u0026ldquo;positive\u0026rdquo;\u003c/li\u003e\n\u003cli\u003e\u003cimg alt=\"image-20220228212916959\" loading=\"lazy\" src=\"/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/image-assets/image-20220228212916959.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eResults\u003c/strong\u003e\u003c/p\u003e","title":"Thang_m_pham Out of Order How Important Is the Sequential Order of Words in a Sentence in Natural Language Understanding Tasks 2021"},{"content":"[TOC]\nTitle: Guided K-best Selection for Semantic Parsing Annotation Author: Anton Belyy et. al. Publish Year: 2021 Review Date: Feb 2022 Summary of paper Motivation They wanted to tackle the challenge of efficient data collection (data annotation) for the conversational semantic parsing task.\nIn the presence of little available training data, they proposed human-in-the-loop interfaces for guided K-best selection, using a prototype model trained on limited data.\nResult Their user studies showed that the keyword searching function combined with a keyword suggestion method strikes the balance between annotation accuracy and speed\nWhat is K-best selection annotation\nThe annotator wanted to annotate the natural utterance into canonical utterance (so that the canonical utterance can be translated into logical forms using synchronise Context-Free Grammar (SCFG) method)\nA pretrained small model will provide the K-best candidates\nWhat are the limitations\nannotation speed as K grows larger, an annotator needs to spend more time reading the candidate list. Annotation accuracy early plausible candidates in the rank-list may bias interpretation; an annotator may commit early to less than perfect result without exploring further. (i.e., lack of diversity later) What is the proposed solution\nGuided K-best selection interface\nSearch-Keyword (C) interface\nthis interface will show a list of top 5 discriminative key-words\nthese keywords are used to narrow down the current candidates\nafter that, users can choose if the keyword should be included or excluded in the correct parse, if it is not included, the pretrained prototyping model will recalculate the candidates sentences.\nHow do we obtain the discriminative keywords\nthey developed a keyword suggestion method inspired by post-decoding clustering (PDC) from (Ippolito et al., 2019) in a way that we first of all use the prototyping model to generate K candidates (K is large) we cluster the K candidates into k clusters, with diverse meanings, and select the k best candidates, one from each cluster. This distills the original K candidates into fewer but more diverse candidates, where k \u0026laquo; K furthermore, they employ a cluster explanation technique proposed by Dasgupta et al. (2020) further distill the k diverse candidates into k\u0026rsquo; keywords keywords are the nodes in the threshold tree since the features in the nodes may be repeated and reused across different branches of the binary tree, k\u0026rsquo; \u0026lt; k \u0026laquo; K Some key terms prototyping model\nthe prototyping model is trained to output canonical utterances from natural utterances. but the training data is limited\nthe prototyping model needs to output a enumeration of candidates ranked by model score.\nconversational semantic parsing and canonical utterances\nthe use of canonical utterance formulates semantic parsing as a paraphrasing task that paraphrases a natural utterance to a controlled language.\nA synchronous context-free grammar (SCFG) defines a mapping between task-specific meaning representations and their corresponding controlled languages.\nThat is to say, using such an SCFG, a complicated meaning representation can be presented as a human-readable canonical utterance (more similar to natural language) so models can focus on learning how to paraphrase a natural utterance to a canonical utterance.\nAnd the human annotators no longer need to learn the syntax of the meaning representation.\nPotential future work The use of canonical utterance and such guided K-best selection annotation interface is really userful for our project.\nFor canonical utterance and SCFG, please check this paper https://arxiv.org/pdf/2104.08768.pdf\nFor training dialogue dataset, we can check SMCalFlow dataset\n","permalink":"https://sino-huang.github.io/posts/anton_belyy-guided-k-best-selection-for-semantic-parsing-annotation-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Guided K-best Selection for Semantic Parsing Annotation\u003c/li\u003e\n\u003cli\u003eAuthor: Anton Belyy et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Feb 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eThey wanted to tackle the challenge of efficient data collection (data annotation) for the conversational semantic parsing task.\u003c/p\u003e\n\u003cp\u003eIn the presence of little available training data, they proposed human-in-the-loop interfaces for guided K-best selection, using a prototype model trained on limited data.\u003c/p\u003e\n\u003ch3 id=\"result\"\u003eResult\u003c/h3\u003e\n\u003cp\u003eTheir user studies showed that the keyword searching function combined with a keyword suggestion method strikes the balance between annotation accuracy and speed\u003c/p\u003e","title":"Anton_belyy Guided K Best Selection for Semantic Parsing Annotation 2021"},{"content":"[TOC]\nTitle: Argumentative Zoning Author: Simone Teufel Publish Year: 2000 Review Date: Feb 2022 https://www.cl.cam.ac.uk/~sht25/az.html\nSummary Abstract We present a new type of analysis for scientific text which we call Argumentative Zoning.\nWe demonstrate that this type of text analysis can be used for generating user-tailored and task-tailored summaries and for performing more informative citation analyses.\nWe also demonstrate that our type of analysis can be applied to unrestricted text, both automatically and by humans. The corpus we use for the analysis (80 conference papers in computational linguistics) is a difficult test bed; it shows great variation with respect to subdomain, writing style, register and linguistic expression. We present reliability studies which we performed on this corpus and for which we use two unrelated trained annotators.\nThe definition of our seven categories (argumentative zones) is not specific to the domain, only to the text type; it is based on the typical argumentation to be found in scientific articles. It reflects the attribution of intellectual ownership in scientific articles, expressions of authors\u0026rsquo; stance towards other work, and typical statements about problem-solving processes.\nOn the basis of sentential features, we use two statistical models (a Naive Bayesian model and an ngram model operating over sentences) to estimate a sentence\u0026rsquo;s argumentative status, taking the hand-annotated corpus as training material. An alternative, symbolic system uses the features in a rule-based way.\nDefinition The definition of the argumentative Zones is given by the sentential-rhetorical speech act of single, important sentences\n(landmark sentences, e.g. \u0026ldquo;in this paper we develop a method for\u0026rdquo; or \u0026ldquo;in contrast to REFERENCE, our approach uses \u0026hellip;\u0026rdquo;). The zones, each associated with one or more sentences, are the following:\nBKG: General scientific background (yellow) OTH: Neutral descriptions of other people\u0026rsquo;s work (orange) OWN: Neutral descriptions of the own, new work (blue) AIM: Statements of the particular aim of the current paper (pink) TXT: Statements of textual organization of the current paper (in chapter 1, we introduce\u0026hellip;) (red) CTR: Contrastive or comparative statements about other work; explicit mention of weaknesses of other work (green) BAS: Statements that own work is based on other work (purple) Example Aim The ultimate aim of this work is to provide an intelligent library search tool for researchers. This could include the summarization of single or multiple articles, and also improved citation indexes. A construct called a \u0026ldquo;citation maps\u0026rdquo; could help people grasp relationships between papers, like this the following graphics showing citation relationships between some articles in my test corpus:\n","permalink":"https://sino-huang.github.io/posts/s_teufel-argumentative-zoning-2000/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Argumentative Zoning\u003c/li\u003e\n\u003cli\u003eAuthor: Simone Teufel\u003c/li\u003e\n\u003cli\u003ePublish Year: 2000\u003c/li\u003e\n\u003cli\u003eReview Date: Feb 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://www.cl.cam.ac.uk/~sht25/az.html\"\u003ehttps://www.cl.cam.ac.uk/~sht25/az.html\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003ch3 id=\"abstract\"\u003eAbstract\u003c/h3\u003e\n\u003cp\u003eWe present a new type of analysis for scientific text which we call \u003cstrong\u003eArgumentative Zoning\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eWe demonstrate that this type of text analysis can be used for generating user-tailored and task-tailored summaries and for performing more informative citation analyses.\u003c/p\u003e\n\u003cp\u003eWe also demonstrate that our type of analysis can be applied to unrestricted text, both automatically and by humans. The corpus we use for the analysis (80 conference papers in computational linguistics) is a difficult test bed; it shows great variation with respect to subdomain, writing style, register and linguistic expression. We present reliability studies which we performed on this corpus and for which we use two unrelated trained annotators.\u003c/p\u003e","title":"S_teufel Argumentative Zoning 2000"},{"content":"[TOC]\nTitle: Compositionality as Lexical Symmetry Author: Ekin Akyurek; Jacob Andreas Publish Year: Jan 2022 Review Date: Feb 2022 Summary of paper Motivation Standard deep network models lack the inductive bias needed to generalize compositionally in tasks like semantic parsing, translation, and question answering.\nSo, a large body of work in NLP seeks to overcome this limitation with new model architectures that enforce a compositional process of sentence interpretation.\nGoal\nthe question of this paper aims to answer is whether compositionality like other domain-specific constraints, can be formalized in the language of symmetry.\nLanguage symmetry\nthey characterise the relationship between compositionality and symmetry for general problem domains.\nthere comes a statement\nif a interpretation process can be computed compositionally e.g., query \u0026ldquo;How many yellow object in the scene\u0026rdquo; -\u0026gt; can be decomposed into interpreting \u0026ldquo;colour\u0026rdquo; and \u0026ldquo;objects\u0026rdquo; the data examples exhibits identifiable symmetries Homomorphism of symbol\nProposition\nif X is L-compositional, f is an L-homomorphism and x $\\in$ X, then f(x) will also $\\in$ X, Thus, based on the definition of symmetry, every homomorphism of L corresponds to a symmetry of X\nAlignment of text data and other-modal data pairs\nSome key terms parameter typing schemes\nTYPE is a high level module type (attention, classification, etc.)\nSymmetry property\nLexicon\nLexicon abstraction\nGood things about the paper (one paragraph) The author noticed that adding constraints on deep learning model architecture in order to operationalize the principle of compositionality is not effective compared to vanilla transformers.\nSo, the author tried to implement a data-centric view of the principle of compositionality, which means that we focus on the \u0026ldquo;transformation of a structural regularity of the lexicon\u0026rdquo; of training data so that we can capture the principle of compositionality.\nMajor comments Essentially, this work gives a theoretical support for the use of data augmentation to improve generalisation performance of models.\n","permalink":"https://sino-huang.github.io/posts/jacob_andreas-compositionality-as-lexical-symmetry-2022/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Compositionality as Lexical Symmetry\u003c/li\u003e\n\u003cli\u003eAuthor: Ekin Akyurek; Jacob Andreas\u003c/li\u003e\n\u003cli\u003ePublish Year: Jan 2022\u003c/li\u003e\n\u003cli\u003eReview Date: Feb 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003cp\u003eStandard deep network models lack the inductive bias needed to generalize compositionally in tasks like semantic parsing, translation, and question answering.\u003c/p\u003e\n\u003cp\u003eSo, a large body of work in NLP seeks to overcome this limitation with new model architectures that enforce a compositional process of sentence interpretation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGoal\u003c/strong\u003e\u003c/p\u003e","title":"Jacob_andreas Compositionality as Lexical Symmetry 2022"},{"content":"[TOC]\nTitle: When Attention Meets Fast Recurrence: Training Language Models with Reduce Compute Author: Tao Lei Publish Year: Sep 2021 Review Date: Jan 2022 Summary of paper As the author mentioned, the inspiration of SRU++ comes from two lines of research:\nparalleization / speed problem of Original RNN leveraging recurrence in conjunction with self-attention Structure of SRU++\nNew discovery :little attention is needed given recurrence.\nSimilar to the observation of Merity (2019), they found using a couple of attention layers sufficient to obtain SOTA results.\nWhere to use attention\nPutting the attention to higher layer closer to the output will get better result.\ne.g., for a 10-layer model, use attention at 7th and 10th layer.\n","permalink":"https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: When Attention Meets Fast Recurrence: Training Language Models with Reduce Compute\u003c/li\u003e\n\u003cli\u003eAuthor: Tao Lei\u003c/li\u003e\n\u003cli\u003ePublish Year: Sep 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Jan 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220114003204904\" loading=\"lazy\" src=\"/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114003204904.png\"\u003e\u003c/p\u003e\n\u003cp\u003eAs the author mentioned, the inspiration of SRU++ comes from two lines of research:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eparalleization / speed problem of Original RNN\n\u003cul\u003e\n\u003cli\u003e\u003cimg alt=\"image-20220114004410134\" loading=\"lazy\" src=\"/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114004410134.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eleveraging recurrence in conjunction with self-attention\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eStructure of SRU++\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220114005227231\" loading=\"lazy\" src=\"/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114005227231.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNew discovery :little attention is needed given recurrence.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to the observation of \u003ca href=\"https://arxiv.org/abs/1911.11423\"\u003eMerity (2019)\u003c/a\u003e, they found using a couple of attention layers sufficient to obtain SOTA results.\u003c/p\u003e","title":"Tao_lei When Attention Meets Fast Recurrence Training Language Models With Reduced Compute 2021"},{"content":"[TOC]\nTitle: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models Author: Alex Nichol et. al. Publish Year: Dec 2021 Review Date: Jan 2022 Summary of paper In author\u0026rsquo;s previous work, the diffusion model can achieve photorealism in the class-conditional setting by augmenting with classifier guidance, a technique which allows diffusion models to condition on a classifier\u0026rsquo;s labels.\nThe classifier is first trained on noised images, and during the diffusion sampling process, gradients from the classifier are used to guide the output sample towards the label. classifier details\nonce you have class label, I add additional probability distribution in the formula to guide the model Algorithm of previous work (classifier guided sampling)\nSo this work extend label classifier to Natural language\nby encoding texts into embeddings and feed into each diffusion layer Some key terms diffusion model\ndiffusion models sample from a distribution by reversing a gradual noising process\nsampling starts with noise x_T and produce gradually less noisy samples x_T-1, x_T-2, .. until reach a final sample x_0. each timestep t corresponds to a certain noise level. and x_t can be thought of as a mixture of a signal x_0 with some noise e where the signal to noise ratio is determined by timestep t. a diffusion model learns to produce a slightly more \u0026ldquo;denoised\u0026rdquo; x_t-1 from x_t Theory behind diffusion model\nwe can add noise again and again and eventually we can get random noise image then we try to invert it. Improvements\nRather than model the denoised image\u0026rsquo;s mean, it predict the noise itself. fix the covariance of the gaussian distribution is good enough further more, learning a interpolation covariance parameter is even better than fixed one give different weight to gradient at different step t (the first few steps are much more important) Good things about the paper (one paragraph) Major comments Minor comments Check the paper reading video:\nhttps://www.youtube.com/watch?v=W-O7AZNzbzQ https://www.youtube.com/watch?v=gwI6g1pBD84 Incomprehension Potential future work \u0026ldquo;text to image\u0026rdquo; task has some similarity with \u0026ldquo;text to goal state/desired action\u0026rdquo; task for Text-based RL environment.\ndiffusion model and denoising process share some similarity with the \u0026ldquo;training with masking\u0026rdquo; idea.\n","permalink":"https://sino-huang.github.io/posts/alex_nichol-glide-towards-photorealistic-image-generation-and-editing-with-text-guided-diffusion-models-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\u003c/li\u003e\n\u003cli\u003eAuthor: Alex Nichol et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Dec 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Jan 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eIn author\u0026rsquo;s \u003ca href=\"https://arxiv.org/pdf/2105.05233.pdf\"\u003eprevious work\u003c/a\u003e, the diffusion model can achieve photorealism in the class-conditional setting by augmenting with \u003cem\u003eclassifier guidance\u003c/em\u003e, a technique which allows diffusion models to condition on a classifier\u0026rsquo;s labels.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe classifier is first trained on noised images, and during the diffusion sampling process, gradients from the classifier are used to guide the output sample towards the label.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eclassifier details\u003c/strong\u003e\u003c/p\u003e","title":"Alex_nichol Glide Towards Photorealistic Image Generation and Editing With Text Guided Diffusion Models 2021"},{"content":"[TOC]\nTitle: M6: A Chinese Multimodal Pretrainer Author: Junyang Lin et. al. Publish Year: May 2021 Review Date: Jan 2022 Summary of paper This paper re-emphasises that\nlarge model trained on big data have extremely large capacity and it can outperform the SOTA in downstream tasks especially in the zero-shot setting. So, the author trained a big multi-modal model\nAlso, they proposed a innovative way to tackle downstream tasks.\nthey use masks to block cross attention between tokens so as to fit different types of downstream task Key idea: mask tokens during cross attention so as to solve certain tasks Overview\nPotential future work Masking might be a good practice for multi-modal model.\n","permalink":"https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: M6: A Chinese Multimodal Pretrainer\u003c/li\u003e\n\u003cli\u003eAuthor: Junyang Lin et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: May 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Jan 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThis paper re-emphasises that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elarge model trained on big data have extremely large capacity and it can outperform the SOTA in downstream tasks especially in the zero-shot setting.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSo, the author trained a big multi-modal model\u003c/p\u003e\n\u003cp\u003eAlso, they proposed a innovative way to tackle downstream tasks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethey use masks to block cross attention between tokens so as to fit different types of downstream task\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKey idea: mask tokens during cross attention so as to solve certain tasks\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eOverview\u003c/strong\u003e\u003c/p\u003e","title":"Junyang_lin M6 a Chinese Multimodal Pretrainer 2021"},{"content":"[TOC]\nTitle: BABYAI++: Towards Grounded-Language Learning Beyond Memorization Author: Tianshi Cao et. al. Publish Year: 2020 ICLR Review Date: Jan 2022 Summary of paper The paper introduced a new RL environment BabyAI++ that can investigate whether RL agents can extract knowledge from descriptive text and eventually increase generalisation performance.\nBabyAI++ environment example\nthe descriptive text describe the feature of the object. notice that the feature of object can easily change as we change the descriptive text. Model\nthey studied three architecture for processing descriptive text and scene representation into a combined visual+text embedding\nconcat-fusion FiLM attention fusion their attention fusion model obtained the best result\nContribution\nTheir results shows that attention based text-image grounded language learning allows the agents to extract knowledge from descriptive text and obtain better generalisation performance.\n","permalink":"https://sino-huang.github.io/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: BABYAI++: Towards Grounded-Language Learning Beyond Memorization\u003c/li\u003e\n\u003cli\u003eAuthor: Tianshi Cao et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020 ICLR\u003c/li\u003e\n\u003cli\u003eReview Date: Jan 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe paper introduced a new RL environment BabyAI++ that can investigate whether RL agents can extract knowledge from descriptive text and eventually increase generalisation performance.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBabyAI++ environment example\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20220104181838965\" loading=\"lazy\" src=\"/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/image-assets/image-20220104181838965.png\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe descriptive text describe the feature of the object.\n\u003cul\u003e\n\u003cli\u003enotice that the feature of object can easily change as we change the descriptive text.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eModel\u003c/strong\u003e\u003c/p\u003e","title":"Tianshi_cao Babyai Plus Plus Towards Grounded Language Learning Beyond Memorization 2020"},{"content":"[TOC]\nTitle: Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction Author: Federico Bianchi Publish Year: 2021 Review Date: Jan 2022 Summary of paper the author investigated grounded language learning through the natural interaction between users and the shopping website search engine.\nHow they do it\nconvert the shopping object dataset into a Latent Grounded Domain\nrelated products end up closer in the embedding space train the mapping model (mapping from text query to a portion of product space) based on the user click behaviour (In the training dataset, the users queries about \u0026ldquo;Nike\u0026rdquo; and the they would click relevant Nike Product)\nThis approach can be seen as a neural generalisation of model-theoretic semantics, where the ex-tension of “shoes” is not a discrete set of objects, but a region in the grounding space. train the functional composition model\n(looks like) this compositional model is designed for solving zero-shot learning problems this is a model that input two DeepSet embedding and output another DeepSet embedding e.g., \u0026ldquo;Nike\u0026rdquo; + \u0026ldquo;Shoes\u0026rdquo; = \u0026ldquo;Nike Shoes\u0026rdquo; Example\nThe contributions are the following\nconverting into a grounded latent domain allows for better generalisation performance Comments about the paper (one paragraph) The user behaviour dataset is very valuable. but they are not open sourced any more\n","permalink":"https://sino-huang.github.io/posts/federico_bianchi-language-in-a-search-box-grounding-language-learning-in-real-world-human-machine-interaction-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction\u003c/li\u003e\n\u003cli\u003eAuthor: Federico Bianchi\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Jan 2022\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003ethe author investigated grounded language learning through the natural interaction between users and the shopping website search engine.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow they do it\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003econvert the shopping object dataset into a Latent Grounded Domain\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cimg alt=\"image-20220103212726450\" loading=\"lazy\" src=\"/posts/federico_bianchi-language-in-a-search-box-grounding-language-learning-in-real-world-human-machine-interaction-2021/image-assets/image-20220103212726450.png\"\u003e\u003c/li\u003e\n\u003cli\u003erelated products end up closer in the embedding space\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etrain the mapping model (mapping from text query to a portion of product space) based on the user click behaviour (In the training dataset, the users queries about \u0026ldquo;Nike\u0026rdquo; and the they would click relevant Nike Product)\u003c/p\u003e","title":"Federico_bianchi Language in a Search Box Grounding Language Learning in Real World Human Machine Interaction 2021"},{"content":"[TOC]\nTitle: Decision Transformer: Reinforcement Learning via Sequence Modeling Author: Lili Chen et. al. Publish Year: Jun 2021 Review Date: Dec 2021 Summary of paper The Architecture of Decision Transformer\nInputs are reward, observation and action\nOutputs are action, in training time, the future action will be masked out.\nI believe this model is able to generate a very good long sequence of actions due to transformer architecture.\nBut somehow this is not RL anymore because the transformer is not trained by reward signal \u0026hellip;\nWhat is the difference between decision transformer and normal RL\nthere is no more value expectation and argmax over action.\ninstead, they will provide the model with the reward value we want, and then the transformer output an action that try to obtain the reward as close as the proposed reward $\\hat R$\n(so no more arg max action, but rather $a \\in A, R(a,s) \\approx \\hat R$)\n(NOTE: this is related to upside down RL)\nNOTE: can we extend this to goal condition RL ?\noff policy RL and training dataset matches transformer architecture\nthis is the illustration\nThe full algorithm\nSome key terms recall dynamic programming\nin addition to action output, the model should also output the value of the state to estimate the final reward of your strategy.\nLater we learn the model using temporal difference learning (TD)\nQ learning is on the concept of dynamic programming\nWhat is Key-To-Door game\nthe author claimed that their transformer model is able to solve tasks with long dependency (i.e., the current action may have big influence in the future state and future reward gain)\nThis is the similar situation when RL is difficult to solve games that require intensive exploration\nPossible Issue The training dataset may be very significant because the training dataset will eventually model the behaviour of the agent.\nThe issue would be \u0026ldquo;how to really let the agent explore by itself\u0026rdquo;\nhow to combine the traditional RL to this kind of sequence modelling\nsequence modelling is highly dependent on the training dataset RL is data inefficient, but it can gradually perform good actions. Potential future work Maybe a sequence modelling is quite good to solve IGLU competition because the setting in IGLU is Markov.\n(i.e., the action you perform is only dependent on the current state and there should not be very long dependency)\n","permalink":"https://sino-huang.github.io/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Decision Transformer: Reinforcement Learning via Sequence Modeling\u003c/li\u003e\n\u003cli\u003eAuthor: Lili Chen et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: Jun 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eThe Architecture of Decision Transformer\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211225170954043\" loading=\"lazy\" src=\"/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/image-assets/image-20211225170954043.png\"\u003e\u003c/p\u003e\n\u003cp\u003eInputs are reward, observation and action\u003c/p\u003e\n\u003cp\u003eOutputs are action, in training time, the future action will be masked out.\u003c/p\u003e\n\u003cp\u003eI believe this model is able to generate a very good long sequence of actions due to transformer architecture.\u003c/p\u003e\n\u003cp\u003eBut somehow this is not RL anymore because the transformer is not trained by reward signal \u0026hellip;\u003c/p\u003e","title":"Lili_chen Decision Transformer Reinforcement Learning via Sequence Modeling 2021"},{"content":"[TOC]\nTitle: Grammar-Based Grounded Lexicon Learning Author: Jiayuan Mao Publish Year: 2021 NeurIPS Review Date: Dec 2021 Summary of paper The paper extend the previous work \u0026ldquo;Neuro-Symbolic Concept Learner\u0026rdquo; by parsing the natural language questions using symbolic manner.\nThe core semantic parsing technique is Combinatory Categorical Grammar with CKY algorithm to prune unlikely expressions.\nThe full picture looks like this\nThe detailed algorithm process looks like this\nHow to derive concept embedding\ne.g., \u0026ldquo;Shiny\u0026rdquo; concept is visually grounded\nIn the end, the concept embedding will freeze.\nHow to understand novel concept\nactually, cannot\u0026hellip;.\nbut the author claimed that this model is very good at compositional generalisation task\nessentially this is few-shot learning task, and the proposed neuro-symbolic model is good at few-shot learning task (of course)\nSome key terms CKY-E algorithm\nMinor comments Notes for Semantic Parsing with CCGs\nCCG : Combinatory Categorial Grammar (CCG) is essential in this work.\nWe can learn it through Youtube videos https://www.youtube.com/watch?v=dOqe-ATkmeA\u0026list=PLun-LUE1uLNvWi-qV-tRHohfHR90Y_cAk\nCheck the following notes\nLambda Calculus Base cases\nlogical constant represent objects, concepts, relations variable literal lambda term example: check what logical form make sense\nSimply Typed Lambda Calculus\nConstructing Lambda Calculus Expressions\nCombinatory Categorial Grammar\nThe grammar formalism that we will use is called combinatory categorical grammar CCG.\nCCG categories\nLexicons\nCCG Operations\nHow to parse with CCG, given an example\nstep 1, use lexicon to match words and phrases with their categories step 2, apply operation rules step 3, further operation rules, e.g., composition rule step 4, coordinate composed adjectives step 5, apply coordinated adjectives to noun Potential future work a good baseline if we want to explore neuro-symbolic semantic parsing\n","permalink":"https://sino-huang.github.io/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Grammar-Based Grounded Lexicon Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Jiayuan Mao\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021 NeurIPS\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe paper extend the previous work \u0026ldquo;\u003ca href=\"https://arxiv.org/pdf/1904.12584.pdf\"\u003eNeuro-Symbolic Concept Learner\u003c/a\u003e\u0026rdquo; by parsing the natural language questions using symbolic manner.\u003c/p\u003e\n\u003cp\u003eThe core semantic parsing technique is Combinatory Categorical Grammar with CKY algorithm to prune unlikely expressions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe full picture looks like this\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211224225350012\" loading=\"lazy\" src=\"/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/image-assets/image-20211224225350012.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe detailed algorithm process looks like this\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211224225456819\" loading=\"lazy\" src=\"/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/image-assets/image-20211224225456819.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow to derive concept embedding\u003c/strong\u003e\u003c/p\u003e","title":"Jiayuan_mao Grammar Based Grounded Lexicon Learning 2021"},{"content":"[TOC]\nTitle: Interactive Grounded Language Understanding in a Collaborative Environment Author: Julia Kiseleva et. al. Publish Year: 2021 Review Date: Dec 2021 Summary of paper The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment.\nThe split the problem into following concrete research questions, which correspond to separate tasks that can be used to study each component individually before joining all of them into one system\nSilent Builder\nEnvironment info\nAction space\nThe agent’s action space might consist of all possible actions in Minecraft. For the current Builder baseline, we offer a discretised version of 18 actions:\nnoop, step forward, step backward, step right, step left, turn up, turn down, turn left, turn right, jump, attack, place block, choose block type 1-6. Sequence of annotation of sub-goals\nThey annotated each dialog’s sub-goals and stored them in a queue, where each sub-goal corresponds to one specific step instruction from the Architect. At each step, they pop up a sub-task (e.g., in about the middle, build a column five tall) and wait until the agent completes it. If the agent completes this sub-task, they pop up the next sub-task. They trained a matching model to decide if the current sub-task has been finished.\n","permalink":"https://sino-huang.github.io/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Interactive Grounded Language Understanding in a Collaborative Environment\u003c/li\u003e\n\u003cli\u003eAuthor: Julia Kiseleva et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eThe primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211222154135180\" loading=\"lazy\" src=\"/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/image-assets/image-20211222154135180.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211222154153920\" loading=\"lazy\" src=\"/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/image-assets/image-20211222154153920.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThe split the problem into following concrete research questions, which correspond to separate tasks that can be used to study each component individually before joining all of them into one system\u003c/p\u003e","title":"Julia_kiseleva Interactive Grounded Language Understanding in a Collaborative Environment 2021"},{"content":"[TOC]\nTitle: Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches Author: Dominik Drexler et. al. Publish Year: 2021 Review Date: Dec 2021 Summary of paper Algorithms like SIW often fail when the goal is not easily serialisable or when some of the subproblems have a high width. In this work, the author address these limitations by using a simple but powerful language for expressing finer problem decompositions called policy sketches.\nSerialised Iterated With with Sketches\nsimilar to original SIW, but the search nodes are further pruned such that the nodes must stay in policy sketches $G_r(s)$ for some pre-defined sketch rule $r$ in $R$ .\nThe only difference between SIW and $SIW_R$ is that in SIW each IW search finishes when the goal counter $#g$ is decremented, while in $SIW_R$ , when a goal or subgoal state is reached.\nthis means that for policy sketch settings, we have to define the goal state in terms of a set of features. All in all, Serialised Iterated With with Sketches is like adding task-dependent constraints to a general SIW search algorithm so that it can prune away unwanted states. Therefore, it solves address the following issues in SIW\nachieving a single goal atom already has a sufficiently large width Greedy goal serialisation generates such avoidable subproblem, including reaching unsolvable states. Some key terms Definition of width\nIW(k) algorithm\nin other words , IW(search) is about pruning based on novelty\nSerialised Iterated Width SIW\nfeatures\nis a function of state f(s)\nthe output are either\nboolean or non-negative integer numerical value $f(s)$ is the feature valuation determined by a state $s$.\nboolean feature valuation over set of feature $\\Phi$ refers to the valuation of the boolean expression $p$ and numerical expression $n=0$.\nthe set of feature $\\Phi$ distinguish or separate the goals if there is a set $B_Q$ of boolean feature valuations such that $s$ is a goal state iff $b(f(s)) \\in B_Q$\nexample\nin this case $B_Q = {n(x)=0, H=false}$\nSketch rule\nover feature $\\Phi$ has the form $C \\rightarrow E$ where $C$ consists of Boolean feature conditions, and $E$ consists of feature effects. A Boolean feature condition is of the form $p$ or $\\neg p$ for a boolean feature $p$ in $\\Phi$ , or $n=0$ or $n\u0026gt;0$ for a numerical feature $n$ in $\\Phi$.\nA feature effect is an expression of the form\n$p,\\neg p, p?$ $n\\downarrow, n\\uparrow, n ?$ downarrow means numerical decrements, uparrow means numerical increments. in sketch rules, the effect can be delivered by longer state sequences\npolicy sketch\n$R_\\Phi$ is a collection of sketch rules over the feature $\\Phi$ and the sketch is well formed if it is built from features that distinguish the goals and is terminating\nA well-formed sketch for a class of problem $Q$ defines a serialisation over $Q$. namely, a \u0026ldquo;preference\u0026rdquo; ordering $\\prec$ over the feature valuations that is irreflexive and transitive. it means that we move towards the goal by satisfying some goal associated features\nthe transition should follow the pre-defined (hand-crafted) sketch rules\nsubgoal states $G_r(s)$\ninput: a sketch rule $r$, a state $s$\noutput: a set of states $s\u0026rsquo;$ such that pair $(f(s) f(s\u0026rsquo;))$ satisfy the sketch rule $r$\nIntuitively, when in a state $s$, the subgoal states $s\u0026rsquo;$ in $G_r(s)$ provide a stepping stone in the search for plans connecting $s$ to the goal of $P$\nPotential future work It looks like the policy sketch in planning domain will have a precise signal that tell the sub-goal is reached.\nHowever, in Modular RL paper, the sub-policy network needs to decide by itself that it should STOP and pass the task to the next policy network.\nthis definitely cause some issues. Let\u0026rsquo;s see if we are able to provide a precise STOP signal to the policy network when a sub-goal is reached. ","permalink":"https://sino-huang.github.io/posts/dominik_drexler-expressing-and-exploiting-the-common-subgoal-structure-of-classical-planning-domains-using-sketches-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches\u003c/li\u003e\n\u003cli\u003eAuthor: Dominik Drexler et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eAlgorithms like SIW often fail when the goal is not easily serialisable or when some of the subproblems have a high width. In this work, the author address these limitations by using a simple but powerful language for expressing finer problem decompositions called policy sketches.\u003c/p\u003e","title":"Dominik_drexler Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches 2021"},{"content":"[TOC]\nTitle: Language as an Abstraction for Hierarchical Deep Reinforcement Learning Author: Yiding Jiang et. al. Publish Year: 2019 NeurIPS Review Date: Dec 2021 Summary of paper Solving complex, temporally-extended tasks is a long-standing problem in RL.\nAcquiring effective yet general abstractions for hierarchical RL is remarkably challenging.\nTherefore, they propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalisation\nThey present their framework for training a 2-layer hierarchical policy with compositional language as the abstraction between the high-level policy and low-level policy.\nLow level policy\nThe low level policy follows \u0026ldquo;goal-conditioned MDP setting\u0026rdquo;, given goal state $s_g$\u0026rsquo;s description $g$ and current state $s_i$, the low-level policy run several action steps to reach the goal state $s_g$, otherwise timeout and they will receive updated description $g$.\nhindsight instruction relabelling\nthis is a data augmentation technique that provides balanced reward signal to the lower-policy agents to learn.\nIt has chances to relabel the goal-description $g$ to match the current state $s$. Therefore the low-level policy will at least received some positive reward.\nHigh level policy\nhigh level policy generates goal descriptions based on the current state and on the current task (such as object ordering or other predefined tasks)\nIn this paper the author did not put many efforts discussing the high level policy.\nThe high-level policy πh(g|s) is trained to solve a standard MDP whose state space is the same $S$ as the low-level policy, and the action space is $G$. In this case, the high-level policy’s supervision comes from thereward function of the environment which may be highly sparse.\nGood things about the paper (one paragraph) Few paper mentioned goal conditioned MDP when they discuss NLP RL model. Essentially the NLP RL model is highly associated with goal conditional MDP setting.\nAlso hindsight instruction relabelling is a very great technique that can be used in any other studies.\nIncomprehension taking care of the overlapping between \u0026ldquo;goal region\u0026rdquo; and the uncertainty in \u0026ldquo;natural language instruction\u0026rdquo;\ni.e., one state can be described by a group of natural language instructions. one natural language instructions can refers to a group of states.\nTherefore we need to think about the \u0026ldquo;sweet point\u0026rdquo;\nPotential future work question: Would the higher level policy network know how to solve low-level tasks? ya, maybe know, but due to some limitations (like limited number of parameters, it can only handle limited tasks)\nso we treat the instructor as control centre, and let the lower level policy network to handle the detailed, specific tasks. but the question is how to let different components to only care about their own works given that their reward signal are the same!!! ","permalink":"https://sino-huang.github.io/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Language as an Abstraction for Hierarchical Deep Reinforcement Learning\u003c/li\u003e\n\u003cli\u003eAuthor: Yiding Jiang et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2019 NeurIPS\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eSolving complex, temporally-extended tasks is a long-standing problem in RL.\u003c/p\u003e\n\u003cp\u003eAcquiring effective yet general abstractions for hierarchical RL is remarkably challenging.\u003c/p\u003e\n\u003cp\u003eTherefore, they propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalisation\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211218205222284\" loading=\"lazy\" src=\"/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/image-assets/image-20211218205222284.png\"\u003e\u003c/p\u003e","title":"Yiding_jiang Language as Abstraction for Hierarchical Deep Reinforcement Learning"},{"content":"[TOC]\nTitle: Hierarchical Decision Making by Generating and Following Natural Language Instructions Author: Hengyuan Hu et. al. FAIR Publish Year: 2019 Review Date: Dec 2021 Summary of paper One line summary: they build a Architect Builder model to clone human behaviour for playing RTS game\nTheir task environment is very similar to IGLU competition setting, but their model is too task-specific\nThe author mentioned some properties about natural language instructions\nhow to solve composition, spatial reference, qualifier, anaphora, conditionals. ellipsis this is a common challenge to all kinds of Grounded Language learning problems Similar to Modular MultiTask paper, this model require a TERMINATE signal for builder to pass the work to instructor and vice versa\nthis shows that TERMINATE signal may be quite necessary element in NLP RL model Major comments Incomprehension of this work\na little bit broad not significantly novel not significantly contributing And obviously this task environment is a language assisted RL environment but the author stoped at supervised behaviour cloning. They should test RL agent without providing language and test if language can improve the performance\nIn Architect and Builder setting setting, the instructor should have more intelligence because they have to have a big picture when they perceive the state information, so as to provide valuable instruction to the executor. But it is still an open question about how to build a good architect that can have a big picture, strong knowledge about the task only based on the current frame of visual input?\nMaybe something like DreamV2 imaginary state prediction?\nAssumption: Architect is more intelligent than Builder. Architect can also solve the task using its knowledge.\nMinor comments I think the author put too many trivial implementation details in the main body\nIncomprehension not really understand why they split many sub-modules. They should compare the performance between their proposed model and some other baseline\nPotential future work This RTS game is quite challenging and their data is also valuable. This game provides a very difficult language assisted RL environment\nIt might be interesting if we migrate their behaviour cloning task into RL task\nIt is important to decouple language from policy model, so that the model can still perform without providing instructions. but this model does not have such option\nPossible experiment: remove the language signals periodically when training the policy model and check what would happen\n","permalink":"https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Hierarchical Decision Making by Generating and Following Natural Language Instructions\u003c/li\u003e\n\u003cli\u003eAuthor: Hengyuan Hu et. al. FAIR\u003c/li\u003e\n\u003cli\u003ePublish Year: 2019\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003eOne line summary: they build a Architect Builder model to clone human behaviour for playing RTS game\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211215191322769\" loading=\"lazy\" src=\"/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191322769.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211215191341893\" loading=\"lazy\" src=\"/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191341893.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211215191350360\" loading=\"lazy\" src=\"/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191350360.png\"\u003e\u003c/p\u003e\n\u003cp\u003eTheir task environment is very similar to IGLU competition setting, but their model is too task-specific\u003c/p\u003e\n\u003cp\u003eThe author mentioned some properties about natural language instructions\u003c/p\u003e","title":"Hengyuan_hu Hierarchical Decision Making by Generating and Following Natural Language Instructions 2019"},{"content":" Title: Attention Over Learned Object Embeddings Enables Complex Visual Reasoning Author: David Ding et. al. Publish Year: 2021 NeurIPS Review Date: Dec 2021 Background info for this paper:\nTheir paper propose a all-in-one transformer model that is able to answer CLEVRER counterfactual questions with higher accuracy (75.6% vs 46.5%) and less training data (- 40%)\nThey believe that their model relies on three key aspects:\nself-attention soft-discretization self-supervised learning What is self-attention\nEssentially they means they used a Transformer architecture What is soft-discretization\ngiven the observation that self-attention module is good at handling discrete entities on a finite sequence, they want to discretise visual information so as to fit into self-attention module Given the neuroscience literature that biological visual system infer and exploit the existence of objects rather than spatial or temporal blocks with artificial boundaries, also given that objects are atomic units of newtonian physics interactions, they discretise the visual image on the level of objects Essentially this means that they used a object detection module to encode objects in the scene into embeddings the embedding $u_{it}$ record the position of object $i$ in local coordinate at time $t$ What is supervised learning\nThey mask object embeddings, and train the model to infer the content of the masked object representations using its knowledge of unmasked objects. They designed six different masking schemes, which is different from original BERT masking scheme because BERT masking scheme only fits word tokens What is their result\nAs we can see, self attention mechanism is super important in their model. In the ablation study, the performance of Aloe model without self-attention will significantly decrease. This means that self-attention mechanism is really essential in answering dynamic visual reasoning questions Their assumptions to break\nA guiding motivation for the design of Aloe is the converging evidence for the value of self-attention mechanisms operating on a finite sequences of discrete entities. \u0026ndash; The author\nOur model relies on three key aspect: 1. Self-attention to effectively integrate information over time 2. \u0026hellip; \u0026ndash; The author\nI do believe that this paper lacks the detail analysis about how attention mechanism solves the reasoning task.\nIn our basic understanding, attention mechanism would only extract association relationship knowledge. Essentially, attention mechanism was to permit the decoder to utilise the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights. \u0026ndash; Stefania Cristina\nThere is a conflict between this paper\u0026rsquo;s result and a common assumption that \u0026ldquo;statistical machine learning struggles with causality\u0026rdquo;\nThere must be more to examine rather than simply saying \u0026ldquo;Self-attention is good because it effectively integrates information over time\u0026rdquo;\n","permalink":"https://sino-huang.github.io/posts/david_ding-attention-over-learned-object-embeddings-enables-complex-visual-reasoning-2021/","summary":"\u003col\u003e\n\u003cli\u003eTitle: Attention Over Learned Object Embeddings Enables Complex Visual Reasoning\u003c/li\u003e\n\u003cli\u003eAuthor: David Ding et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021 NeurIPS\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eBackground info for this paper:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTheir paper propose a all-in-one transformer model that is able to answer CLEVRER counterfactual questions with higher accuracy (75.6% vs 46.5%) and less training data (- 40%)\u003c/p\u003e\n\u003cp\u003eThey believe that their model relies on three key aspects:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eself-attention\u003c/li\u003e\n\u003cli\u003esoft-discretization\u003c/li\u003e\n\u003cli\u003eself-supervised learning\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211214201703442\" loading=\"lazy\" src=\"/posts/david_ding-attention-over-learned-object-embeddings-enables-complex-visual-reasoning-2021/image-assets/image-20211214201703442.png\"\u003e\u003c/p\u003e","title":"David_ding Attention Over Learned Object Embeddings Enables Complex Visual Reasoning 2021"},{"content":" Title: Modular Multitask Reinforcement Learning with Policy Sketches Author: Jacob Andreas et. al. Publish Year: 2017 Review Date: Dec 2021 Background info for this paper:\nTheir paper describe a framework that is inspired by on options MDP, for which a reinforcement learning task is handled by several sub-MDP modules. (that is why they call it Modular RL)\nThey consider a multitask RL problem in a shared environment. (See the figure below). The IGLU Minecraft challenge as well as Angry Birds also belongs to this category.\nThe tasks (goals) and the associated states are different but they shared the same environment setting What is policy sketches:\na structured, short sequence of language instructions for a single task different tasks may share same sketches. e.g., in the figure, they both have \u0026ldquo;get wood\u0026rdquo; instructions Why do they want policy sketches:\nRL agents are difficult to learn a good policy in a multitask RL problem in a shared environment with sparse rewards only. The policy sketches helps to break a big task into smaller sub-task and thus may provide a smooth learning experience What model they build to utilise policy sketches\nassign each short line $b_i$ with its own policy network $\\pi_{b_{i}}$. Besides output actions based on current state, the policy network will also output STOP signal that informs the next policy $\\pi_{b_{i+1}}$ to handle the following states How is their result\nthey shows that this sketch assisted modular model can obtain higher reward compared to a single policy model in this multitask, single environment RL problem Their delimitations to break:\nWhen we consider unstructured natural language instruction, the number of vocabulary in the corpora will be largely increased. In their work, they tried to keep a small size of vocabulary otherwise they need to maintain numerous policy networks (see figure below)\nIn their experiments, they keep a very small size of vocabulary and therefore they can maintain feasible number of policy networks, the advantages are: 1. ensure enough training for each policy network 2. ensure enough knowledge sharing among different tasks For unstructured natural language instructions, the number of words will surge and we will face synonym, paraphrase etc.\nBut I assume that keep small number of policy network is necessary, so we may want to cluster the unstructured natural language instructions and let one policy network handle one cluster\nTheir assumptions to break\nIn their training algorithm, in order to provide a smooth learning experience, they start to select simple tasks to train first. Their assumption is: a brand new RL agent cannot solve complex tasks as they can easily get stuck in local optimum. (e.g., those RL tasks like Montezuma\u0026rsquo;s Revenge that require intensive exploration) (reasonable assumption)\nSo, if they provide it with simple tasks first, the agent can gain some basic knowledge about the environment and then it can continue to learn more complex tasks.\nBut, they assume that task with smaller length of policy sketch is considered as \u0026ldquo;simple\u0026rdquo;. e.g., see the figure above, \u0026ldquo;make rope\u0026rdquo; is simpler than \u0026ldquo;make bridge\u0026rdquo; due to a smaller length of the policy sketch.\nThis assumption is broken when we consider unstructured natural language instructions. For example, \u0026ldquo;building house\u0026rdquo; is not simpler than \u0026ldquo;moving towards north for two steps\u0026rdquo;\nTherefore, a new algorithm that can rate the difficulty of tasks based on the unstructured natural language instructions is needed\n","permalink":"https://sino-huang.github.io/posts/jacob_andreas-modular-multitask-reinforcement-learning-with-policy-sketches-2017/","summary":"\u003col\u003e\n\u003cli\u003eTitle: Modular Multitask Reinforcement Learning with Policy Sketches\u003c/li\u003e\n\u003cli\u003eAuthor: Jacob Andreas et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2017\u003c/li\u003e\n\u003cli\u003eReview Date: Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eBackground info for this paper:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTheir paper describe a framework that is inspired by on \u003ca href=\"https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf\"\u003e\u003cem\u003eoptions MDP\u003c/em\u003e\u003c/a\u003e, for which a reinforcement learning task is handled by several sub-MDP modules. (that is why they call it Modular RL)\u003c/p\u003e\n\u003cp\u003eThey consider a multitask RL problem in a shared environment. (See the figure below). The IGLU Minecraft challenge as well as Angry Birds also belongs to this category.\u003c/p\u003e","title":"Jacob_andreas Modular Multitask Reinforcement Learning With Policy Sketches 2017"},{"content":"[TOC]\nTitle: On the Expressivity of Markov Reward Author: David Abel et. al. Publish Year: NuerIPS 2021 Review Date: 6 Dec 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nThe author found out that in the Markov Decision Process scenario, (i.e., we do not look at the history of the trajectory to provide rewards), some tasks cannot be realised perfectly by reward functions. i.e.,\nthere exists instances of tasks that no Markov reward function can capture.\nAssumption:\nwhen we think of a task, we are able to find its associated acceptable/prefered policies/behaviours i.e., A task convey general preference over behaviour or outcome. So, the author found that\nTheorem 4.1. For each of SOAP, PO, and TO, there exists (Env, Task) pairs for which no Markov reward function realize Task in Environment \u0026ndash; author\nexample of inexpressible reward,\ngiven the environment and we design a task such that ${\\pi_{12}, \\pi_{21}}$ are desired policies (i.e., The XOR like issue, in S0 we prefer any acitions and in S1 we prefer the opposite action, It means that the prefered action for the second step depends on the choice of the first step, history matters!).\nPrefered Policy (dependent on the task) realisation of a reward function, according to the author, means that the value V value of the starting state of the prefered policy must be greater than the V value of the starting state from other \u0026lsquo;bad\u0026rsquo; policy when using this reward function. (it means that only the prefered policy leads to maximum accumulative reward)\nThen in Entailment Case, we cannot define a reward function such that the Value of starting state of the prefered policy is greater than others like $\\pi_{11}$ (easy to prove)\nreward function design\nThe author also proposed a reward function design algorithm that can determine whether an appropriate reward function can be constructed, so that the reward function can realise the prefered policy of the task.\nfringe policies: the policies that are just differ from the good policies with just one action.\nthe algoritm ensures that non-optimal policies will always have lower value of its starting state than the prefered policies\n$X$ is the variable for the Value of the state\ndiscovery\nfor this famous grid world task\nthe author found that the traditional reward function -\u0026gt; +1 for reach goal, -1 for reach fire, does not induce a perfect match in policy. (see the figure)\nIt shows the usefulness of that reward design algorithm.\nAlso, the experiments shows that, the hyperparameter of the environment will strongly affect the expressivity of the previous good reward function designed by their algorithm.\nSome key terms Markov reward function\nMarkov reward functions can represent immediate worth in an intuitive manner that also allows for reasoning about combinations, sequences, or reoccurrences of such states of affairs.\nexpressing a task\ntreat a reward function as accuratelyexpressinga task just when the valuefunction it induces in an environment adheres to the constraints of a given task\nroles of reward\ncan both define the task the agent learns to solve, and define the “bread crumbs” that allow agents to efficiently learn to solve the task.\n**reward in terms of expressivity **\nreward is treated as an expressive language for incentivizing reward-maximizing agents\nTask definition\nIndeed, characterizing a task in terms of choice of $\\pi$ or goal fails to capture these distinctions. Our view is that a suitably rich account of task should allowfor the characterization of this sort of preference, offering the flexibility to scale from specifying onlythe desirable behavior (or outcomes) to an arbitrary ordering over behaviors (or outcomes).\nthis leads to for a task, there should be associated set of prefered policies or behaviours.\nSOAPs\nSet Of Acceptable Policies, a task will have its own set of prefered policies, and a desired reward function will maximise the reward accumulation only when using the prefered policies\nPOs\ngeneralise the SOAP, Partial Ordering on policies,\nfor example, instead of just the good poilcies, the task is also characterised by some bad policies that must be avoided.\nTOs\nThis means we characterise the task by defining prefered/bad trajectories\nGood things about the paper (one paragraph) This is not always necessary, especially when the review is generally favorable. However, it is strongly recommended if the review is critical. Such introductions are good psychology if you want the author to drastically revise the paper.\nA really interesting paper about RL reward and reward designing.\nThe research on the expressivity of reward would help to decide whether we should use RL methods on certain tasks.\nMajor comments Discuss the author’s assumptions, technical approach, analysis, results, conclusions, reference, etc. Be constructive, if possible, by suggesting improvements.\nIt looks like the issue mentioned in the paper can just be easily alleviated by introducing history information to the reward function.\nMinor comments This section contains comments on style, figures, grammar, etc. If any of these are especially poor and detract from the overall presentation, then they might escalate to the ‘major comments’ section. It is acceptable to write these comments in list (or bullet) form.\nMarkov Decision Process intro video\nIncomprehension List what you don’t understand.\nGiven the assumption: \u0026ldquo;all task should have their associated set of prefered policies or bad policies\u0026rdquo;, and in SOAPs condition,\nthe author claim that the expressivity of reward function is just about if the reward function can realise all the prefered policies in the set. But why do we require just one reward function to satisfy all of the prefered policies?\nEventually an agent would just pick one policy to use.\n","permalink":"https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: On the Expressivity of Markov Reward\u003c/li\u003e\n\u003cli\u003eAuthor: David Abel et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: NuerIPS 2021\u003c/li\u003e\n\u003cli\u003eReview Date: 6 Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe author found out that in the \u003cem\u003eMarkov Decision Process\u003c/em\u003e scenario, (i.e., we do not look at the \u003cem\u003ehistory\u003c/em\u003e of the trajectory to provide rewards), some tasks cannot be \u003cem\u003erealised\u003c/em\u003e perfectly by reward functions. i.e.,\u003c/p\u003e","title":"David_abel on the Expressivity of Markov Reward 2021"},{"content":"[TOC]\nTitle: Deep Reinforcement Learning at the Edge of the Statistical Precipice Author: Rishabh Agarwal et. al. Publish Year: NeurIPS 2021 Review Date: 3 Dec 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nMost current published results on deep RL benchmarks uses point estimate of aggregate performance such as mean and median score across the task.\nThe issue of this performance measurement is:\nsince the benchmarks are computially demanding, one cannot run the evaluation process many times. the statistical uncertainty implied by the use of a finite number of training runs will \u0026ldquo;slow down progress in the field\u0026rdquo; only reporting point estimates obscure nuances in comparison and can erroneously lead the filed to conclude which methods are SOTA, ensuing wasted effort when applied in practice. To address this deficiencies, the author presented more efficient and robust alternatives\ninterquartile mean not easily affected by outlier reporting performance distribution across all runs (performance profiles) interval estimates via strtified bootstrap confidence intervals Recommendation of reliable evaluation\nSome key terms point estimate\npoint estimate: In statistics, point estimation involves the use of sample data to calculate a single value which is to serve as a \u0026ldquo;best guess\u0026rdquo;\n**bootstraping method **\nhttps://www.youtube.com/watch?v=Xz0x-8-cgaQ instead of repeating the expensive experiments multiple times, we can do bootstrapping to evaluation the performance.\nbootstrap dataset: sample the data from the original dataset with replacement (meaning you can pick one element twice)\noptimality gap\noptimality gap: the amount by which thealgorithm fails to meet a minimum score ofγ= 1.0\nprobability of improvement his metricshows how likely it is forXto outperformYon a randomly selected task.\nGood things about the paper (one paragraph) This is not always necessary, especially when the review is generally favorable. However, it is strongly recommended if the review is critical. Such introductions are good psychology if you want the author to drastically revise the paper.\nclear\nand the concern is serious\nit presents a better way of RL method evaluation\nMajor comments Discuss the author’s assumptions, technical approach, analysis, results, conclusions, reference, etc. Be constructive, if possible, by suggesting improvements.\nThis is not about discovery of new algorithm, but an appeal for better evaluation metric\nMinor comments This section contains comments on style, figures, grammar, etc. If any of these are especially poor and detract from the overall presentation, then they might escalate to the ‘major comments’ section. It is acceptable to write these comments in list (or bullet) form.\nThe team publish a open source evaluation library\nhttps://github.com/google-research/rliable\nPotential future work List what you can improve from the work\nWe can use this evaluation method to evaluate future RL systems\n","permalink":"https://sino-huang.github.io/posts/rishabh_agarwal-deep-reinforcement-learning-at-the-edge-of-the-stats-precipice-2021/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Deep Reinforcement Learning at the Edge of the Statistical Precipice\u003c/li\u003e\n\u003cli\u003eAuthor: Rishabh Agarwal et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: NeurIPS 2021\u003c/li\u003e\n\u003cli\u003eReview Date: 3 Dec 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMost current published results on deep RL benchmarks uses \u003cem\u003epoint estimate\u003c/em\u003e of aggregate performance such as mean and median score across the task.\u003c/p\u003e","title":"Rishabh_agarwal Deep Reinforcement Learning at the Edge of the Stats Precipice 2021"},{"content":"[TOC]\nTitle: Reward learning from human preferences and demonstractions in Atari Author: Borja Ibarz et. al. Publish Year: 2018 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nThe author proposed a method that uses human expert\u0026rsquo;s annotation rather than extrinsic reward from the environment to guide the reinforcement learning.\nthe proposed trainnig algorithm two feedback channels are provided:\nDemonstrations: several trajectories of human behavour on the task Preferences: the human compare pairwise short trajectory segments of the agent\u0026rsquo;s behaviour and prefer those that are closer to the intended goal. When training the policy, they use TD error to train.\nThe training objective for the agent\u0026rsquo;s policy is the cost function $$ J(Q) = J_{PDDQ_n}(Q) + \\lambda_2J_E(Q) + \\lambda_3J_{L2}(Q) $$\nthe first is the prioritized dueling double Q-loss,\nthe second is the large-margin supervised loss, applied only to expert demonstrations\nthe third is the regularization loss\nWhen training the reward function, the model is trained by predicting the probability of preferring a segment $\\sigma_1$ over the other. This is trained by minimizing the cross-entropy loss between the prediction and the actual juedgement labels provided by the human experts.\nThe author claimed that this method outperform the imitation learning where the agent is trained by only the demonstrations from the human experts.\n","permalink":"https://sino-huang.github.io/posts/borja_ibarz-reward-learning-from-human-preferences-and-demonstrations-in-atari-2018/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Reward learning from human preferences and demonstractions in Atari\u003c/li\u003e\n\u003cli\u003eAuthor: Borja Ibarz et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2018\u003c/li\u003e\n\u003cli\u003eReview Date: Nov 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe author proposed a method that uses \u003cstrong\u003e\u003cu\u003ehuman expert\u0026rsquo;s annotation\u003c/u\u003e\u003c/strong\u003e rather than extrinsic reward from the environment to guide the reinforcement learning.\u003c/p\u003e","title":"Borja_ibarz Reward Learning From Human Preferences and Demonstrations in Atari 2018"},{"content":"[TOC]\nTitle: Go-Explore: a New Approach for Hard-Exploration Problems Author: Adrien Ecoffet et. al. Publish Year: 2021 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nThe author hypothesised that there are two main issues that prevent DRL agents from achieving high score in exploration-hard game (e.g., Montezuma\u0026rsquo;s Revenge)\nDetachment: the agent just forget how to reach the promising frontier state (think about \u0026ldquo;I wanna\u0026rdquo; game) due to poor policy, and thus waste a lot of computational power return to the frontier states why return to the frontier state, it is because exploring from the frontier state is more efficient than exploring from the starting point for each episode. (think about I wanna game) Derailment: the agent face difficulty obtain a balanced explore-exploit trade off. e.g., the agent may just underrate a repeated but necessary path (assign this path with a low intrinsic reward score) and then start to explore without completing this necessary path In order to solve this, the author introduced a new training algorithm named \u0026ldquo;Go-Explore\u0026rdquo;\nIt consists of two phase\nExploration phase\nIt builds an archive of the different states it has visited in the environment, cache the information from the simulator so that it ensure the agent can immediately return to the state (like Save and Load mechanism)\nThe game states in Atari game is huge and thus we cannot store evey frame in the archive. Instead, we group state with the same features (features are selected by domain knowledge) into one cell. And each cell only stores one state that obtains the highest score among other similar states.\nBy first returning before exploring, Go-Explore avoids derailment by minimising exploration when returning (thus minimising failure to return) after which it can purely focus on exploration.\nRobustification phase\nThe best trajectories in the archive are not robust to stochasticity or unexpected outcome. (e.g., a robot may slip and miss a crucial turn, invalidating the entire trajectory). To solve this issue, a policy is required to control the agent. The policy is trained using backward algorithm so as to match the demonstration trajectory.\nSome key terms The backward algorithm\nThe backward algorithm places the agent close to the end of the trajectory and runs PPO until the performance of the agent matches that of the demonstration. Once that is achieved, the agent\u0026rsquo;s starting point is moved closer to the trajectory\u0026rsquo;s begining and process is repeated\nDomain knowledge representation\nthis is used to classify states into different cells. Domain knowledge is essential for extensive exploration games.\nIt shows that with domain knowledge, the exploration phase will reach really high score, hence during the robustification phase, the agent is able to obtain high quality demonstrations. Good things about the paper (one paragraph) This is not always necessary, especially when the review is generally favorable. However, it is strongly recommended if the review is critical. Such introductions are good psychology if you want the author to drastically revise the paper.\nThe proposed model achieve SOTA performance on Montezuma\u0026rsquo;s Revenge\nMajor comments Discuss the author’s assumptions, technical approach, analysis, results, conclusions, reference, etc. Be constructive, if possible, by suggesting improvements.\nThe author did not present their domain knowledge representation for Montezuma\u0026rsquo;s Revenge game, which is essential for their cell formation process. (i.e., group similar states into one cluster)\nBut in fact, providing the domain knowledge (e.g., providing complete map to the agent) may already violate the original purpose of the extensive exploration games.\nPotential future work List what you can improve from the work\nHow to learn features to \u0026ldquo;structure\u0026rdquo; the exploration using NLP technique rather than hand-crafted features using domain knowledge.\nGiven the fact that the text corpus for Montezuma\u0026rsquo;s Revenge is scarce, extract abstract and general knowledge is a difficult task.\n","permalink":"https://sino-huang.github.io/posts/adrien_ecoffet-go-explore-a-new-approach-for-hard-exploration-problems-2021-paper-review/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Go-Explore: a New Approach for Hard-Exploration Problems\u003c/li\u003e\n\u003cli\u003eAuthor: Adrien Ecoffet et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Nov 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe author hypothesised that there are two main issues that prevent DRL agents from achieving high score in exploration-hard game (e.g., Montezuma\u0026rsquo;s Revenge)\u003c/p\u003e","title":"Adrien_ecoffet Go Explore a New Approach for Hard Exploration Problems 2021 Paper Review"},{"content":"[TOC]\n[论文简析]SAC: Soft Actor-Critic Part 1[1801.01290] hat means estimation\n","permalink":"https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003ch2 id=\"论文简析sac-soft-actor-critic-part-1180101290httpswwwbilibilicomvideobv1yk4y1t7b6fromsearchseid13778340264869221460\"\u003e\u003ca href=\"https://www.bilibili.com/video/BV1YK4y1T7b6?from=search\u0026amp;seid=13778340264869221460\"\u003e[论文简析]SAC: Soft Actor-Critic Part 1[1801.01290]\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211123221032880\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221032880.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211123221054067\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221054067.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211123221439305\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221439305.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211123221905958\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221905958.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211123221928661\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221928661.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124234149933\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234149933.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124234615632\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234615632.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124234654185\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234654185.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124234853137\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234853137.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235018547\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235018547.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235141106\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235141106.png\"\u003e\u003cimg alt=\"image-20211124235309125\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235309125.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235518333\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235518333.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235636919\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235636919.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ehat\u003c/strong\u003e means estimation\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235727839\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235727839.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211124235947697\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235947697.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000155557\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000155557.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000319745\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000319745.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000439109\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000439109.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000645586\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000645586.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000751016\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000751016.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125000932147\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000932147.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002108776\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002108776.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002223831\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002223831.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002441206\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002441206.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002504865\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002504865.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002514461\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002514461.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125002922725\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002922725.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125003125841\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003125841.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125003148666\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003148666.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image-20211125003208320\" loading=\"lazy\" src=\"/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003208320.png\"\u003e\u003c/p\u003e","title":"Tuomas_haarnoja Soft Actor Critic Off Policy Maximum Entropy Deep Reinforcement Learning With a Stochastic Actor 2018 Paper Review"},{"content":"[TOC]\nTitle: Agent57: Outperforming the Atari Human Benchmark 2020 Author: Adria Badia et. al. Publish Year: 2020 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nAgent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like \u0026ldquo;Montezuma\u0026rsquo;s Revenge, \u0026ldquo;Pitfall\u0026rdquo;, \u0026ldquo;Solaris\u0026rdquo; and \u0026ldquo;Skiing\u0026rdquo;.\nBefore we understand how Agent57 works, we shall look at its ancestry and how it evolves from DQN agent in 2015.\nRecurrent Replay Distributed DQN (R2D2) DQN efficiency and effectiveness improvements Double DQN (https://www.youtube.com/watch?v=ECV5yeigZIg\u0026list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2\u0026index=8)\nthe Original DQN would often overestimate the Q value (make it higher). It\u0026rsquo;s true because the maximum Q value estimated by the network may be outdated very quickly.\nBut if we has this online learning and update the Network immidiately for every action, then we would have Correlated Sampling in Online-Q-Learning problem.\nThis cause the network to always overfit to a small neighbourhood of the states.\nBesides that, the target Q value is generated by our current network. (in fact, we do not have exact target Q value from online learning because we haven\u0026rsquo;t reached the end of the game)\nSo, the agent need a experience memory (Experience Replay) to collect the experience of the whole episodes. The reason is because otherwise we cannot get our fixed Q-value label. (our Q value label require we complete enough rounds of episodes)\nAdvantages:\nBreaks correlations between consecutive samples Each experience step may influence multiple gradient updates (no need to drop) But again our target Q value is generated by our current network and thus no stable gradient through this target Q value because it also moves as our network improves.\nSolution -\u0026gt; force our target Q value (obtained by Bellman function) fixed for some duration.\nNow let\u0026rsquo;s solve the overestimation problem\nprioritised experience replay (https://www.youtube.com/watch?v=MqZmwQoOXw4)\nwe do not want randomly sample the experience.\nHowever, we need to take care of the sample distribution problem and we do not want to overfit to the small neighbourhood of those hard samples, thus we introduce a importance weight.\nThus, now we coarsely learn the easy scenes but finely learn the hard scenes\nDueling architecture\nFor some states, estimating Q value of all actions is not useful.\n(I may believe that this is a similar improvement as ResNet to ResNeXt (divide and conquer))\nDistributed agents distributed RL agent decouples the data collection and learning process.\nMany actors interact with independent copies of the environment, feeding data to a central \u0026lsquo;memory bank\u0026rsquo; in the form of a prioritized replay buffer.\nA learner then samples training data from this replay buffer and get trained.\nThe learner weights are sent to the actors frequently, allowing actors to update their own weights in a manner determined by their individual priorities.\nShort-term memory Memory allows agents to make decisions based on a sequence of observations, which can reveal more information about the environment as a whole.\nTherefore the role of memor is to aggregate information from past observations to improve the decision making procecss. In deep RL and deep learning, RNN such as LSTM are used as short term memories.\nBut how to combine memories with off-policy learner?\n(R2D2)\nInstead of regular (s,a,r,s\u0026rsquo;) transition tuples in the replay buffer, we store fixed-length (m=80) sequence of (s,a,r), with adjacent sequence overlapping each other by 40 time steps, and never crossing episode boundaries. When training, we unroll bot online and target networks on the same sequence of states to generate value estimates and targets.\nNever Give Up RL agent Episodic memory The episodic memory M is a dynamically-sized slot-based memory that stores the controllable states in an online fashion (add memory on the fly). At time t, the memory contains the controllable states of all the observations visited in the current episode, ${f(x_0), \u0026hellip;, f(x_t-1)}$\nthis enables the agent to detect when new espisodes are encountered, so the agent can explore more when encountering unseen environments rather than exploit.\nExploration improvement Intrinsic motivation rewards encourages an agent to explore and visit as many states as possible by providing more dense \u0026ldquo;internal\u0026rdquo; rewards for novelty-seeking behaviours.\nThere are two types of rewards\nlong-term novelty rewards that encourage visiting many states throughout training, across many episodes. adjusted by how often the agent has seen a state similar to the current one relative to states seen overall. Short-term novelty rewards that encourage visiting many states within a single episode of a game. use the episodic memory to recognise novel experiences. the magnitude of the reward is determined by measuring the distance between the present state and previous state recorded in episodic memory. One thing about measuring the novelty is that measuring the novelty features rather than the original observations would be a better approach.\nAfter that, assigning actors with different policies based on the importance weighting on the total novelty reward would produce different experiences, ensuring more exploration\nMeta controller Use a traditional sliding window UCB bandit to control the exploration factor as well as the discount factor\nSome key terms Extensive exploration problem\nFor Montezuma\u0026rsquo;s Revenge and Pitfall, a extensive exploration is required for the agent to understand the environment.\nLong-term credit assignment problem\nFor Skiing and Solaris Atari games, the current action may affect the far future reward and thus a low discount factor for the Q value is not suitable.\nOn-policy learner\nwhich can only learn the value of its direct actions.\nOff-policy learner\nwhich can learn about optimal actions even when not performing those actions.\ne.g., it might be taking random actions, but can still learn what the best possible action would be later.\nOff-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environments.\n","permalink":"https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Agent57: Outperforming the Atari Human Benchmark 2020\u003c/li\u003e\n\u003cli\u003eAuthor: Adria Badia et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2020\u003c/li\u003e\n\u003cli\u003eReview Date: Nov 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAgent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like \u0026ldquo;Montezuma\u0026rsquo;s Revenge, \u0026ldquo;Pitfall\u0026rdquo;, \u0026ldquo;Solaris\u0026rdquo; and \u0026ldquo;Skiing\u0026rdquo;.\u003c/p\u003e","title":"Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review"},{"content":"[TOC]\nTitle: Width-based Lookaheads with Learnt Base Policies and Heuristics Over the Atari-2600 Benchmark Author: Stefan O\u0026rsquo;Toole et. al. Publish Year: 2021 Review Date: Tue 16 Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nThis paper proposed a new width-based planning and learning agent that can play Atari-2600 games (though it cannot play Montezuma\u0026rsquo;s Revenge). The author claimed that width-based planning exploration plus (greedy) optimal MDP policy exploitation is able to achieve better performance than Monte-Carlo Tree Search.\nThe core algorithm for the Atari agent The procedue of the width-based planning is the following:\nForm a lookahead tree. Maintaining the tree require the agent to access to the simulator (i.e., the simulator has to provide future states to the agent before it takes the actual move (foresee the future states)) In order to search for the useful potential next states, the agent use width-based planning to expand the tree. the width-based planner check if the state is novel or not, it will explore the novel states after deciding which states need to be explored, the pointer will move to the new state and the agent will start a new round of exploration again (loop) After it reaches the terminal states or the computational budget is used up, we stop the exploration and the lookahead tree is completed We select the action that maximise the accumulated reward in the lookahead tree. Now we perform the action in the real game environment (though we has already explored the game environment during planning phase,) during testing time, the agent is not able to access to the simulator to build lookahead tree (maybe it can, but in reality, the agent cannot execute actions multiple times and replay the task), thus the agent also used a neural network to mimic the policy during the planning phase. Some key terms Width-based planner\nWidth-based planner must has a lookahead memory space that allows the agent to plan its moves. (Check the procedure explanaion in the previous section)\nWhen the width-based planner plans its moves, it will prefer the states that are novel. IW(1) considers a state in the lookahead as novel if it is the first state within the lookahead to make a particular feature within the feature set true (i.e. novel)\nTherefore the width-based planner helps to explore the game environmnent because it always expands the novel states.\nAfter it reached terminal states or the computational budgets are used up, the current exploration round ends.\nNovelty features Novelty features is essential for width-based planner. The author suggests several novelty features for the Atari game.\nThe pixel difference of the game display (simple and vanilla) B-PROST \u0026mdash; capture temporal and spatial relationship between the past and present screen pixel latent representation of the game display obtained by Variational Auto Encoder (VAE) RAM of the game (this is tricky) Critical Path\nThe author refered \u0026ldquo;critical path\u0026rdquo; as the trajectory fromed by optimal state-action from the lookahead memory.\nLearning schedule\nThe author wants to ensure that the new policy learnt from the current episode is better than the previous policy.\nTherefore, the author used Welch\u0026rsquo;s t-test to test if the performance improves or not. If not, the new parameters learned from this episode will be dropped.\nThis mechanism is similar to residual connection that can prevent performance loss due to unstable training.\nOnline Planning\nOnline planning means the planner tried to find a good path given partial information. it is required to access to the simulator in real time to obtain new information to expand the knowledge\nGood things about the paper (one paragraph) This is not always necessary, especially when the review is generally favorable. However, it is strongly recommended if the review is critical. Such introductions are good psychology if you want the author to drastically revise the paper.\nThe paper explains in detail the implementation of the width-planning algorithm.\nMajor comments Discuss the author’s assumptions, technical approach, analysis, results, conclusions, reference, etc. Be constructive, if possible, by suggesting improvements.\nThe abstract says \u0026ldquo;This analysis of thegames provides further insight into the behaviour and performance of the algorithms introduced.\u0026rdquo;\nIn fact I would like to get more concrete information about what is the insights. I am not sure if explaining in more details in the abstract helps more.\nMinor comments This section contains comments on style, figures, grammar, etc. If any of these are especially poor and detract from the overall presentation, then they might escalate to the ‘major comments’ section. It is acceptable to write these comments in list (or bullet) form.\nA little bit messy when explaning the algorithm\nIncomprehension List what you don’t understand.\nThere is no clear mathematical proof that width-based-planning is better than tranditional Monte-Carlo Tree Search. I believe this work compete with MCTS but there is not much comparison in this paper.\nPotential future work List what you can improve from the work\n","permalink":"https://sino-huang.github.io/posts/stefan-o-toole-width-based-lookaheads-with-learnt-base-policies-and-heuristics-over-the-atari-2600-benchmark-2021-paper-reivew/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: Width-based Lookaheads with Learnt Base Policies and Heuristics Over the Atari-2600 Benchmark\u003c/li\u003e\n\u003cli\u003eAuthor: Stefan O\u0026rsquo;Toole et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021\u003c/li\u003e\n\u003cli\u003eReview Date: Tue 16 Nov 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis paper proposed a new width-based planning and learning agent that can play Atari-2600 games (though it cannot play Montezuma\u0026rsquo;s Revenge). The author claimed that width-based planning \u003cem\u003eexploration\u003c/em\u003e plus (greedy) optimal MDP policy exploitation is able to achieve better performance than Monte-Carlo Tree Search.\u003c/p\u003e","title":"Stefan O Toole Width Based Lookaheads With Learnt Base Policies and Heuristics Over the Atari 2600 Benchmark 2021 Paper Reivew"},{"content":"[TOC]\nTitle: MINDCRAFT: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks Author: Cristian-Paul Bara et. al. Publish Year: 2021 EMNLP Review Date: 12 Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\nThe contribution of this paper is the mind modelling dataset (Using Minecraft environment).\nThe dataset collects the players\u0026rsquo; belief during their playing period, by asking three types of mind modelling questions:\nCompleted task status the player is asked about whether a specific material has been created. Player knowledge ask whether the player (themselves and the teammates) knows some crafting knowledge Player\u0026rsquo;s Current task ask what they believe their partner to be making The author regarded this as mind modelling (a.k.a, belief recording).\nAfter that, the dataset collects players\u0026rsquo; timestamped dialogue utterances in the chat box, their questions and answers to the periodic popups for belief recording, internal game logs that represent the game state, and three sets of video recording in different perspectives.\nInterestingly, the accuracy that the human player predicts partner\u0026rsquo;s belief increases from 40% (low) to 80% (high) as they communicate more. This shows that\ncommunication cause the common ground to evolve. with limited, partial, knowledge at the early stage, the performance of predicting partner\u0026rsquo;s belief is just bad. In the Second Section, the author introdueced LSTM and Transformer baseline model to \u0026ldquo;infer players\u0026rsquo; beliefs\u0026rdquo; (i.e., the problem for the model is to predict the human player\u0026rsquo;s answer on the belief recording questions using the observatoins the player preceived) (the potential contribution of this research problem is to better understand human collaborative behaviours and several theory of mind tasks)\nIn the end, the author expected more types of research questions that can use their dataset.\nSome key terms belief state\nA belief state encapsulates the beliefs an agent has about its current state\nGood things about the paper (one paragraph) This is not always necessary, especially when the review is generally favorable. However, it is strongly recommended if the review is critical. Such introductions are good psychology if you want the author to drastically revise the paper.\nThe belief recording dataset is worth further research.\nmore can be discovered for the phenomenon \u0026mdash; human\u0026rsquo;s accuracy of belief increases as communications and collaborations go on. This gradually transformation looks common and important in human-human interaction, thus it might be also applicable in human-machine interaction, and thus more needs to be discovered.\nYes, common ground formalisation is essential for collaboration tasks.\nMajor comments Discuss the author’s assumptions, technical approach, analysis, results, conclusions, reference, etc. Be constructive, if possible, by suggesting improvements.\nThe experiment in the Second section \u0026mdash; baseline computational model \u0026mdash; is not rigorous.\nWhen evaluating the performance of different models, the author should at least compare the number of parameters in both LSTM and Transformer models rather than merely comparing the architecture. In fact, there are more that can affect the performance.\nAlso the author did not provide detailed explanation about why one architecture could outperform the other. I believe the readers want to know more.\nIncomprehension List what you don’t understand.\nShould have a basic intro about Thoery of mind modelling in Introduction section, otherwise I still cannot understand why the author designed \u0026rsquo;the three questions\u0026rsquo; and also why three types of questions are adequate for belief recording.\nPotential future work List what you can improve from the work\nYes, as we can see, the evolution of common ground (shared knowledge) is a really important topic for human-machine interaction, more needs to be discovered. E.g.,\nhow could an intelligent agent reacts when it shares limtied knowledge with the partner. what kinds of communication are effective in this collaboration work so that the common ground can evolve efficently. ","permalink":"https://sino-huang.github.io/posts/cristian-paul-bara-mindcraft-theory-of-mind-modelling-2021-paper-review/","summary":"\u003cp\u003e[TOC]\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTitle: MINDCRAFT: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks\u003c/li\u003e\n\u003cli\u003eAuthor: Cristian-Paul Bara et. al.\u003c/li\u003e\n\u003cli\u003ePublish Year: 2021 EMNLP\u003c/li\u003e\n\u003cli\u003eReview Date: 12 Nov 2021\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"summary-of-paper\"\u003eSummary of paper\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe contribution of this paper is \u003cstrong\u003ethe mind modelling dataset\u003c/strong\u003e (Using Minecraft environment).\u003c/p\u003e","title":"Cristian Paul Bara Mindcraft Theory of Mind Modelling 2021 Paper Review"}]