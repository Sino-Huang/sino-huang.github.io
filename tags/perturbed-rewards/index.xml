<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Perturbed Rewards on Sukai Huang</title>
    <link>https://sino-huang.github.io/tags/perturbed-rewards/</link>
    <description>Recent content in Perturbed Rewards on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 26 Dec 2022 01:11:23 +1100</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/tags/perturbed-rewards/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tom_everitt Reinforcement Learning With a Corrupted Reward Channel 2017</title>
      <link>https://sino-huang.github.io/posts/tom_everitt-reinforcement-learning-with-a-corrupted-reward-channel-2017/</link>
      <pubDate>Mon, 26 Dec 2022 01:11:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tom_everitt-reinforcement-learning-with-a-corrupted-reward-channel-2017/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With a Corrupted Reward Channel&lt;/li&gt;
&lt;li&gt;Author: Tom Everitt&lt;/li&gt;
&lt;li&gt;Publish Year: August 22, 2017&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Dec 26, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP&lt;/li&gt;
&lt;li&gt;Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be &lt;strong&gt;completely managed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;second, by using randomisation to blunt the agent&amp;rsquo;s optimisation, reward corruption can be partially managed under some assumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vincent_zhuang No Regret Reinforcement Learning With Heavy Tailed Rewards 2021</title>
      <link>https://sino-huang.github.io/posts/vincent_zhuang-no-regret-reinforcement-learning-with-heavy-tailed-rewards-2021/</link>
      <pubDate>Sun, 25 Dec 2022 18:15:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/vincent_zhuang-no-regret-reinforcement-learning-with-heavy-tailed-rewards-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: No-Regret Reinforcement Learning With Heavy Tailed Rewards&lt;/li&gt;
&lt;li&gt;Author: Vincent Zhuang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Dec 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To the best of our knowledge, no prior work has considered our setting of heavy-tailed rewards in the MDP setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We demonstrate that &lt;strong&gt;robust mean estimation techniques&lt;/strong&gt; can be broadly applied to reinforcement learning algorithms (specifically
confidence-based methods) in order to provably han-
dle the heavy-tailed reward setting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Robust UCB algorithm&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wenshuai_zhao Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning 2020</title>
      <link>https://sino-huang.github.io/posts/wenshuai_zhao-towards-closing-the-sim-to-real-gap-in-collaborative-multi-robot-deep-reinforcement-learning-2020/</link>
      <pubDate>Sun, 25 Dec 2022 16:54:11 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/wenshuai_zhao-towards-closing-the-sim-to-real-gap-in-collaborative-multi-robot-deep-reinforcement-learning-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Wenshuai Zhao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Dec 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning&lt;/li&gt;
&lt;li&gt;we discuss on how both the different types of perturbations and how the number of agents experiencing those perturbations affect the collaborative learning effort&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is, to the best of our knowledge, the first work exploring the limitation of PPO in multi-robot systems when considering that different robots might be exposed to different environment where their sensors or actuators have induced errors&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jan_corazza Reinforcement Learning With Stochastic Reward Machines 2022</title>
      <link>https://sino-huang.github.io/posts/jan_corazza-reinforcement-learning-with-stochastic-reward-machines-2022/</link>
      <pubDate>Sat, 24 Dec 2022 22:36:07 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jan_corazza-reinforcement-learning-with-stochastic-reward-machines-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Stochastic Reward Machines&lt;/li&gt;
&lt;li&gt;Author: Jan Corazza et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: AAAI 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequence of actions. However, existing algorithms for learning reward machines assume an overly idealized   setting where rewards have to be free of noise.&lt;/li&gt;
&lt;li&gt;to overcome this practical limitation, we introduce a novel type of reward machines called stochastic reward machines, and an algorithm for learning them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discussing the handling of noisy reward for non-markovian reward function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;limitation&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;the solution introduces multiple sub value function models, which is different from the standard RL algorithm.&lt;/li&gt;
&lt;li&gt;The work does not emphasise on the sample efficiency of the algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reward machine&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oguzhan_dogru Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering 2022</title>
      <link>https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/</link>
      <pubDate>Sat, 24 Dec 2022 19:32:25 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering&lt;/li&gt;
&lt;li&gt;Author: Oguzhan Dogru et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: July 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this study consider a type of uncertainty, which is caused by the sensor that are utilised for reward function. When the noise is Gaussian and the system is linear&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221224214550390&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/image-assets/image-20221224214550390.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this work used &amp;ldquo;particle filtering&amp;rdquo; technique to estimate the true reward function from the perturbed discrete reward sampling points.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;h2 id=&#34;good-things-about-the-paper-one-paragraph&#34;&gt;Good things about the paper (one paragraph)&lt;/h2&gt;
&lt;h2 id=&#34;major-comments&#34;&gt;Major comments&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/</link>
      <pubDate>Sat, 24 Dec 2022 17:06:12 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Inaam Ilahi et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 13 Sep 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications.&lt;/li&gt;
&lt;li&gt;Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms&lt;/li&gt;
&lt;li&gt;we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures&lt;/li&gt;
&lt;li&gt;we discuss the available benchmarks and metrics for the robustness of DRL&lt;/li&gt;
&lt;li&gt;finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions .&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;organisation of this article&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zuxin_liu on the Robustness of Safe Reinforcement Learning Under Observational Perturbations 2022</title>
      <link>https://sino-huang.github.io/posts/zuxin_liu-on-the-robustness-of-safe-reinforcement-learning-under-observational-perturbations-2022/</link>
      <pubDate>Thu, 22 Dec 2022 22:38:13 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zuxin_liu-on-the-robustness-of-safe-reinforcement-learning-under-observational-perturbations-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: On the Robustness of Safe Reinforcement Learning Under Observational Perturbations&lt;/li&gt;
&lt;li&gt;Author: Zuxin Liu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 3 Oct 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Dec 22, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;While many recent safe RL methods with deep policies can achieve outstanding constraint satisfaction in noise-free simulation environment, such a concern regarding their vulnerability under adversarial perturbation has not been studies in the safe RL setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we are the first to formally analyze the unique vulnerability of the optimal policy in safe RL under observational corruptions. We define the state-adversarial safe RL problem and investigate its fundamental properties. We show that optimal solutions of safe RL problems are theoretically vulnerable under observational adversarial attacks&lt;/li&gt;
&lt;li&gt;we show that existing adversarial attack algorithms focusing on minimizing agent rewards do not always work, and propose two effective attack algorithms with theoretical justifications &amp;ndash; one directly maximise the constraint violation cost, and one maximise the task reward to induce a tempting but risky policy.
&lt;ul&gt;
&lt;li&gt;Surprisingly, the maximum reward attack is very strong in inducing unsafe behaviors, both in theory and practice&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we propose an adversarial training algorithm with the proposed attackers and show contraction properties of their Bellman operators. Extensive experiments in continuous control tasks show that our method is more robust against adversarial perturbations in terms of constraint satisfaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Safe reinforcement learning definition&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ruben_majadas Disturbing Reinforcement Learning Agents With Corrupted Rewards 2021</title>
      <link>https://sino-huang.github.io/posts/ruben_majadas-disturbing-reinforcement-learning-agents-with-corrupted-rewards-2021/</link>
      <pubDate>Sat, 17 Dec 2022 00:38:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ruben_majadas-disturbing-reinforcement-learning-agents-with-corrupted-rewards-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Disturbing Reinforcement Learning Agents With Corrupted Rewards&lt;/li&gt;
&lt;li&gt;Author: Ruben Majadas et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Feb 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 17, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;recent works have shown how the performance of RL algorithm decreases under the influence of soft changes in the reward function.&lt;/li&gt;
&lt;li&gt;However, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning learning exploration strategy.&lt;/li&gt;
&lt;li&gt;it chooses a subclass of MDPs: episodic, stochastic goal-only rewards MDPs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;it demonstrated that smoothly crafting adversarial rewards are able to mislead the learner&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the policy that is learned using low exploration probability values is more robust to corrupt rewards.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;(though this conclusion seems valid only for the proposed experiment setting)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the agent is completely lost with attack probabilities higher that than p=0.4&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;deterministic goal only reward MDP&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jingkang_wang Reinforcement Learning With Perturbed Rewards 2020</title>
      <link>https://sino-huang.github.io/posts/jingkang_wang-reinforcement-learning-with-perturbed-rewards-2020/</link>
      <pubDate>Fri, 16 Dec 2022 20:48:51 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jingkang_wang-reinforcement-learning-with-perturbed-rewards-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Perturbed Rewards&lt;/li&gt;
&lt;li&gt;Author: Jingkang Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Feb 2020&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Dec 16, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned. Some experiments are used to support the algorithm (i.e., estimate the confusion matrix and revert) using existing techniques from the supervised learning (and crowdsourcing) literature.&lt;/li&gt;
&lt;li&gt;Limitation
&lt;ul&gt;
&lt;li&gt;reviewers had concerns over the scope / significance of this work, mostly about how the confusion matrix is learned. If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.&lt;/li&gt;
&lt;li&gt;Specifically, the work seems to be limited in two substantial ways, both related to how confusion matrix is learned
&lt;ul&gt;
&lt;li&gt;the reward function needs to be deterministic&lt;/li&gt;
&lt;li&gt;majority voting requires the number of states to be finite
&lt;ul&gt;
&lt;li&gt;the significance of this work is therefore limited to finite-state problems with deterministic rewards, which is quite restricted.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;overall, the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work. However, the exact problem setting is not completely clear in the paper, and the limitation of the technical contribution is somewhat unclear.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The SOTA PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;reward function is often perturbed&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
