<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Reinforcement Learning | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/tags/reinforcement-learning/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/tags/reinforcement-learning/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/tags/reinforcement-learning/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/tags/reinforcement-learning/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Reinforcement Learning" property="og:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" name="twitter:image"/>
<meta content="Reinforcement Learning" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/tags/">Tags</a></div>
<h1>
    Reinforcement Learning
    <a aria-label="RSS" href="/tags/reinforcement-learning/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="the belief desire intention model" loading="lazy" src="https://sino-huang.github.io/posts/jacob_andreas-language-models-as-agent-models-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jacob_andreas Language Models as Agent Models 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Language Models as Agent Models Author: Jacob Andreas Publish Year: 3 Dec 2022 Review Date: Sat, Dec 10, 2022 https://arxiv.org/pdf/2212.01681.pdf
Summary of paper Motivation during training, LMs have access only to the text of the documents, with no direct evidence of the internal states of the human agent that produce them. (kind of hidden MDP thing) this is a fact often used to argue that LMs are incapable of modelling goal-directed aspects of human language production and comprehension. The author stated that even in today’s non-robust and error-prone models – LM infer and use representations of fine-grained communicative intensions and more abstract beliefs and goals. Despite that limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally. In other words, the author said that language model can be used to communicate intention of human agent, and hence it can be treated as a agent model. Contribution the author claimed that in the course of performing next-word prediction in context, current LMs sometimes infer inappropriate, partial representations of beliefs ,desires and intentions possessed by the agent that produced the context, and other agents mentioned within it. Once these representations are inferred, they are causally linked to LM prediction, and thus bear the same relation to generated text that an intentional agent’s state bears to its communicative actions. The high-level goals of this paper are twofold: first, to outline a specific sense in which idealised language models can function as models of agent belief, desires and intentions; second, to highlight a few cases in which existing models appear to approach this idealization (and describe the ways in which they still fall short) Training on text alone produces ready-made models of the map from agent states to text; these models offer a starting point for language processing systems that communicate intentionally. Some key terms Current language model is bad
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-10 00:47:33 +1100 AEDT">December 10, 2022</span> · 3 min · 639 words · Sukai Huang</footer>
<a aria-label="post link to Jacob_andreas Language Models as Agent Models 2022" class="entry-link" href="https://sino-huang.github.io/posts/jacob_andreas-language-models-as-agent-models-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="MEME agent network architecture" loading="lazy" src="https://sino-huang.github.io/posts/steven_kapturowski-human-level-atari-200x-faster-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Steven_kapturowski Human Level Atari 200x Faster 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Human Level Atari 200x Faster Author: Steven Kapturowski et. al. DeepMind Publish Year: September 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2209.07550.pdf
Motivation Agent 57 came at the cost of poor data-efficiency , requiring nearly 80,000 million frames of experience to achieve. this one can achieve the same performance in 390 million frames Contribution Some key terms NFNet - Normalisation Free Network
https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee Batch normalisation – the bad it is expensive batch normalisation breaks the assumption of data independence NFNet applies 3 different techniques: Modified residual branches and convolutions with Scaled Weight standardisation Adaptive Gradient Clipping Architecture optimisation for improved accuracy and training speed. https://github.com/vballoli/nfnets-pytorch Previous Non-Image features
...</p>
</div>
<footer class="entry-footer"><span title="2022-10-05 23:22:01 +1100 AEDT">October 5, 2022</span> · 2 min · 357 words · Sukai Huang</footer>
<a aria-label="post link to Steven_kapturowski Human Level Atari 200x Faster 2022" class="entry-link" href="https://sino-huang.github.io/posts/steven_kapturowski-human-level-atari-200x-faster-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="CoBERL architecture" loading="lazy" src="https://sino-huang.github.io/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: CoBERL Contrastive BERT for Reinforcement Learning Author: Andrea Banino et. al. DeepMind Publish Year: Feb 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2107.05431.pdf
Motivation Contribution Some key terms Representation learning in reinforcement learning
motivation: if state information could be effectively extracted from raw observations it may then be possible to learn from there as fast as from states. however, given the often sparse reward signal coming from the environment, learning representations in RL has to be achieved with little to no supervision. approach types class 1: auxiliary self-supervised losses to accelerate the learning speed in model-free RL algorithm class 2: learn a world model and use this to collect imagined rollouts, which then act as extra data to train the RL algorithm reducing the samples required from the environment CoBERL is in class 1 ​	it uses both masked language modelling and contrastive learning RL using BERT architecture – RELIC
...</p>
</div>
<footer class="entry-footer"><span title="2022-10-05 23:04:49 +1100 AEDT">October 5, 2022</span> · 2 min · 258 words · Sukai Huang</footer>
<a aria-label="post link to Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022" class="entry-link" href="https://sino-huang.github.io/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="architecture" loading="lazy" src="https://sino-huang.github.io/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Sample Factory: Asynchronous Rl at Very High FPS Author: Alex Petrenko Publish Year: Oct, 2020 Review Date: Sun, Sep 25, 2022 Summary of paper Motivation Identifying performance bottlenecks
RL involves three workloads:
environment simulation inference backpropagation overall performance depends on the lowest workload In existing methods (A2C/PPO/IMPALA) the computational workloads are dependent -&gt; under-utilisation of the system resources. Existing high-throughput methods focus on distributed training, therefore introducing a lot of overhead such as networking serialisation, etc.
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-25 16:34:09 +1000 AEST">September 25, 2022</span> · 1 min · 154 words · Sukai Huang</footer>
<a aria-label="post link to Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020" class="entry-link" href="https://sino-huang.github.io/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="architecture diagram" loading="lazy" src="https://sino-huang.github.io/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text Based Games Author: Dongwon Kelvin Ryu et. al. Publish Year: ACL 2022 Review Date: Thu, Sep 22, 2022 Summary of paper Motivation Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action space. A fundamental challenges in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. So, we want to inject external commonsense knowledge into the agent during training when the agent is most uncertain about its next action. Contribution In addition to performance increase, the produced trajectory of actions exhibit lower perplexity, when tested with a pre-trained LM, indicating better closeness to human language. Some key terms Exploration efficiency
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-22 19:38:56 +1000 AEST">September 22, 2022</span> · 2 min · 276 words · Sukai Huang</footer>
<a aria-label="post link to Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022" class="entry-link" href="https://sino-huang.github.io/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Younggyo_seo Masked World Models for Visual Control 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Masked World Models for Visual Control 2022 Author: Younggyo Seo et. al. Publish Year: 2022 Review Date: Fri, Jul 1, 2022 https://arxiv.org/abs/2206.14244?context=cs.AI
https://sites.google.com/view/mwm-rl
Summary of paper Motivation TL:DR: Masked autoencoders (MAE) has emerged as a scalable and effective self-supervised learning technique. Can MAE be also effective for visual model-based RL? Yes! with the recipe of convolutional feature masking and reward prediction to capture fine-grained and task-relevant information.
Some key terms Decouple visual representation learning and dynamics learning
...</p>
</div>
<footer class="entry-footer"><span title="2022-07-01 12:03:57 +1000 AEST">July 1, 2022</span> · 2 min · 227 words · Sukai Huang</footer>
<a aria-label="post link to Younggyo_seo Masked World Models for Visual Control 2022" class="entry-link" href="https://sino-huang.github.io/posts/younggyo_seo-masked-world-models-for-visual-control-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Hao_hu Generalisable Episodic Memory for Drl 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Generalisable episodic memory for Deep Reinforcement Learning Author: Hao Hu et. al. Publish Year: Jun 2021 Review Date: April 2022 Summary of paper Motivation The author proposed Generalisable Episodic Memory (GEM), which effectively organises the state-action values of episodic memory in a generalisable manner and supports implicit planning on memorised trajectories.
so compared to traditional memory table, GEM learns a virtual memory table memorized by deep neural networks to aggregate similar state-action pairs that essentially have the same nature.
...</p>
</div>
<footer class="entry-footer"><span title="2022-04-07 12:12:20 +1000 AEST">April 7, 2022</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Hao_hu Generalisable Episodic Memory for Drl 2021" class="entry-link" href="https://sino-huang.github.io/posts/hao_hu-generalisable-episodic-memory-for-drl-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Ilya_kostrikov Offline Rl With Implicit Q Learning 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Offline Reinforcement Learning with Implicit Q-learning Author:Ilya Kostrikov et. al. Publish Year: 2021 Review Date: Mar 2022 Summary of paper Motivation conflict in offline reinforcement learning
offline reinforcement learning requires reconciling two conflicting aims:
learning a policy that improves over the behaviour policy (old policy) that collected the dataset while at the same time minimizing the deviation from the behaviour policy so as to avoid errors due to distributional shift (e.g., obtain out of distribution actions) -&gt; the challenge is how to constrain those unseen actions to be in-distribution. (meaning there is no explicit Q-function for actions, and thus the issue of unseen action is gone) all the previous solutions like 1. limit how far the new policy deviates from the behaviour policy and 2. assign low value to out of distribution actions impose a trade-off between how much the policy improve and how vulnerable it is to misestimation due to distributional shift.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-22 19:01:49 +1100 AEDT">March 22, 2022</span> · 4 min · Sukai Huang</footer>
<a aria-label="post link to Ilya_kostrikov Offline Rl With Implicit Q Learning 2021" class="entry-link" href="https://sino-huang.github.io/posts/ilya_kostrikov-offline-rl-with-implicit-q-learning-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Qinqing_zheng Online Decision Transformer 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Online Decision Transformer Author: Qinqing Zheng Publish Year: Feb 2022 Review Date: Mar 2022 Summary of paper Motivation the author proposed online Decision transformer (ODT), an RL algorithm based on sequence modelling that blends offline pretraining with online fine-tuning in a unified framework.
ODT builds on the decision transformer architecture previously introduced for offline RL
quantify exploration
compared to DT, they shifted from deterministic to stochastic policies for defining exploration objectives during the online phase. They quantify exploration via the entropy of the policy similar to max-ent RL frameworks.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-21 21:56:45 +1100 AEDT">March 21, 2022</span> · 4 min · Sukai Huang</footer>
<a aria-label="post link to Qinqing_zheng Online Decision Transformer 2022" class="entry-link" href="https://sino-huang.github.io/posts/qinqing_zheng-online-decision-transformer-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Machel_reid Can Wikipedia Help Offline Rl 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Can Wikipedia Help Offline Reinforcement Learning Author: Machel Reid et. al. Publish Year: Mar 2022 Review Date: Mar 2022 Summary of paper Motivation Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments.
Moreover, when the model is trained from scratch, it suffers from slow convergence speeds
In this paper, they look to take advantage of this formulation of reinforcement learning as sequence modelling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when fine tuned on offline RL tasks (control, games).
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-16 21:18:24 +1100 AEDT">March 16, 2022</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Machel_reid Can Wikipedia Help Offline Rl 2022" class="entry-link" href="https://sino-huang.github.io/posts/machel_reid-can-wikipedia-help-offline-rl-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Wenfeng_feng Extracting Action Sequences From Texts by Rl
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Extracting Action Sequences from Texts Based on Deep Reinforcement Learning Author: Wenfeng Feng et. al. Publish Year: Mar 2018 Review Date: Mar 2022 Summary of paper Motivation the author want to build a model that learns to directly extract action sequences without external tools like POS tagging and dependency parsing results…
Annotation dataset structure
example
Model
they exploit the framework to learn two models to predict action names and arguments respectively.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-15 14:40:38 +1100 AEDT">March 15, 2022</span> · 1 min · Sukai Huang</footer>
<a aria-label="post link to Wenfeng_feng Extracting Action Sequences From Texts by Rl" class="entry-link" href="https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Giuseppe_de_giacomo Foundations for Retraining Bolts Rl With Ltl 2019
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf Restraining Specification Author: Giuseppe De Giacomo et. al. Publish Year: 2019 Review Date: Mar 2022 Summary of paper The author investigated the concept of “restraining bolt” that can control the behaviour of learning agents.
Essentially, the way to control a RL agent is that the bolt provides additional rewards to the agent
Although this method is essentially the same as reward shaping (providing additional rewards to the agent), the contribution of this paper is
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-04 12:12:57 +1100 AEDT">March 4, 2022</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Giuseppe_de_giacomo Foundations for Retraining Bolts Rl With Ltl 2019" class="entry-link" href="https://sino-huang.github.io/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Joseph_kim Collaborative Planning With Encoding of High Level Strategies 2017
    </h2>
</header>
<div class="entry-content">
<p>please modify the following
[TOC]
Title: Collaborative Planning with Encoding of Users’ High-level Strategies Author: Joseph Kim et. al. Publish Year: 2017 Review Date: Mar 2022 Summary of paper Motivation Automatic planning is computationally expensive. Greedy search heuristics often yield low-quality plans that can result in wasted resources; also, even in the event that an adequate plan is generated, users may have difficulty interpreting the reason why the plan performs well and trusting it.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-04 12:12:27 +1100 AEDT">March 4, 2022</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Joseph_kim Collaborative Planning With Encoding of High Level Strategies 2017" class="entry-link" href="https://sino-huang.github.io/posts/joseph_kim-collaborative-planning-with-encoding-of-high-level-strategies-2017/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Mikayel_samvelyan Minihack the Planet a Sandbox for Open Ended Rl Research 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research Author: Mikayel Samvelyan et. al. Publish Year: Nov 2021 Review Date: Mar 2022 Summary of paper They presented MiniHack, an easy-to-use framework for creating rich and varied RL environments, as well as a suite of tasks developed using this framework. Built upon NLE and the des-file format, MiniHack enables the use of rich entities and dynamics from the game of NetHack to create a large variety of RL environments for targeted experimentation, while also allowing painless scaling-up of the difficulty of existing environments. MiniHack’s environments are procedurally generated by default, ensuring the evaluation of systematic generalization of RL agents.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-04 12:11:55 +1100 AEDT">March 4, 2022</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Mikayel_samvelyan Minihack the Planet a Sandbox for Open Ended Rl Research 2021" class="entry-link" href="https://sino-huang.github.io/posts/mikayel_samvelyan-minihack-the-planet-a-sandbox-for-open-ended-rl-research-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Richard_shin Constrained Language Models Yield Few Shot Semantic Parsers 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Constrained Language models yield few-shot semantic parsers Author: Richard Shin et. al. Publish Year: Nov 2021 Review Date: Mar 2022 Summary of paper Motivation The author wanted to explore the use of large pretrained language models as few-shot semantic parsers
However, language models are trained to generate natural language. To bridge the gap, they used language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. (using synchronous context-free grammar SCFG)
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-02 00:19:18 +1100 AEDT">March 2, 2022</span> · 1 min · Sukai Huang</footer>
<a aria-label="post link to Richard_shin Constrained Language Models Yield Few Shot Semantic Parsers 2021" class="entry-link" href="https://sino-huang.github.io/posts/richard_shin-constrained-language-models-yield-few-shot-semantic-parsers-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Heinrich_kuttler the Nethack Learning Environment 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: The NetHack Learning Environment Author: Heinrich Kuttler et. al. Publish Year: Dec 2020 Review Date: Mar 2022 Summary of paper The NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack.
NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-02 00:18:35 +1100 AEDT">March 2, 2022</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Heinrich_kuttler the Nethack Learning Environment 2020" class="entry-link" href="https://sino-huang.github.io/posts/heinrich_kuttler-the-nethack-learning-environment-2020/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021
    </h2>
</header>
<div class="entry-content">
<p>please modify the following
[TOC]
Title: LTL2Action: Generalizing LTL Instructions for Multi-Task RL Author: Pashootan Vaezipoor et. al. Publish Year: 2021 Review Date: March 2022 Summary of paper Motivation they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language – linear temporal logic (LTL)
Limitation of the vanilla MDP
temporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards.
...</p>
</div>
<footer class="entry-footer"><span title="2022-03-01 20:53:10 +1100 AEDT">March 1, 2022</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021" class="entry-link" href="https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Roma_patel Learning to Ground Language Temporal Logical Form 2019
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Learning to Ground Language to Temporal Logical Form Author: Roma Patel et. al. Publish Year: 2019 Review Date: Feb 2022 Summary of paper Motivation natural language commands often exhibits sequential (temporal) constraints e.g., “go through the kitchen and then into the living room”.
But this constraints cannot be expressed in the reward of Markov Decision Process setting. (see this paper)
Therefore, they proposed to ground language to Linear Temporal logic (LTL) and after that continue to map from LTL expressions to action sequences.
...</p>
</div>
<footer class="entry-footer"><span title="2022-02-28 21:40:53 +1100 AEDT">February 28, 2022</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Roma_patel Learning to Ground Language Temporal Logical Form 2019" class="entry-link" href="https://sino-huang.github.io/posts/roma_patel-learning-to-ground-language-temporal-logical-form-2019/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Tianshi_cao Babyai Plus Plus Towards Grounded Language Learning Beyond Memorization 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: BABYAI++: Towards Grounded-Language Learning Beyond Memorization Author: Tianshi Cao et. al. Publish Year: 2020 ICLR Review Date: Jan 2022 Summary of paper The paper introduced a new RL environment BabyAI++ that can investigate whether RL agents can extract knowledge from descriptive text and eventually increase generalisation performance.
BabyAI++ environment example
the descriptive text describe the feature of the object. notice that the feature of object can easily change as we change the descriptive text. Model
...</p>
</div>
<footer class="entry-footer"><span title="2022-01-03 22:38:40 +1100 AEDT">January 3, 2022</span> · 1 min · Sukai Huang</footer>
<a aria-label="post link to Tianshi_cao Babyai Plus Plus Towards Grounded Language Learning Beyond Memorization 2020" class="entry-link" href="https://sino-huang.github.io/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Lili_chen Decision Transformer Reinforcement Learning via Sequence Modeling 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Decision Transformer: Reinforcement Learning via Sequence Modeling Author: Lili Chen et. al. Publish Year: Jun 2021 Review Date: Dec 2021 Summary of paper The Architecture of Decision Transformer
Inputs are reward, observation and action
Outputs are action, in training time, the future action will be masked out.
I believe this model is able to generate a very good long sequence of actions due to transformer architecture.
But somehow this is not RL anymore because the transformer is not trained by reward signal …
...</p>
</div>
<footer class="entry-footer"><span title="2021-12-24 23:29:49 +1100 AEDT">December 24, 2021</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Lili_chen Decision Transformer Reinforcement Learning via Sequence Modeling 2021" class="entry-link" href="https://sino-huang.github.io/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Yiding_jiang Language as Abstraction for Hierarchical Deep Reinforcement Learning
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Language as an Abstraction for Hierarchical Deep Reinforcement Learning Author: Yiding Jiang et. al. Publish Year: 2019 NeurIPS Review Date: Dec 2021 Summary of paper Solving complex, temporally-extended tasks is a long-standing problem in RL.
Acquiring effective yet general abstractions for hierarchical RL is remarkably challenging.
Therefore, they propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalisation
...</p>
</div>
<footer class="entry-footer"><span title="2021-12-15 19:49:28 +1100 AEDT">December 15, 2021</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Yiding_jiang Language as Abstraction for Hierarchical Deep Reinforcement Learning" class="entry-link" href="https://sino-huang.github.io/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">David_abel on the Expressivity of Markov Reward 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: On the Expressivity of Markov Reward Author: David Abel et. al. Publish Year: NuerIPS 2021 Review Date: 6 Dec 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.
The author found out that in the Markov Decision Process scenario, (i.e., we do not look at the history of the trajectory to provide rewards), some tasks cannot be realised perfectly by reward functions. i.e.,
...</p>
</div>
<footer class="entry-footer"><span title="2021-12-05 12:02:23 +1100 AEDT">December 5, 2021</span> · 5 min · Sukai Huang</footer>
<a aria-label="post link to David_abel on the Expressivity of Markov Reward 2021" class="entry-link" href="https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Rishabh_agarwal Deep Reinforcement Learning at the Edge of the Stats Precipice 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Deep Reinforcement Learning at the Edge of the Statistical Precipice Author: Rishabh Agarwal et. al. Publish Year: NeurIPS 2021 Review Date: 3 Dec 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.
Most current published results on deep RL benchmarks uses point estimate of aggregate performance such as mean and median score across the task.
...</p>
</div>
<footer class="entry-footer"><span title="2021-12-03 19:50:10 +1100 AEDT">December 3, 2021</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Rishabh_agarwal Deep Reinforcement Learning at the Edge of the Stats Precipice 2021" class="entry-link" href="https://sino-huang.github.io/posts/rishabh_agarwal-deep-reinforcement-learning-at-the-edge-of-the-stats-precipice-2021/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Borja_ibarz Reward Learning From Human Preferences and Demonstrations in Atari 2018
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Reward learning from human preferences and demonstractions in Atari Author: Borja Ibarz et. al. Publish Year: 2018 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.
The author proposed a method that uses human expert’s annotation rather than extrinsic reward from the environment to guide the reinforcement learning.
...</p>
</div>
<footer class="entry-footer"><span title="2021-11-27 19:14:04 +1100 AEDT">November 27, 2021</span> · 2 min · Sukai Huang</footer>
<a aria-label="post link to Borja_ibarz Reward Learning From Human Preferences and Demonstrations in Atari 2018" class="entry-link" href="https://sino-huang.github.io/posts/borja_ibarz-reward-learning-from-human-preferences-and-demonstrations-in-atari-2018/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Adrien_ecoffet Go Explore a New Approach for Hard Exploration Problems 2021 Paper Review
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Go-Explore: a New Approach for Hard-Exploration Problems Author: Adrien Ecoffet et. al. Publish Year: 2021 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.
The author hypothesised that there are two main issues that prevent DRL agents from achieving high score in exploration-hard game (e.g., Montezuma’s Revenge)
...</p>
</div>
<footer class="entry-footer"><span title="2021-11-27 18:58:32 +1100 AEDT">November 27, 2021</span> · 4 min · Sukai Huang</footer>
<a aria-label="post link to Adrien_ecoffet Go Explore a New Approach for Hard Exploration Problems 2021 Paper Review" class="entry-link" href="https://sino-huang.github.io/posts/adrien_ecoffet-go-explore-a-new-approach-for-hard-exploration-problems-2021-paper-review/"></a>
</article>
<footer class="page-footer">
<ul class="post-tags">
</ul>
<nav class="pagination">
<a class="prev" href="https://sino-huang.github.io/tags/reinforcement-learning/">
      « Prev 1/3
    </a>
<a class="next" href="https://sino-huang.github.io/tags/reinforcement-learning/page/3/">Next 3/3 »
    </a>
</nav>
</footer>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
