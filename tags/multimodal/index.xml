<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Multimodal on Sukai Huang</title>
    <link>https://sino-huang.github.io/tags/multimodal/</link>
    <description>Recent content in Multimodal on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 28 Aug 2023 18:48:08 +1000</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Junnan_li Blip2 Boostrapping Language Image Pretraining 2023</title>
      <link>https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/</link>
      <pubDate>Mon, 28 Aug 2023 18:48:08 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  BLIP2 - Boostrapping Language Image Pretraining 2023&lt;/li&gt;
&lt;li&gt;Author: Junnan Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 15 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Aug 28, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.12597.pdf&#34;&gt;https://arxiv.org/pdf/2301.12597.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper titled &amp;ldquo;BLIP-2&amp;rdquo; proposes a new and efficient pre-training strategy for vision-and-language models. The cost of training such models has been increasingly prohibitive due to the large scale of the models. BLIP-2 aims to address this issue by leveraging off-the-shelf, pre-trained image encoders and large language models (LLMs) that are kept frozen during the pre-training process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Peng_gao Llama Adapter V2 2023</title>
      <link>https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/</link>
      <pubDate>Mon, 28 Aug 2023 18:47:05 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Llama Adapter V2&lt;/li&gt;
&lt;li&gt;Author: Peng Gao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 28 Apr 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Aug 28, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2304.15010.pdf&#34;&gt;https://arxiv.org/pdf/2304.15010.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper presents LLaMA-Adapter V2, an enhanced version of the original LLaMA-Adapter designed for multi-modal reasoning and instruction following. The paper aims to address the limitations of the original LLaMA-Adapter, which could not generalize well to open-ended visual instructions and lagged behind GPT-4 in performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021</title>
      <link>https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/</link>
      <pubDate>Sat, 03 Sep 2022 17:17:47 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: VinVL: Revisiting Visual Representations in Vision Language Models&lt;/li&gt;
&lt;li&gt;Author: Pengchuan Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Mar 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Sep 3, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In our experiments we feed the visual features generated by &lt;strong&gt;the new object detection model&lt;/strong&gt; into a Transformer-based VL fusion model Oscar.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And utilise an improved approach OSCAR + to pretrain the VL model&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;has a bigger Object Detection model with larger amount of training data, called &amp;ldquo;ResNeXt-152 C4&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Vision Language Pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020</title>
      <link>https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/</link>
      <pubDate>Sat, 03 Sep 2022 17:12:54 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks&lt;/li&gt;
&lt;li&gt;Author: Xiujun Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 26 Jul 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Sep 3, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Existing method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiali_duan Multimodal Alignment Using Representation Codebook 2022</title>
      <link>https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/</link>
      <pubDate>Tue, 09 Aug 2022 07:26:46 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Multi-modal Alignment Using Representation Codebook&lt;/li&gt;
&lt;li&gt;Author: Jiali Duan, Liqun Chen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022 CVPR&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Aug 9, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion.&lt;/li&gt;
&lt;li&gt;since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we treat image and text as two &amp;ldquo;views&amp;rdquo; of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook).&lt;/li&gt;
&lt;li&gt;to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Types of Vision language pre-training tasks&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022</title>
      <link>https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/</link>
      <pubDate>Wed, 11 May 2022 16:35:03 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Flamingo: a Visual Language Model for Few-Shot Learning&lt;/li&gt;
&lt;li&gt;Author: Jean-Baptiste Alayrac et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Apr 2022&lt;/li&gt;
&lt;li&gt;Review Date: May 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;flamingo-architecture&#34;&gt;Flamingo architecture&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pretrained&lt;/strong&gt; &lt;strong&gt;vision encoder: from pixels to features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the model&amp;rsquo;s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)&lt;/p&gt;
&lt;p&gt;they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper &amp;ldquo;Learning Transferable Visual Models From Natural Language Supervision&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Junyang_lin M6 a Chinese Multimodal Pretrainer 2021</title>
      <link>https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/</link>
      <pubDate>Wed, 12 Jan 2022 13:38:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: M6: A Chinese Multimodal Pretrainer&lt;/li&gt;
&lt;li&gt;Author: Junyang Lin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: May 2021&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;This paper re-emphasises that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large model trained on big data have extremely large capacity and it can outperform the SOTA in downstream tasks especially in the zero-shot setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the author trained a big multi-modal model&lt;/p&gt;
&lt;p&gt;Also, they proposed a innovative way to tackle downstream tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they use masks to block cross attention between tokens so as to fit different types of downstream task&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key idea: mask tokens during cross attention so as to solve certain tasks&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
