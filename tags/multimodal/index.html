<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Multimodal | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/tags/multimodal/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/tags/multimodal/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/tags/multimodal/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/tags/multimodal/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Multimodal" property="og:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" name="twitter:image"/>
<meta content="Multimodal" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/tags/">Tags</a></div>
<h1>
    Multimodal
    <a aria-label="RSS" href="/tags/multimodal/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Junnan_li Blip2 Boostrapping Language Image Pretraining 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: BLIP2 - Boostrapping Language Image Pretraining 2023 Author: Junnan Li et. al. Publish Year: 15 Jun 2023 Review Date: Mon, Aug 28, 2023 url: https://arxiv.org/pdf/2301.12597.pdf Summary of paper The paper titled “BLIP-2” proposes a new and efficient pre-training strategy for vision-and-language models. The cost of training such models has been increasingly prohibitive due to the large scale of the models. BLIP-2 aims to address this issue by leveraging off-the-shelf, pre-trained image encoders and large language models (LLMs) that are kept frozen during the pre-training process.
...</p>
</div>
<footer class="entry-footer"><span title="2023-08-28 18:48:08 +1000 AEST">August 28, 2023</span> · 2 min · 327 words · Sukai Huang</footer>
<a aria-label="post link to Junnan_li Blip2 Boostrapping Language Image Pretraining 2023" class="entry-link" href="https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Peng_gao Llama Adapter V2 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Llama Adapter V2 Author: Peng Gao et. al. Publish Year: 28 Apr 2023 Review Date: Mon, Aug 28, 2023 url: https://arxiv.org/pdf/2304.15010.pdf Summary of paper The paper presents LLaMA-Adapter V2, an enhanced version of the original LLaMA-Adapter designed for multi-modal reasoning and instruction following. The paper aims to address the limitations of the original LLaMA-Adapter, which could not generalize well to open-ended visual instructions and lagged behind GPT-4 in performance.
...</p>
</div>
<footer class="entry-footer"><span title="2023-08-28 18:47:05 +1000 AEST">August 28, 2023</span> · 2 min · 246 words · Sukai Huang</footer>
<a aria-label="post link to Peng_gao Llama Adapter V2 2023" class="entry-link" href="https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="add object detection pretrain model" loading="lazy" src="https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: VinVL: Revisiting Visual Representations in Vision Language Models Author: Pengchuan Zhang et. al. Publish Year: 10 Mar 2021 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model Oscar.
And utilise an improved approach OSCAR + to pretrain the VL model
Contribution has a bigger Object Detection model with larger amount of training data, called “ResNeXt-152 C4” Some key terms Vision Language Pretraining
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-03 17:17:47 +1000 AEST">September 3, 2022</span> · 2 min · 332 words · Sukai Huang</footer>
<a aria-label="post link to Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021" class="entry-link" href="https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="illustration of Oscar model" loading="lazy" src="https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks Author: Xiujun Li et. al. Publish Year: 26 Jul 2020 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation Existing method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.
the lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-03 17:12:54 +1000 AEST">September 3, 2022</span> · 3 min · 462 words · Sukai Huang</footer>
<a aria-label="post link to Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020" class="entry-link" href="https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="learnable codebook" loading="lazy" src="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jiali_duan Multimodal Alignment Using Representation Codebook 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Multi-modal Alignment Using Representation Codebook Author: Jiali Duan, Liqun Chen et. al. Publish Year: 2022 CVPR Review Date: Tue, Aug 9, 2022 Summary of paper Motivation aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion. since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. Contribution in this paper, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook). to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. Some key terms Types of Vision language pre-training tasks
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-09 07:26:46 +1000 AEST">August 9, 2022</span> · 3 min · 513 words · Sukai Huang</footer>
<a aria-label="post link to Jiali_duan Multimodal Alignment Using Representation Codebook 2022" class="entry-link" href="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Flamingo: a Visual Language Model for Few-Shot Learning Author: Jean-Baptiste Alayrac et. al. Publish Year: Apr 2022 Review Date: May 2022 Summary of paper Flamingo architecture Pretrained vision encoder: from pixels to features
the model’s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)
they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper “Learning Transferable Visual Models From Natural Language Supervision”
...</p>
</div>
<footer class="entry-footer"><span title="2022-05-11 16:35:03 +1000 AEST">May 11, 2022</span> · 3 min · Sukai Huang</footer>
<a aria-label="post link to Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022" class="entry-link" href="https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/"></a>
</article>
<article class="post-entry tag-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Junyang_lin M6 a Chinese Multimodal Pretrainer 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: M6: A Chinese Multimodal Pretrainer Author: Junyang Lin et. al. Publish Year: May 2021 Review Date: Jan 2022 Summary of paper This paper re-emphasises that
large model trained on big data have extremely large capacity and it can outperform the SOTA in downstream tasks especially in the zero-shot setting. So, the author trained a big multi-modal model
Also, they proposed a innovative way to tackle downstream tasks.
they use masks to block cross attention between tokens so as to fit different types of downstream task Key idea: mask tokens during cross attention so as to solve certain tasks Overview
...</p>
</div>
<footer class="entry-footer"><span title="2022-01-12 13:38:14 +1100 AEDT">January 12, 2022</span> · 1 min · Sukai Huang</footer>
<a aria-label="post link to Junyang_lin M6 a Chinese Multimodal Pretrainer 2021" class="entry-link" href="https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/"></a>
</article>
<footer class="page-footer">
<ul class="post-tags">
</ul>
<nav class="pagination">
</nav>
</footer>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
