<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Llm Weak Supervision on Sukai Huang</title>
    <link>https://sino-huang.github.io/tags/llm-weak-supervision/</link>
    <description>Recent content in Llm Weak Supervision on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 29 Jan 2024 15:32:21 +1100</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/tags/llm-weak-supervision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision</title>
      <link>https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/</link>
      <pubDate>Mon, 29 Jan 2024 15:32:21 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision&lt;/li&gt;
&lt;li&gt;Author: Collin Burns et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 14 Dec 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2312.09390v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240129161843295&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Superalignment: OPENAI believe that RLHF is essentially use human to supervise the model (RM is trained by human annotation). One day when superhuman models come out, human are no longer to annotate the good / bad of the model&amp;rsquo;s output. e.g., superhuman model generate a 1M lines complex code and human cannot review it.&lt;/li&gt;
&lt;li&gt;How to do the alignment in for this case?&lt;/li&gt;
&lt;li&gt;thus the research question is can we use a weak teacher model to improve strong student model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they used weak model to generate annotations and fine tune the strong model, they empirically did a lot of experiments&lt;/li&gt;
&lt;li&gt;note: although they use the term teacher and student, the alignment task is not about &amp;ldquo;teaching&amp;rdquo;, alignment is to elicit learnt stuffs from strong foundation model (something like finetuning), rather than asking strong model to follow weak teacher model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
