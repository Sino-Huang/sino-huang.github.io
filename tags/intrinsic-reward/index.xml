<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Intrinsic Reward on Sukai Huang</title>
    <link>https://sino-huang.github.io/tags/intrinsic-reward/</link>
    <description>Recent content in Intrinsic Reward on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Fri, 12 Apr 2024 15:12:54 +1000</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/tags/intrinsic-reward/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Daniel Hierarchies of Reward Machines 2023</title>
      <link>https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/</link>
      <pubDate>Fri, 12 Apr 2024 15:12:54 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Hierarchies of Reward Machines&lt;/li&gt;
&lt;li&gt;Author: Daniel Furelos-Blanco et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 4 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Apr 12, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2205.15752&#34;&gt;https://arxiv.org/abs/2205.15752&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240412151420609&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Finite state machine are a simple yet powerful formalism for abstractly representing temporal tasks in a structured manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The work introduces Hierarchies of Reinforcement Models (HRMs) to enhance the abstraction power of existing models. Key contributions include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HRM Abstraction Power&lt;/strong&gt;: HRMs allow for the creation of hierarchies of Reinforcement Models (RMs), enabling constituent RMs to call other RMs. It&amp;rsquo;s proven that any HRM can be converted into an equivalent flat HRM with identical behavior. However, the equivalent flat HRM can have significantly more states and edges, especially under specific conditions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shanchuan Efficient N Robust Exploration Through Discriminative Ir 2023</title>
      <link>https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/</link>
      <pubDate>Fri, 12 Apr 2024 15:07:58 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards&lt;/li&gt;
&lt;li&gt;Author: Shanchuan Wan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Apr 12, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2304.10770&#34;&gt;https://arxiv.org/abs/2304.10770&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240412151513920&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recent studies have shown the effectiveness of encouraging exploration with intrinsic rewards estimated from novelties in observations&lt;/li&gt;
&lt;li&gt;However, there is a gap between the novelty of an observation and an exploration, as both the stochasticity in the environment and agent&amp;rsquo;s behaviour may affect the observation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose DEIR, a novel method in which we theoretically derive an intrinsic reward with a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and then implement the reward
with a discriminative forward model.&lt;/li&gt;
&lt;li&gt;want to design a novel intrinsic reward design that considers not only the observed novelty but also the effective contribution brought by the agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;internal rewards&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
