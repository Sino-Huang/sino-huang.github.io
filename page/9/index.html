<!DOCTYPE html>
<html dir="auto" lang="en">
<head>
<meta content="Hugo 0.139.2" name="generator"/><script data-no-instant="" defer="" src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload"></script><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Sukai Huang</title>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="http://localhost:1313/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="http://localhost:1313/favicon.ico" rel="icon"/>
<link href="http://localhost:1313/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="http://localhost:1313/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="http://localhost:1313/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="http://localhost:1313/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="http://localhost:1313/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="http://localhost:1313/index.json" rel="alternate" type="application/json"/>
<link href="http://localhost:1313/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="http://localhost:1313/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Sukai Huang" property="og:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="http://localhost:1313/cute_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="http://localhost:1313/cute_avatar.jpg" name="twitter:image"/>
<meta content="Sukai Huang" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Sukai Huang",
  "url": "http://localhost:1313/",
  "description": "Sukai's academic blog - storing weekly reports and research paper reviews",
  "logo": "http://localhost:1313/favicon.ico",
  "sameAs": [
      "https://github.com/Sino-Huang", "mailto:sukaih@student.unimelb.edu.au", "https://au.linkedin.com/in/sukai-huang-683368169"
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="http://localhost:1313/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="http://localhost:1313/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="http://localhost:1313/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="http://localhost:1313/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="http://localhost:1313/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="http://localhost:1313/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="http://localhost:1313/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Sanchit_agarwal Building Goal Oriented Dialogue Systems With Situated Visual Context 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Building Goal Oriented Dialogue Systems With Situated Visual Context 2021 Author: Sanchit Agarwal et. al. Publish Year: 22 Nov 2021 Review Date: Sun, Nov 20, 2022 Summary of paper Motivation with the surge of virtual assistants with screen, the next generation of agents are required to also understand screen context in order to provide a proper interactive experience, and better understand users’ goals. So in this paper, they propose a novel multimodal conversational framework, where the agent’s next action and their arguments are derived jointly conditioned on the conversational and the visual context. The model can recognise visual features such as color and shape as well as the metadata based features such as price or star rating associated with a visual entity. Contribution propose a novel multimodal conversational system that considers screen context, in addition to dialogue context, while deciding the agent’s next action The proposed visual grounding model takes both metadata and images as input allowing it to reason over metadata and visual information Our solution encodes the user query and each visual entities and then compute the similarity between them. to improve the visual entity encoding, they introduced query guided attention and entity self-attention layers. collect the MTurk survey and also create a multimodal dialogue simulator Architecture ...</p>
</div>
<footer class="entry-footer"><span title="2022-11-20 16:29:14 +1100 AEDT">November 20, 2022</span> · 1 min · 211 words · Sukai Huang</footer>
<a aria-label="post link to Sanchit_agarwal Building Goal Oriented Dialogue Systems With Situated Visual Context 2021" class="entry-link" href="http://localhost:1313/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="http://localhost:1313/posts/yichi_zhang-danli-deliberative-agent-for-following-natural-language-instructions-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yichi_zhang Danli Deliberative Agent for Following Natural Language Instructions 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: DANLI: Deliberative Agent for Following Natural Language Instructions Author: Yichi Zhang Publish Year: 22 Oct, 2022 Review Date: Sun, Nov 20, 2022 Summary of paper Motivation reactive agent simply learn and imitate behaviours encountered in the training data these reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from the past experience. Contribution We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark Some key terms Natural language instruction following with embodied AI agents
...</p>
</div>
<footer class="entry-footer"><span title="2022-11-20 16:28:23 +1100 AEDT">November 20, 2022</span> · 2 min · 343 words · Sukai Huang</footer>
<a aria-label="post link to Yichi_zhang Danli Deliberative Agent for Following Natural Language Instructions 2022" class="entry-link" href="http://localhost:1313/posts/yichi_zhang-danli-deliberative-agent-for-following-natural-language-instructions-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="http://localhost:1313/posts/xiang_li-diffusion_lm_improves_controllable_text_generation_2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xiang_li Diffusion-LM Improves Controllable Text Generation 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Diffusion-LM Improves Controllable Text Generation Author: Xiang Lisa Li Publish Year: May 2022 Review Date: Mon, Nov 14, 2022 https://arxiv.org/pdf/2205.14217.pdf
Summary of paper Motivation can language tokens be represented as floating number? they develop a new non-autoregressive language model based on continuous diffusion Diffusion LM iteratively denoises as sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variable. how to convert from continuous embeddings back to words they used rounding and many other tricks to stabilise the training process Contribution they tried diffusion model for Language Model Incomprehension Not sure if the model is good at text generation.
...</p>
</div>
<footer class="entry-footer"><span title="2022-11-14 16:32:31 +1100 AEDT">November 14, 2022</span> · 1 min · 104 words · Sukai Huang</footer>
<a aria-label="post link to Xiang_li Diffusion-LM Improves Controllable Text Generation 2022" class="entry-link" href="http://localhost:1313/posts/xiang_li-diffusion_lm_improves_controllable_text_generation_2022/"></a>
</article>
<article class="post-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Consider incremental publication of results Nov, 2022
    </h2>
</header>
<div class="entry-content">
<p>You need password to access to the content, go to Slack *#phdsukai to find more.
...</p>
</div>
<footer class="entry-footer"><span title="2022-11-13 15:59:12 +1100 AEDT">November 13, 2022</span> · 7 min · Sukai Huang</footer>
<a aria-label="post link to Consider incremental publication of results Nov, 2022" class="entry-link" href="http://localhost:1313/posts/paper_proposal_nov_2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="Relatedness and naturalness" loading="lazy" src="http://localhost:1313/posts/jie_huang-can-language-models-be-specific-how-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jie_huang Can Language Models Be Specific How 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Can Language Models Be Specific? How? Author: Jie Huang et. al. Publish Year: 11 Oct 2022 Review Date: Tue, Nov 8, 2022 Summary of paper Motivation they propose to measure how specific the language of pre-trained language models (PLM) is, To achieve this, they introduced a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. for instance given “J.K. Rowling was born in [MASK]”, we want to test whether a more specific answer will be better filled by PLMs. e.g., Yate instead of England it is known that if the prediction is more specific, we can retrieve more fine-grained information from language models, and further acquire more information. viewer’s opinion: we are not saying that summarisation is easy or having less useful information, there are cases that abstract info is more useful Contribution although there are works on measuring how much knowledge is stored in PLMs or improving the correctness of the predictions, non attempted to measure or improve the specificity of prediction made by PLMs. Understanding how specific the language of PLMs is can help us better understand the behaviour of language models and facilitate downstream applications such as question answering etc. setup a dataset benchmark for specificity, The quality of the benchmark is high, where the judgment on which answer is more specific is ∼ 97% consistent with humans. Discovery in general, PLMs prefer less specific answers without subjects given, and they only have a weak ability to differentiate coarse-grained/fine-grained objects by measuring their (cosine) similarities to subjects. the results indicate that specificity was neglected by existing research on language models Improving specificity of the prediction few-shot prompting
...</p>
</div>
<footer class="entry-footer"><span title="2022-11-08 20:41:04 +1100 AEDT">November 8, 2022</span> · 3 min · 429 words · Sukai Huang</footer>
<a aria-label="post link to Jie_huang Can Language Models Be Specific How 2022" class="entry-link" href="http://localhost:1313/posts/jie_huang-can-language-models-be-specific-how-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="http://localhost:1313/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yizhou_zhao Semantic Aligned Fusion Transformer for One Shot Object Detection 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Semantic-Aligned Fusion Transformer for One Shot Object Detection Author: Yizhou Zhao et. al. Publish Year: 2022 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/2203.09093v2.pdf
Summary of paper Motivation with extreme data scarcity, current approaches, explore various feature fusions to obtain directly transferable meta-knowledge in this paper, they, attribute the previous limitation to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structure and scale variances. </p>
</div>
<footer class="entry-footer"><span title="2022-10-24 19:14:34 +1100 AEDT">October 24, 2022</span> · 1 min · 67 words · Sukai Huang</footer>
<a aria-label="post link to Yizhou_zhao Semantic Aligned Fusion Transformer for One Shot Object Detection 2022" class="entry-link" href="http://localhost:1313/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="architecture" loading="lazy" src="http://localhost:1313/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Ting_i_hsieh One Shot Object Detection With Co Attention and Co Excitation 2019
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: One-Shot Object Detection With Co-Attention and Co-Excitation Author: Ting-I Hsieh et. al. Publish Year: Nov 2019 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/1911.12529.pdf
Summary of paper Motivation this paper aims to tackle the challenging problem of one-shot object detection, Given a query image patch whose class label is not included in the training data, To this end, they developed a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects first, use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasise correlated feature channels to help uncover relevant object proposals and eventually the target objects third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen training. ...</p>
</div>
<footer class="entry-footer"><span title="2022-10-24 19:13:10 +1100 AEDT">October 24, 2022</span> · 1 min · 158 words · Sukai Huang</footer>
<a aria-label="post link to Ting_i_hsieh One Shot Object Detection With Co Attention and Co Excitation 2019" class="entry-link" href="http://localhost:1313/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="architecture" loading="lazy" src="http://localhost:1313/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Ayan_kumar_bhunia a Deep One Shot Network for Query Based Logo Retrieval 2019
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: A Deep-One Shot Network for Query-Based Logo Retrieval Author: Ayan Kumar Bhunia et. al. Publish Year: Jul 2019 Review Date: Mon, Oct 24, 2022 https://arxiv.org/pdf/1811.01395.pdf
Summary of paper Motivation Existing general purpose just cannot handle unseen new logos (not labelled logos) in this work, they developed an easy-to-implement query based logo detection and localisation system by employing a one-shot learning technique using off-the-shelf neural network components. Limitation of current work Deep-learning based framework are largely data-driven, contrary to logo-dataset that have several image classes but few images. need to be robust to new unseen logos, the model should be designed to satisfy the incremental demands for logo classes, contrary to existing methods which are limited to a set of seen logos and are not. Contribution propose a scalable solution for the logo detection problem, they present a query-based logo search and detection system by employing a simple fully differentiable one-shot learning framework which can be used for new logo classes without further training the whole network. to deal with the logos of varying sizes, we propose a novel one-shot framework through multi-scale conditioning that is specially designed to learn the similarity between the query image and target image at multiple scales and resolutions. Architecture ...</p>
</div>
<footer class="entry-footer"><span title="2022-10-24 19:12:22 +1100 AEDT">October 24, 2022</span> · 2 min · 258 words · Sukai Huang</footer>
<a aria-label="post link to Ayan_kumar_bhunia a Deep One Shot Network for Query Based Logo Retrieval 2019" class="entry-link" href="http://localhost:1313/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="overall architecture" loading="lazy" src="http://localhost:1313/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yuetian_weng an Efficient Spatio Temporal Pyramid Transformer for Action Detection 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: An Efficient Spatio-Temporal Pyramid Transformer for Action Detection Author: Yuetian Weng et. al. Publish Year: Jul 2022 Review Date: Thu, Oct 20, 2022 Summary of paper Motivation the task of action detection aims at deducing both the action category and localisation of the start and end moment for each action instance in a long, untrimmed video. it is non-trivial to design an efficient architecture for action detection due to the prohibitively expensive self-attentions over a long sequence of video clips To this end, they present an efficient hierarchical spatial temporal transformer for action detection Building upon the fact that the early self-attention layer in Transformer still focus on local patterns. Background to date, the majority of action detection methods are driven by 3D convolutional neural networks (CNNs), e.g., C3D, I3D, to encode video segment features from video RGB frames and optical flows however, the limited receptive field hinders the CNN-based models to capture long-term spatio-temporal dependencies. alternatively, vision transformers have shown the advantage of capturing global dependencies via the self-attention mechanism. Hierarchical ViTs divide Transformer blocks into several stages and progressively reduce the spatial size of feature maps when the network goes deeper. but having self-attention over a sequence of images is expensive also they found out that the global attention in the early layers actually only encodes local visual pattens (i.e., it only attends to its nearby tokens in adjacent frames while rarely interacting with tokens in distance frames) Efficient Spatio-temporal Pyramid Transformer ...</p>
</div>
<footer class="entry-footer"><span title="2022-10-20 19:06:41 +1100 AEDT">October 20, 2022</span> · 4 min · 649 words · Sukai Huang</footer>
<a aria-label="post link to Yuetian_weng an Efficient Spatio Temporal Pyramid Transformer for Action Detection 2022" class="entry-link" href="http://localhost:1313/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="MEME agent network architecture" loading="lazy" src="http://localhost:1313/posts/steven_kapturowski-human-level-atari-200x-faster-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Steven_kapturowski Human Level Atari 200x Faster 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Human Level Atari 200x Faster Author: Steven Kapturowski et. al. DeepMind Publish Year: September 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2209.07550.pdf
Motivation Agent 57 came at the cost of poor data-efficiency , requiring nearly 80,000 million frames of experience to achieve. this one can achieve the same performance in 390 million frames Contribution Some key terms NFNet - Normalisation Free Network
https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee Batch normalisation – the bad it is expensive batch normalisation breaks the assumption of data independence NFNet applies 3 different techniques: Modified residual branches and convolutions with Scaled Weight standardisation Adaptive Gradient Clipping Architecture optimisation for improved accuracy and training speed. https://github.com/vballoli/nfnets-pytorch Previous Non-Image features
...</p>
</div>
<footer class="entry-footer"><span title="2022-10-05 23:22:01 +1100 AEDT">October 5, 2022</span> · 2 min · 357 words · Sukai Huang</footer>
<a aria-label="post link to Steven_kapturowski Human Level Atari 200x Faster 2022" class="entry-link" href="http://localhost:1313/posts/steven_kapturowski-human-level-atari-200x-faster-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="CoBERL architecture" loading="lazy" src="http://localhost:1313/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: CoBERL Contrastive BERT for Reinforcement Learning Author: Andrea Banino et. al. DeepMind Publish Year: Feb 2022 Review Date: Wed, Oct 5, 2022 Summary of paper https://arxiv.org/pdf/2107.05431.pdf
Motivation Contribution Some key terms Representation learning in reinforcement learning
motivation: if state information could be effectively extracted from raw observations it may then be possible to learn from there as fast as from states. however, given the often sparse reward signal coming from the environment, learning representations in RL has to be achieved with little to no supervision. approach types class 1: auxiliary self-supervised losses to accelerate the learning speed in model-free RL algorithm class 2: learn a world model and use this to collect imagined rollouts, which then act as extra data to train the RL algorithm reducing the samples required from the environment CoBERL is in class 1 ​	it uses both masked language modelling and contrastive learning RL using BERT architecture – RELIC
...</p>
</div>
<footer class="entry-footer"><span title="2022-10-05 23:04:49 +1100 AEDT">October 5, 2022</span> · 2 min · 258 words · Sukai Huang</footer>
<a aria-label="post link to Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022" class="entry-link" href="http://localhost:1313/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="architecture" loading="lazy" src="http://localhost:1313/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Sample Factory: Asynchronous Rl at Very High FPS Author: Alex Petrenko Publish Year: Oct, 2020 Review Date: Sun, Sep 25, 2022 Summary of paper Motivation Identifying performance bottlenecks
RL involves three workloads:
environment simulation inference backpropagation overall performance depends on the lowest workload In existing methods (A2C/PPO/IMPALA) the computational workloads are dependent -&gt; under-utilisation of the system resources. Existing high-throughput methods focus on distributed training, therefore introducing a lot of overhead such as networking serialisation, etc.
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-25 16:34:09 +1000 AEST">September 25, 2022</span> · 1 min · 154 words · Sukai Huang</footer>
<a aria-label="post link to Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020" class="entry-link" href="http://localhost:1313/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="3D U-Net" loading="lazy" src="http://localhost:1313/posts/jonathan_ho-video-diffusion-models-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jonathan_ho Video Diffusion Models 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Google Video Diffusion Models Author: Jonathan Ho et. al. Publish Year: 22 Jun 2022 Review Date: Thu, Sep 22, 2022 Summary of paper Motivation proposing a diffusion model for video generation that shows very promising initial results Contribution this is the extension of image diffusion model they introduce a new conditional sampling technique for spatial and temporal video extension that performs better. Some key terms Diffusion model
A diffusion model specified in continuous time is a generative model with latents Training diffusion model
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-22 20:40:21 +1000 AEST">September 22, 2022</span> · 3 min · 471 words · Sukai Huang</footer>
<a aria-label="post link to Jonathan_ho Video Diffusion Models 2022" class="entry-link" href="http://localhost:1313/posts/jonathan_ho-video-diffusion-models-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="architecture diagram" loading="lazy" src="http://localhost:1313/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text Based Games Author: Dongwon Kelvin Ryu et. al. Publish Year: ACL 2022 Review Date: Thu, Sep 22, 2022 Summary of paper Motivation Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action space. A fundamental challenges in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. So, we want to inject external commonsense knowledge into the agent during training when the agent is most uncertain about its next action. Contribution In addition to performance increase, the produced trajectory of actions exhibit lower perplexity, when tested with a pre-trained LM, indicating better closeness to human language. Some key terms Exploration efficiency
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-22 19:38:56 +1000 AEST">September 22, 2022</span> · 2 min · 276 words · Sukai Huang</footer>
<a aria-label="post link to Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022" class="entry-link" href="http://localhost:1313/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="model structure" loading="lazy" src="http://localhost:1313/posts/wenlong_huang-language-models-as-zero-shot-planners-extracting-actionable-knowledge-for-embodied-agents-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Wenlong_huang Language Models as Zero Shot Planners Extracting Actionable Knowledge for Embodied Agents 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Language Models as Zero Shot Planners: Extracting Actionable Knowledge for Embodied Agents Author: Wenlong Huang et. al. Publish Year: Mar 2022 Review Date: Mon, Sep 19, 2022 Summary of paper Motivation Large language models are learning general commonsense world knowledge. so this paper, the author investigate the possibility of grounding high-level tasks, expressed as natural language (e.g., “make breakfast”) to a chosen set of action steps (“open fridge”). Contribution they found out that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. they proposed several tools to improve executability of the model generation without invasive probing or modifications to the model. Some key terms What is prompt learning
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-19 21:55:13 +1000 AEST">September 19, 2022</span> · 2 min · 253 words · Sukai Huang</footer>
<a aria-label="post link to Wenlong_huang Language Models as Zero Shot Planners Extracting Actionable Knowledge for Embodied Agents 2022" class="entry-link" href="http://localhost:1313/posts/wenlong_huang-language-models-as-zero-shot-planners-extracting-actionable-knowledge-for-embodied-agents-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="add object detection pretrain model" loading="lazy" src="http://localhost:1313/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: VinVL: Revisiting Visual Representations in Vision Language Models Author: Pengchuan Zhang et. al. Publish Year: 10 Mar 2021 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model Oscar.
And utilise an improved approach OSCAR + to pretrain the VL model
Contribution has a bigger Object Detection model with larger amount of training data, called “ResNeXt-152 C4” Some key terms Vision Language Pretraining
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-03 17:17:47 +1000 AEST">September 3, 2022</span> · 2 min · 332 words · Sukai Huang</footer>
<a aria-label="post link to Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021" class="entry-link" href="http://localhost:1313/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="illustration of Oscar model" loading="lazy" src="http://localhost:1313/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks Author: Xiujun Li et. al. Publish Year: 26 Jul 2020 Review Date: Sat, Sep 3, 2022 Summary of paper Motivation Existing method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.
the lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.
...</p>
</div>
<footer class="entry-footer"><span title="2022-09-03 17:12:54 +1000 AEST">September 3, 2022</span> · 3 min · 462 words · Sukai Huang</footer>
<a aria-label="post link to Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020" class="entry-link" href="http://localhost:1313/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="Illustration of DiffCSE" loading="lazy" src="http://localhost:1313/posts/yung_sung_chuang-diffcse-difference-based-contrastive-learning-for-sentence-embeddings-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yung_sung_chuang Diffcse Difference Based Contrastive Learning for Sentence Embeddings 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: DiffCSE: Difference Based Contrastive Learning for Sentence Embeddings Author: Yung-Sung Chuang et. al. Publish Year: 21 Apr 2022 Review Date: Sat, Aug 27, 2022 Summary of paper Motivation DiffCSE learns sentences that are sensitive to the difference between the original sentence and and edited sentence. Contribution we propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings Some key terms DiffCSE
this is an unsupervsied contrastive learning framework rather than model architecture Contrastive learning in single modality data
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-27 16:03:42 +1000 AEST">August 27, 2022</span> · 2 min · 351 words · Sukai Huang</footer>
<a aria-label="post link to Yung_sung_chuang Diffcse Difference Based Contrastive Learning for Sentence Embeddings 2022" class="entry-link" href="http://localhost:1313/posts/yung_sung_chuang-diffcse-difference-based-contrastive-learning-for-sentence-embeddings-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="Different architectures for image and text retrieval" loading="lazy" src="http://localhost:1313/posts/gregor_geigle-retrieve-fast-rerank-smart-coorperative-and-joint-approaches-for-improved-cross-modal-retrieval-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Gregor_geigle Retrieve Fast Rerank Smart Cooperative and Joint Approaches for Improved Cross Modal Retrieval 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval Author: Gregor Geigle et. al. Publish Year: 19 Feb, 2022 Review Date: Sat, Aug 27, 2022 Summary of paper Motivation they want to combine the cross encoder and the bi encoder advantages and have a more efficient cross-modal search and retrieval
efficiency and simplicity of BE approach based on twin network expressiveness and cutting-edge performance of CE methods. Contribution We propose a novel joint Cross Encoding and Binary Encoding model (Joint-Coop), which is trained to simultaneously cross-encode and embed multi-modal input; it achieves the highest scores overall while maintaining retrieval efficiency
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-27 00:31:38 +1000 AEST">August 27, 2022</span> · 3 min · 453 words · Sukai Huang</footer>
<a aria-label="post link to Gregor_geigle Retrieve Fast Rerank Smart Cooperative and Joint Approaches for Improved Cross Modal Retrieval 2022" class="entry-link" href="http://localhost:1313/posts/gregor_geigle-retrieve-fast-rerank-smart-coorperative-and-joint-approaches-for-improved-cross-modal-retrieval-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="MP-Net structure" loading="lazy" src="http://localhost:1313/posts/kaitao_song-mpnet-masked-and-permuted-retrain-for-language-understanding-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Kaitao_song Mpnet Masked and Permuted Retrain for Language Understanding 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: MPNet: Masked and Permuted Pre-training for Language Understanding Author: Kaitao Song et. al. Publish Year: 2020 Review Date: Thu, Aug 25, 2022 Summary of paper Motivation BERT adopts masked language modelling (MLM) for pre-training and is one of the most successful pre-training models.
Since BERT is all attention block and the positional embedding is the only info that care about the ordering, BERT neglects dependency among predicted tokens
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-25 12:24:55 +1000 AEST">August 25, 2022</span> · 2 min · 378 words · Sukai Huang</footer>
<a aria-label="post link to Kaitao_song Mpnet Masked and Permuted Retrain for Language Understanding 2020" class="entry-link" href="http://localhost:1313/posts/kaitao_song-mpnet-masked-and-permuted-retrain-for-language-understanding-2020/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="multimodal framework" loading="lazy" src="http://localhost:1313/posts/sergios_karagiannakos-vision-language-models-towards-multimodal-dl-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Sergios_karagiannakos Vision Language Models Towards Multimodal Dl 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Vision Language Models Towards Multimodal Deep Learning Author: Sergios Karagiannakos Publish Year: 03 Mar 2022 Review Date: Tue, Aug 9, 2022 https://theaisummer.com/vision-language-models/
</p>
</div>
<footer class="entry-footer"><span title="2022-08-09 07:37:30 +1000 AEST">August 9, 2022</span> · 1 min · 24 words · Sukai Huang</footer>
<a aria-label="post link to Sergios_karagiannakos Vision Language Models Towards Multimodal Dl 2022" class="entry-link" href="http://localhost:1313/posts/sergios_karagiannakos-vision-language-models-towards-multimodal-dl-2022/"></a>
</article>
<article class="post-entry">
<figure class="entry-cover"><img alt="learnable codebook" loading="lazy" src="http://localhost:1313/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jiali_duan Multimodal Alignment Using Representation Codebook 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Multi-modal Alignment Using Representation Codebook Author: Jiali Duan, Liqun Chen et. al. Publish Year: 2022 CVPR Review Date: Tue, Aug 9, 2022 Summary of paper Motivation aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion. since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. Contribution in this paper, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook). to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. Some key terms Types of Vision language pre-training tasks
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-09 07:26:46 +1000 AEST">August 9, 2022</span> · 3 min · 513 words · Sukai Huang</footer>
<a aria-label="post link to Jiali_duan Multimodal Alignment Using Representation Codebook 2022" class="entry-link" href="http://localhost:1313/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/"></a>
</article>
<article class="post-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">A preliminary idea about using instruction following as a intermediate training step towards a general learning-based agent
    </h2>
</header>
<div class="entry-content">
<p>This page is not completed yet
You need password to access to the content, go to Slack *#phdsukai to find more.
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-07 17:17:07 +1000 AEST">August 7, 2022</span> · 5 min · Sukai Huang</footer>
<a aria-label="post link to A preliminary idea about using instruction following as a intermediate training step towards a general learning-based agent" class="entry-link" href="http://localhost:1313/posts/instruction-following-as-a-path-to-general-problem-solving-agent-aug-2022/"></a>
</article>
<article class="post-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Supplementary explanations for proposed methods and PhD thesis structure
    </h2>
</header>
<div class="entry-content">
<p>You need password to access to the content, go to Slack *#phdsukai to find more.
...</p>
</div>
<footer class="entry-footer"><span title="2022-08-04 12:59:17 +1000 AEST">August 4, 2022</span> · 11 min · Sukai Huang</footer>
<a aria-label="post link to Supplementary explanations for proposed methods and PhD thesis structure" class="entry-link" href="http://localhost:1313/posts/supplementary-notes-for-mindmap-aug-2022/"></a>
</article>
<article class="post-entry">
<header class="entry-header">
<h2 class="entry-hint-parent">Younggyo_seo Masked World Models for Visual Control 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Masked World Models for Visual Control 2022 Author: Younggyo Seo et. al. Publish Year: 2022 Review Date: Fri, Jul 1, 2022 https://arxiv.org/abs/2206.14244?context=cs.AI
https://sites.google.com/view/mwm-rl
Summary of paper Motivation TL:DR: Masked autoencoders (MAE) has emerged as a scalable and effective self-supervised learning technique. Can MAE be also effective for visual model-based RL? Yes! with the recipe of convolutional feature masking and reward prediction to capture fine-grained and task-relevant information.
Some key terms Decouple visual representation learning and dynamics learning
...</p>
</div>
<footer class="entry-footer"><span title="2022-07-01 12:03:57 +1000 AEST">July 1, 2022</span> · 2 min · 227 words · Sukai Huang</footer>
<a aria-label="post link to Younggyo_seo Masked World Models for Visual Control 2022" class="entry-link" href="http://localhost:1313/posts/younggyo_seo-masked-world-models-for-visual-control-2022/"></a>
</article>
<footer class="page-footer">
<nav class="pagination">
<a class="prev" href="http://localhost:1313/page/8/">
      « Prev 8/11
    </a>
<a class="next" href="http://localhost:1313/page/10/">Next 10/11 »
    </a>
</nav>
</footer>
</main>
<footer class="footer">
<span>© 2024 <a href="http://localhost:1313/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
