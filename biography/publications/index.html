<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Publications | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="Sukai’s Research Paper Collection" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/biography/publications/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/biography/publications/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/biography/publications/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Publications" property="og:title"/>
<meta content="Sukai’s Research Paper Collection" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="biography" property="article:section"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" name="twitter:image"/>
<meta content="Publications" name="twitter:title"/>
<meta content="Sukai’s Research Paper Collection" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Biography",
      "item": "https://sino-huang.github.io/biography/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Publications",
      "item": "https://sino-huang.github.io/biography/publications/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Publications",
  "name": "Publications",
  "description": "Sukai\u0026rsquo;s Research Paper Collection",
  "keywords": [
    
  ],
  "articleBody": " submitted to ARR Dec The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards Sukai Huang, Nir Lipovetzky and Trevor Cohn arXiv ePrint 2024 Abstract paper code While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents AAAI25 Workshop LM4Plan Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation Sukai Huang, Trevor Cohn and Nir Lipovetzky Workshop on Planning in the Era of LLMs (LM4Plan @ AAAI 2025) Abstract paper code The capability of Large Language Models (LLMs) to plan remains a topic of debate. Some critics argue that strategies to boost LLMs' reasoning skills are ineffective in planning tasks, while others report strong outcomes merely from training models on a planning corpus. This study reassesses recent strategies by developing an end-to-end LLM planner and employing diverse metrics for a thorough evaluation. We find that merely fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets. At the same time, we find that various strategies, including Chain-of-Thought, do enhance the probability of a plan being executable. This indicates progress towards better plan quality, despite not directly enhancing the final validity rate. Among the strategies we evaluated, reinforcement learning with our novel `Longest Contiguous Common Subsequence' reward emerged as the most effective, contributing to both plan validity and executability. Overall, our research addresses key misconceptions in the LLM-planning literature; we validate incremental progress in plan executability, although plan validity remains a challenge. Hence, future strategies should focus on both these aspects, drawing insights from our findings.\nAAAI25 Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts Sukai Huang, Nir Lipovetzky and Trevor Cohn Thirty-Ninth AAAI Conference on Artificial Intelligence Abstract paper code Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.\npreprint A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents Sukai Huang, Nir Lipovetzky and Trevor Cohn arXiv ePrint 2023 Abstract paper code Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.\nhonours thesis Angry Birds Level Generation Using Walkthrough Descriptions Sukai Huang For the degree of Bachelor of Advanced Computing (Honours) at The Australian National University Abstract paper Angry Birds is a famous environment for agents to learn physical reasoning. How- ever, the deep reinforcement learning agents often underperform due to a lack of training set of game levels. To address the issue, procedural level generation is used to synthesise new Angry Birds game levels. However, the current rule-based Angry Birds procedural level generator is incapable of generating game levels that aid agents in learning physical reasoning, as it cannot guarantee the level of physical reasoning required in order to solve the generated game levels. Hence, in a new approach, we use walkthrough descriptions to generate Angry Birds game levels and train the Generative Adversarial Networks (GANs) based pro- cedural level generator by imitating the high-quality handcrafted levels. Unlike the conventional imitation approach, the proposed one is able to control the style of the generated game levels and also enhance the diversity of the game level dataset via manipulating the input walkthrough descriptions. Both qualitative and quantitative evaluations are conducted to demonstrate that the generated game levels using this method demand high level of physical reasoning to solve, just like the handcrafted game levels. Besides that, we developed a new Angry Birds walkthrough dataset called AbVat. It is a valuable dataset capable of facilitating a variety of meaningful research tasks in the domain of spatial-temporal understanding and reasoning.\n",
  "wordCount" : "994",
  "inLanguage": "en",
  "image": "https://sino-huang.github.io/sukai_avatar.jpg","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/biography/publications/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/biography/">Biography</a></div>
<h1 class="post-title entry-hint-parent">
      Publications
    </h1>
<div class="post-meta">5 min · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/biography/publications/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<div class="post-content"><style>
    .badge {
        display: inline-block;
        padding: .25em .4em;
        font-size: 75%;
        font-weight: 700;
        line-height: 1;
        text-align: center;
        white-space: nowrap;
        vertical-align: baseline;
        border-radius: .25rem;
        transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out;
    }

    .badge {
        color: rgb(81, 194, 29) !important;
        border-radius: .125rem;
        -webkit-box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.16), 0 2px 10px 0 rgba(0, 0, 0, 0.12);
        box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.16), 0 2px 10px 0 rgba(0, 0, 0, 0.12);
    }

    .publications ol.bibliography li .title {
        font-weight: bolder;
    }

    .publications ol.bibliography {
        list-style: none;
        padding: 0;
        margin-top: 0;
    }

    .publications ol.bibliography li {
        margin-bottom: 1rem;
    }

    .row {
        display: -ms-flexbox;
        display: flex;
        -ms-flex-wrap: wrap;
        flex-wrap: wrap;
        margin-right: -15px;
        margin-left: 0px;
    }

    .publications ol.bibliography li .links a.btn {
	color: #111;
	border: 1px solid #111;
	padding-left: 1rem;
	padding-right: 1rem;
	padding-top: 0.25rem;
	padding-bottom: 0.25rem;
    margin-top: 0.55rem;
    font-size: small;
}

.publications ol.bibliography li .periodical {
	padding-bottom: 0.15rem;
    margin: 0.15rem;
}

    .publications ol.bibliography li .abbr {
        height: 2rem;
        margin-bottom: 0.5rem;
    }

    .publications ol.bibliography li .links a.btn:hover {
        color: #579968;
        border-color: #579968;
    }

    .publications ol.bibliography li div.abstract.hidden.open {
	border-color: #828282;
}

.publications ol.bibliography li div.abstract.hidden {
	border: dashed 1px white;
}
.publications ol.bibliography li .hidden.open {
	max-height: 100em;
	-webkit-transition: 0.15s ease;
	-moz-transition: 0.15s ease;
	-ms-transition: 0.15s ease;
	-o-transition: 0.15s ease;
	transition: all 0.15s ease;
}
.publications ol.bibliography li .hidden {
	font-size: 0.875rem;
	max-height: 0px;
	overflow: hidden;
	text-align: justify;
	-webkit-transition: 0.15s ease;
	-moz-transition: 0.15s ease;
	-ms-transition: 0.15s ease;
	-o-transition: 0.15s ease;
	transition: all 0.15s ease;
}
</style>
<script crossorigin="anonymous" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" src="https://code.jquery.com/jquery-3.7.0.min.js"></script>
<script>
    $(document).ready(function() {
    $('a.abstract').click(function() {
        $(this).parent().parent().find(".abstract.hidden").toggleClass('open');
    });
    $('a.bibtex').click(function() {
        $(this).parent().parent().find(".bibtex.hidden").toggleClass('open');
    });
});

</script>
<div class="publications">
<ol class="bibliography">
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">submitted to ARR Dec</abbr>
</div>
<div class="col-sm-8">
<div class="title">The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards</div>
<div class="author">
                         Sukai Huang, Nir Lipovetzky and Trevor Cohn
 
                     </div>
<div class="periodical">
<em>arXiv ePrint 2024</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2409.15922" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://www.shorturl.at/VsH70" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents </p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">AAAI25 Workshop LM4Plan</abbr>
</div>
<div class="col-sm-8">
<div class="title">Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation</div>
<div class="author">
                        Sukai Huang, Trevor Cohn and Nir Lipovetzky

                    </div>
<div class="periodical">
<em>Workshop on Planning in the Era of LLMs (LM4Plan @ AAAI 2025)</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2412.10675" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://anonymous.4open.science/r/official-misconcept-lm-plan-gen-D34B" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>The capability of Large Language Models (LLMs) to plan remains a topic  of debate. Some critics argue that strategies to boost LLMs' reasoning  skills are ineffective in planning tasks, while others report strong  outcomes merely from training models on a planning corpus. This study  reassesses recent strategies by developing an end-to-end LLM planner and employing diverse metrics for a thorough evaluation. We find that  merely fine-tuning LLMs on a corpus of planning instances does not lead  to robust planning skills, as indicated by poor performance on  out-of-distribution test sets. At the same time, we find that various  strategies, including Chain-of-Thought, do enhance the probability of a  plan being executable. This indicates progress towards better plan  quality, despite not directly enhancing the final validity rate. Among  the strategies we evaluated, reinforcement learning with our novel  `Longest Contiguous Common Subsequence' reward emerged as the most  effective, contributing to both plan validity and executability.  Overall, our research addresses key misconceptions in the LLM-planning  literature; we validate incremental progress in plan executability,  although plan validity remains a challenge. Hence, future strategies  should focus on both these aspects, drawing insights from our findings.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">AAAI25</abbr>
</div>
<div class="col-sm-8">
<div class="title">Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts</div>
<div class="author">
                         Sukai Huang, Nir Lipovetzky and Trevor Cohn
 
                     </div>
<div class="periodical">
<em>Thirty-Ninth AAAI Conference on Artificial Intelligence</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2409.15915" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://github.com/Sino-Huang/Official-LLM-Symbolic-Planning-without-Experts" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">preprint</abbr>
</div>
<div class="col-sm-8">
<div class="title">A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents</div>
<div class="author">
                        Sukai Huang, Nir Lipovetzky and Trevor Cohn

                    </div>
<div class="periodical">
<em>arXiv ePrint 2023</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2305.16621" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://github.com/Sino-Huang/Brittleness_of_LRS" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">honours thesis</abbr>
</div>
<div class="col-sm-8">
<div class="title">Angry Birds Level Generation Using Walkthrough Descriptions</div>
<div class="author">
                        Sukai Huang

                    </div>
<div class="periodical">
<em>For the degree of Bachelor of Advanced Computing (Honours) at The Australian National University</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">Abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="./honours_thesis.pdf" role="button" target="_blank">paper</a>
</div>
<div class="abstract hidden">
<p>Angry Birds is a famous environment for agents to learn physical reasoning. How- ever, the deep reinforcement learning agents often underperform due to a lack of training set of game levels. To address the issue, procedural level generation is used to synthesise new Angry Birds game levels. However, the current rule-based Angry Birds procedural level generator is incapable of generating game levels that aid agents in learning physical reasoning, as it cannot guarantee the level of physical reasoning required in order to solve the generated game levels. Hence, in a new approach, we use walkthrough descriptions to generate Angry Birds game levels and train the Generative Adversarial Networks (GANs) based pro- cedural level generator by imitating the high-quality handcrafted levels. Unlike the conventional imitation approach, the proposed one is able to control the style of the generated game levels and also enhance the diversity of the game level dataset via manipulating the input walkthrough descriptions. Both qualitative and quantitative evaluations are conducted to demonstrate that the generated game levels using this method demand high level of physical reasoning to solve, just like the handcrafted game levels. Besides that, we developed a new Angry Birds walkthrough dataset called AbVat. It is a valuable dataset capable of facilitating a variety of meaningful research tasks in the domain of spatial-temporal understanding and reasoning.</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
<footer class="post-footer">
<ul class="post-tags">
</ul>
<ul class="share-buttons">
<li>
<a aria-label="share Publications on x" href="https://x.com/intent/tweet/?text=Publications&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;hashtags=" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;title=Publications&amp;summary=Publications&amp;source=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;title=Publications" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on whatsapp" href="https://api.whatsapp.com/send?text=Publications%20-%20https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on telegram" href="https://telegram.me/share/url?text=Publications&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Publications on ycombinator" href="https://news.ycombinator.com/submitlink?t=Publications&amp;u=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
