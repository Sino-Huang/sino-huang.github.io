<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Proximal Policy Optimisation Explained Blog | Sukai Huang</title>
<meta content="reinforcement learning" name="keywords"/>
<meta content="[TOC]

Title: Proximal Policy Optimisation Explained Blog
Author: Xiao-Yang Liu; DI engine
Publish Year: May 4, 2021
Review Date: Mon, Dec 26, 2022


Highly recommend reading this blog

https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
https://zhuanlan.zhihu.com/p/487754664



Difference between on-policy and off-policy


For on-policy algorithms, they update the policy network based on the  transitions generated by the current policy network. The critic network would make a more accurate value-prediction for the current policy  network in common environments.
For off-policy algorithms, they allow to update the current policy  network using the transitions from old policies. Thus, the old  transitions could be reutilized, as shown in Fig. 1 the points are  scattered on trajectories that are generated by different policies,  which improves the sample efficiency and reduces the total training steps.

Question: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit.

PPO solves the problem of sample efficiency by utilizing surrogate  objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both 1. regularizes the policy update and enables the 2. reuse of training data.






Algorithm
" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Proximal Policy Optimisation Explained Blog" property="og:title"/>
<meta content="[TOC]
Title: Proximal Policy Optimisation Explained Blog Author: Xiao-Yang Liu; DI engine Publish Year: May 4, 2021 Review Date: Mon, Dec 26, 2022 Highly recommend reading this blog https://lilianweng.github.io/posts/2018-04-08-policy-gradient/ https://zhuanlan.zhihu.com/p/487754664 Difference between on-policy and off-policy
For on-policy algorithms, they update the policy network based on the transitions generated by the current policy network. The critic network would make a more accurate value-prediction for the current policy network in common environments. For off-policy algorithms, they allow to update the current policy network using the transitions from old policies. Thus, the old transitions could be reutilized, as shown in Fig. 1 the points are scattered on trajectories that are generated by different policies, which improves the sample efficiency and reduces the total training steps. Question: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit. PPO solves the problem of sample efficiency by utilizing surrogate objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both 1. regularizes the policy update and enables the 2. reuse of training data. Algorithm " property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/cover.png" name="twitter:image"/>
<meta content="Proximal Policy Optimisation Explained Blog" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Proximal Policy Optimisation Explained Blog",
      "item": "https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Proximal Policy Optimisation Explained Blog
    <a aria-label="RSS" href="/posts/proximal-policy-optimisation-explained-blog/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Proximal Policy Optimisation Explained Blog</li>
<li>Author: Xiao-Yang Liu; DI engine</li>
<li>Publish Year: May 4, 2021</li>
<li>Review Date: Mon, Dec 26, 2022</li>
</ol>
<ul>
<li>Highly recommend reading this blog
<ul>
<li><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/487754664">https://zhuanlan.zhihu.com/p/487754664</a></li>
</ul>
</li>
</ul>
<p><strong>Difference between on-policy and off-policy</strong></p>
<p><img alt="image-20221226195443427" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195443427.png"/></p>
<ul>
<li>For on-policy algorithms, they update the policy network based on the  transitions generated by the current policy network. The <strong>critic network</strong> would make a more accurate value-prediction for the current policy  network in common environments.</li>
<li>For off-policy algorithms, they allow to update the current policy  network using the transitions from old policies. Thus, the old  transitions could be <strong>reutilized</strong>, as shown in Fig. 1 the points are  scattered on trajectories that are generated by different policies,  which <strong>improves the sample efficiency and reduces the total training steps</strong>.</li>
</ul>
<h2 id="question-is-there-a-way-to-improve-the-sample-efficiency-of-on-policy-algorithms-without-losing-their-benefit">Question: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit.<a aria-hidden="true" class="anchor" hidden="" href="#question-is-there-a-way-to-improve-the-sample-efficiency-of-on-policy-algorithms-without-losing-their-benefit">#</a></h2>
<ul>
<li>PPO solves the problem of sample efficiency by utilizing surrogate  objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both <strong>1. regularizes</strong> the policy update and enables the <strong>2. reuse</strong> of training data.</li>
<li><img alt="image-20221226195751351" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195751351.png"/></li>
<li>
<img alt="image-20221226200007957" src="image-assets/image-20221226200007957.png" style="width:50%;"/>
</li>
<li><img alt="image-20221226195936296" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195936296.png"/></li>
</ul>
<h2 id="algorithm">Algorithm<a aria-hidden="true" class="anchor" hidden="" href="#algorithm">#</a></h2>
<p><img alt="image-20221226200313414" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226200313414.png"/></p>
<h2 id="explanation">explanation<a aria-hidden="true" class="anchor" hidden="" href="#explanation">#</a></h2>
<p><img alt="image-20221226200425376" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226200425376.png"/></p>
<h2 id="generalized-advantage-estimator-gae">Generalized advantage estimator (GAE)<a aria-hidden="true" class="anchor" hidden="" href="#generalized-advantage-estimator-gae">#</a></h2>
<p><img alt="image-20221226201313870" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226201313870.png"/></p>
<h2 id="total-ppo-loss">total PPO loss<a aria-hidden="true" class="anchor" hidden="" href="#total-ppo-loss">#</a></h2>
<p><img alt="image-20221226201451409" loading="lazy" src="/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226201451409.png"/></p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
