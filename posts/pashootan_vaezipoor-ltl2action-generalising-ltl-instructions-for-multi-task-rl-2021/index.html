<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021 | Sukai Huang</title>
<meta content="reinforcement learning, linear temporal logic" name="keywords"/>
<meta content="please modify the following
[TOC]

Title: LTL2Action: Generalizing LTL Instructions for Multi-Task RL
Author: Pashootan Vaezipoor et. al.
Publish Year: 2021
Review Date: March 2022

Summary of paper
Motivation
they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language – linear temporal logic (LTL)
Limitation of the vanilla MDP
temporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards." name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021" property="og:title"/>
<meta content="please modify the following
[TOC]
Title: LTL2Action: Generalizing LTL Instructions for Multi-Task RL Author: Pashootan Vaezipoor et. al. Publish Year: 2021 Review Date: March 2022 Summary of paper Motivation they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language – linear temporal logic (LTL)
Limitation of the vanilla MDP
temporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards." property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" name="twitter:image"/>
<meta content="Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021",
      "item": "https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021
    <a aria-label="RSS" href="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>please modify the following</p>
<p>[TOC]</p>
<ol>
<li>Title: LTL2Action: Generalizing LTL Instructions for Multi-Task RL</li>
<li>Author: Pashootan Vaezipoor et. al.</li>
<li>Publish Year: 2021</li>
<li>Review Date: March 2022</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<p>they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language – linear temporal logic (LTL)</p>
<p><strong>Limitation of the vanilla MDP</strong></p>
<p>temporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards.</p>
<p><img alt="image-20220302230532699" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302230532699.png"/></p>
<p><strong>Contribution</strong></p>
<ul>
<li>proposed a novel approach for teaching RL agents to follow LTL instructions that has theoretical advantages over other methods</li>
<li>encode LTL instruction via a neural architecture equipped with LTL progression (GNN)</li>
<li>use environment-agnostic LTL pre-training</li>
</ul>
<p><strong>From LTL Instructions to Rewards</strong></p>
<p>we have a labelling function L(s,a) assigns truth values to the propositions in P given the current state s (or state history) of the environment and the action  $a \in A$ (also known as event detector)</p>
<p>Formally, given an LTL instruction $\varphi$ over $P$ and a labelling function $L : S \times A \rarr 2^P$</p>
<p><img alt="image-20220302000924235" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302000924235.png"/></p>
<p>it just means that if all the trajectory follows the instruction, then we can have such reward</p>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>Multitask learning</strong></p>
<p>in order to instruct RL using language, the first step is to agree upon a common vocabulary between different tasks.</p>
<p><strong>Propositional symbols</strong></p>
<p>propositional logic -&gt; deals with <a href="https://en.wikipedia.org/wiki/Propositions">propositions</a> (which can be true or false) and relations between propositions,</p>
<p>proposition -&gt; is the meaning of a declarative sentence. “meaning” is understood to be a non-linguistic entity which is shared by all sentences with the same meaning.</p>
<p><strong>Linear Temporal Logic</strong></p>
<p><img alt="image-20220302182522573" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302182522573.png"/></p>
<p><em><strong>definition of formula:</strong></em> <strong>formula</strong>, is a finite <a href="https://en.wikipedia.org/wiki/Sequence">sequence</a> of <a href="https://en.wikipedia.org/wiki/Symbol_(formal)">symbols</a> from a given <a href="https://en.wikipedia.org/wiki/Alphabet_(computer_science)">alphabet</a> that is part of a <a href="https://en.wikipedia.org/wiki/Formal_language">formal language</a></p>
<p><img alt="image-20220302183038022" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302183038022.png"/></p>
<p>the unary connectives have higher precedence than the binary connectives</p>
<img alt="image-20220302183306652" src="image-assets/image-20220302183306652.png" style="zoom:50%;"/>
<p><img alt="image-20220302185554276" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302185554276.png"/></p>
<p><img alt="image-20220302185630290" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302185630290.png"/></p>
<p>A until B means A will hold continuously until B comes up and A will stop to hold</p>
<p><strong>Example</strong></p>
<p><img alt="image-20220302193118184" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302193118184.png"/></p>
<p><img alt="image-20220302212138626" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302212138626.png"/></p>
<p>“infinitely often” means it happens multiple times</p>
<p><img alt="image-20220302212229886" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302212229886.png"/></p>
<p><img alt="image-20220302212705182" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302212705182.png"/></p>
<p><img alt="image-20220302212840288" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302212840288.png"/></p>
<p><img alt="image-20220302212945783" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302212945783.png"/></p>
<p><img alt="image-20220302213252912" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302213252912.png"/></p>
<p><img alt="image-20220302214130222" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302214130222.png"/></p>
<p><strong>Lift Example</strong></p>
<p><img alt="image-20220302214958916" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302214958916.png"/></p>
<p><strong>Formal semantics</strong></p>
<p><img alt="image-20220302230212572" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302230212572.png"/></p>
<p><img alt="image-20220302230401073" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302230401073.png"/></p>
<p><strong>Minigrid example</strong></p>
<p><img alt="image-20220302231806584" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302231806584.png"/></p>
<p>Let’s say that the set of propositions $P$ includes $R, G$ and $B$, which are true iff the agent is standing on red/green/blue square (respectively) <em>in the current time step</em></p>
<p><img alt="image-20220302232520842" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220302232520842.png"/></p>
<p><strong>How to analyse the LTL formula and how to make the progress Markovian again</strong></p>
<p>what I mean is to check if LTL formula is satisfied or not</p>
<p>solution: LTL progression</p>
<p><img alt="image-20220303003036593" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303003036593.png"/></p>
<p><img alt="image-20220303005045172" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303005045172.png"/></p>
<p>we can see the progression will decompose the instructions gradually.</p>
<p>i.e., Progress towards completion of the task is reflected in diminished remaining instructions.</p>
<p><img alt="image-20220303010414021" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303010414021.png"/></p>
<p>all in all, as we consume the instruction $\varphi$ , we can then neglect the trajectory and make the policy Markovian</p>
<p><img alt="image-20220303011604341" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303011604341.png"/></p>
<p>check the transition distribution definition</p>
<p><img alt="image-20220303012433613" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303012433613.png"/></p>
<p><img alt="image-20220303010837269" loading="lazy" src="/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/image-assets/image-20220303010837269.png"/></p>
<h2 id="minor-comments">Minor comments<a aria-hidden="true" class="anchor" hidden="" href="#minor-comments">#</a></h2>
<p>check LTL introduction videos</p>
<p><a href="https://www.youtube.com/watch?v=a9fo3dUly8A&amp;list=PLMBx8HjvK7672qEl6bdnXdzYEbLP_lWPw">https://www.youtube.com/watch?v=a9fo3dUly8A&amp;list=PLMBx8HjvK7672qEl6bdnXdzYEbLP_lWPw</a></p>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<p>I can see that LTL is also care about the low level instructions rather than strategies.</p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
