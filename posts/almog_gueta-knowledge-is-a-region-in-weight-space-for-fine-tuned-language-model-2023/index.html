<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 | Sukai Huang</title>
<meta content="fine tuning, language model, multitask learning" name="keywords"/>
<meta content="[TOC]

Title: Knowledge Is a Region in Weight Space for Fine Tuned Language Model
Author: Almog Gueta et. al.
Publish Year: 12 Feb 2023
Review Date: Wed, Mar 1, 2023
url: https://arxiv.org/pdf/2302.04863.pdf

Summary of paper

Motivation

relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets.

Contribution

we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa
language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster.
traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning
Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning.

more findings" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023" property="og:title"/>
<meta content="[TOC]
Title: Knowledge Is a Region in Weight Space for Fine Tuned Language Model Author: Almog Gueta et. al. Publish Year: 12 Feb 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2302.04863.pdf Summary of paper Motivation relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets. Contribution we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. more findings" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2023-03-01T12:45:54+11:00" property="article:published_time"/>
<meta content="2023-03-01T12:45:54+11:00" property="article:modified_time"/>
<meta content="Fine Tuning" property="article:tag"/>
<meta content="Language Model" property="article:tag"/>
<meta content="Multitask Learning" property="article:tag"/>
<meta content="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png" name="twitter:image"/>
<meta content="Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023" name="twitter:title"/>
<meta content="[TOC]

Title: Knowledge Is a Region in Weight Space for Fine Tuned Language Model
Author: Almog Gueta et. al.
Publish Year: 12 Feb 2023
Review Date: Wed, Mar 1, 2023
url: https://arxiv.org/pdf/2302.04863.pdf

Summary of paper

Motivation

relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets.

Contribution

we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa
language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster.
traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning
Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning.

more findings" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023",
      "item": "https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023",
  "name": "Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023",
  "description": "[TOC]\nTitle: Knowledge Is a Region in Weight Space for Fine Tuned Language Model Author: Almog Gueta et. al. Publish Year: 12 Feb 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2302.04863.pdf Summary of paper Motivation relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets. Contribution we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. more findings\n",
  "keywords": [
    "fine tuning", "language model", "multitask learning"
  ],
  "articleBody": "[TOC]\nTitle: Knowledge Is a Region in Weight Space for Fine Tuned Language Model Author: Almog Gueta et. al. Publish Year: 12 Feb 2023 Review Date: Wed, Mar 1, 2023 url: https://arxiv.org/pdf/2302.04863.pdf Summary of paper Motivation relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets. Contribution we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. more findings\nwe show that after a pre-trained model is fine-tuned on similar datasets, the resulting fine-tuned models are close to each other in the weight space. models fine-tuned on the sae data are closer to each other than to to other models models that were fine-tuned on the same task also cluster together models fine-tuned on language tasks are not spread around the pre-trained space arbitrarily but rather correspond to a constrained region in weight space Some key terms rather than fine-tuning\nNotably, such points in weight space might not necessarily be reached via fine-tuning, but rather via spatial transformation. points on a line between the two points representing two models fine-tuned on the same dataset\nwe find that points on a line between the two points representing two models fine-tuned on the same dataset attain similar or even lower loss than the two individual models. empirical findings\nsuggesting, for example, that the best models may not lie at the edges of the region, but rather closer to its center, while fine-tuning often yields models at the edge of the region motivated by these findings, we demonstrate that a model created by averaging the weights of fine-tuned models from the same region outperforms the pre-trained model on a variety of tasks after subsequent fine-tuning. Comparing models comparing loss difference is the core idea but the loss of a given model is often incomparable across datasets or tasks to define a loss that is comparable across models, we first adopt the typical perspective that the model $f_\\theta$ consist of a representation encoder $f_w$ followed by a task-specific $f_\\phi$, i.e., $f_\\theta = f_\\phi \\circ f_w$ to calculate the loss we do the following first, remove any existing masked language modeling layers or classification heads and replace them with a new randomly initialised classification head. This leaves the rest of the weights i.e., the encoder $f_w$ fixed. We then perform linear probing i.e., we train only the new classification head on a desired target data and its label Lastly, we pass the test data through the model and report the loss with respect to the label. Projection by t-SNE\nt-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualising high-dimensional data by giving each datapoint a location in a two-or three dimensional map. ",
  "wordCount" : "548",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png","datePublished": "2023-03-01T12:45:54+11:00",
  "dateModified": "2023-03-01T12:45:54+11:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023
    </h1>
<div class="post-meta"><span title="2023-03-01 12:45:54 +1100 AEDT">March 1, 2023</span> · 3 min · 548 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="Summary of paper" href="#summary-of-paper">Summary of paper</a><ul>
<li>
<a aria-label="Motivation" href="#motivation">Motivation</a></li>
<li>
<a aria-label="Contribution" href="#contribution">Contribution</a></li>
<li>
<a aria-label="Some key terms" href="#some-key-terms">Some key terms</a></li>
<li>
<a aria-label="Comparing models" href="#comparing-models">Comparing models</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Knowledge Is a Region in Weight Space for Fine Tuned Language Model</li>
<li>Author: Almog Gueta et. al.</li>
<li>Publish Year: 12 Feb 2023</li>
<li>Review Date: Wed, Mar 1, 2023</li>
<li>url: <a href="https://arxiv.org/pdf/2302.04863.pdf">https://arxiv.org/pdf/2302.04863.pdf</a></li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<p><img alt="image-20230301124703839" loading="lazy" src="/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png"/></p>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa</li>
<li>language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster.</li>
<li>traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning</li>
<li>Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning.</li>
</ul>
<p><strong>more findings</strong></p>
<ol>
<li>we show that after a pre-trained model is fine-tuned on similar datasets, the resulting fine-tuned models are close to each other in the weight space.</li>
<li>models fine-tuned on the sae data are closer to each other than to to other models</li>
<li>models that were fine-tuned on the same task also cluster together</li>
<li>models fine-tuned on language tasks are not spread around the pre-trained space arbitrarily but rather correspond to a constrained region in weight space</li>
</ol>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>rather than fine-tuning</strong></p>
<ul>
<li>Notably, such points in weight space might not necessarily be reached via fine-tuning, but rather via spatial transformation.</li>
</ul>
<p><strong>points on a line between the two points representing two models fine-tuned on the same dataset</strong></p>
<ul>
<li>we find that points on a line between the two points representing two models fine-tuned on the same dataset attain similar or even lower loss than the two individual models.</li>
</ul>
<p><strong>empirical findings</strong></p>
<ul>
<li>suggesting, for example, that the best models may not lie at the edges of the region, but rather closer to its center, while fine-tuning often yields models at the edge of the region</li>
<li>motivated by these findings, we demonstrate that a model created by averaging the weights of fine-tuned models from the same region outperforms the pre-trained model on a variety of tasks after subsequent fine-tuning.</li>
</ul>
<h3 id="comparing-models">Comparing models<a aria-hidden="true" class="anchor" hidden="" href="#comparing-models">#</a></h3>
<ul>
<li>comparing loss difference is the core idea but the loss of a given model is often incomparable across datasets or tasks</li>
<li>to define a loss that is comparable across models, we first adopt the typical perspective that the model $f_\theta$ consist of a representation encoder $f_w$ followed by a task-specific $f_\phi$, i.e., $f_\theta = f_\phi \circ f_w$</li>
<li>to calculate the loss we do the following
<ul>
<li>first, remove any existing masked language modeling layers or classification heads and replace them with a new randomly initialised classification head. This leaves the rest of the weights i.e., the encoder $f_w$ fixed.</li>
<li>We then perform linear probing i.e., we train only the new classification head on a desired target data and its label</li>
<li>Lastly, we pass the test data through the model and report the loss with respect to the label.</li>
</ul>
</li>
</ul>
<p><strong>Projection by t-SNE</strong></p>
<ul>
<li>t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualising high-dimensional data by giving each datapoint a location in a two-or three dimensional map.</li>
</ul>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/fine-tuning/">Fine Tuning</a></li>
<li><a href="https://sino-huang.github.io/tags/language-model/">Language Model</a></li>
<li><a href="https://sino-huang.github.io/tags/multitask-learning/">Multitask Learning</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/">
<span class="title">« Prev</span>
<br/>
<span>Timo_schick Toolformer Language Models Can Teach Themselves to Use Tools 2023</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/">
<span class="title">Next »</span>
<br/>
<span>Xiwen_liang Contrastive Instruction Trajectory Learning for Vision Language Navigation 2022</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on x" href="https://x.com/intent/tweet/?text=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f&amp;hashtags=finetuning%2clanguagemodel%2cmultitasklearning" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f&amp;title=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023&amp;summary=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f&amp;title=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on whatsapp" href="https://api.whatsapp.com/send?text=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on telegram" href="https://telegram.me/share/url?text=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Almog_gueta%20Knowledge%20Is%20a%20Region%20in%20Weight%20Space%20for%20Fine%20Tuned%20Language%20Model%202023&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2falmog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
