<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Jiali_duan Multimodal Alignment Using Representation Codebook 2022 | Sukai Huang</title>
<meta content="multimodal, future work" name="keywords"/>
<meta content="[TOC]

Title: Multi-modal Alignment Using Representation Codebook
Author: Jiali Duan, Liqun Chen et. al.
Publish Year: 2022 CVPR
Review Date: Tue, Aug 9, 2022

Summary of paper
Motivation

aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion.
since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training.

Contribution

in this paper, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook).
to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.

Some key terms
Types of Vision language pre-training tasks" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Jiali_duan Multimodal Alignment Using Representation Codebook 2022" property="og:title"/>
<meta content="[TOC]
Title: Multi-modal Alignment Using Representation Codebook Author: Jiali Duan, Liqun Chen et. al. Publish Year: 2022 CVPR Review Date: Tue, Aug 9, 2022 Summary of paper Motivation aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion. since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training. Contribution in this paper, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook). to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. Some key terms Types of Vision language pre-training tasks" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/cover.png" name="twitter:image"/>
<meta content="Jiali_duan Multimodal Alignment Using Representation Codebook 2022" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Jiali_duan Multimodal Alignment Using Representation Codebook 2022",
      "item": "https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Jiali_duan Multimodal Alignment Using Representation Codebook 2022
    <a aria-label="RSS" href="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Multi-modal Alignment Using Representation Codebook</li>
<li>Author: Jiali Duan, Liqun Chen et. al.</li>
<li>Publish Year: 2022 CVPR</li>
<li>Review Date: Tue, Aug 9, 2022</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion.</li>
<li>since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>in this paper, we treat image and text as two “views” of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook).</li>
<li>to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>Types of Vision language pre-training tasks</strong></p>
<ol>
<li>multimodal alignment: aligning the feature spaces of different modalities
<ul>
<li>late fusion approaches such as CLIP and ALIGN focus on this</li>
</ul>
</li>
<li>cross-modal fusion: capturing the interaction across modalities.
<ul>
<li>early fusion approaches such as OSCAR, VinVL and VilLT focus on this</li>
</ul>
</li>
</ol>
<p>**momentum distillation, **</p>
<ol>
<li>for each of the image, text and fusion encoder, there is a corresponding encoder that is updated through moving average without gradient back propagation. These momentum encoder serve as teachers to guide the self-supervised learning process. In this paper, we use the teachers to guide codebook learning as well as for the cross-modal and intra-modal alignment</li>
</ol>
<p><strong>codebook</strong></p>
<p>codebook is a d-by-K matrix used as projector to project image and text feature into a common space.</p>
<h3 id="method">Method<a aria-hidden="true" class="anchor" hidden="" href="#method">#</a></h3>
<p><img alt="image-20220809142443262" loading="lazy" src="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/image-20220809142443262.png"/></p>
<ol>
<li>in this work, features from image and text modalities were first aligned and then fused using a transformer encoder.</li>
<li>the main focus of the work is on the feature alignment stage -&gt; make it more efficient</li>
<li>the main contribution of this method is: using a codebook that quantizes the common text-image feature into codewords (cluster centre).
<ol>
<li>this cluster centre provide a more stable means for contrastive reasoning compared to individual text or visual features.</li>
<li><img alt="image-20220809143313638" loading="lazy" src="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/image-20220809143313638.png"/></li>
</ol>
</li>
</ol>
<p><strong>Inspiration from SwAV</strong></p>
<p>Two augmented versions (views) of the same input image were passed through a deep network for feature extraction. visual embedding was learned by optimising an objective function that enforces the consistency between the <strong>feature</strong> from one and the <strong>assigned cluster</strong> from the other view (different view leads to the same <strong>entity</strong> and that entity is represented as <strong>cluster</strong> (or <strong>codeword</strong> in this paper))</p>
<ul>
<li>Effectively, visual and text features are lined up via aligning with the common codewords during
training.</li>
</ul>
<p><strong>Overview of the framework</strong></p>
<p><img alt="image-20220809165957333" loading="lazy" src="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/image-20220809165957333.png"/></p>
<p><img alt="image-20220809170012530" loading="lazy" src="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/image-20220809170012530.png"/></p>
<p><strong>Optimal Transport</strong></p>
<p><a href="http://alexhwilliams.info/itsneuronalblog/2020/10/09/optimal-transport/">http://alexhwilliams.info/itsneuronalblog/2020/10/09/optimal-transport/</a></p>
<p>this allows the feature vector to be similar to one of the codeword cluster centre</p>
<p><img alt="image-20220809215150928" loading="lazy" src="/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/image-assets/image-20220809215150928.png"/></p>
<p>Optimal Transport is a little bit complex, we may want to use alternative way to implement this.</p>
<p>Essentially the idea is that we want to have an intermediate cluster centre vector so that both image feature and text feature can take projection on this.</p>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<p>Use the alignment loss in this paper to train our model</p>
<p>Although this model does not consider the temporal order / sequence alignment</p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
