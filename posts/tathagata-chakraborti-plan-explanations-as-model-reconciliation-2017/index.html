<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017 | Sukai Huang</title>
<meta content="pddl" name="keywords"/>
<meta content="[TOC]

Title: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy
Author: Tathagata Chakraborti
Publish Year: 30 May 2017
Review Date: Tue, Sep 19, 2023
url: https://arxiv.org/pdf/1701.08317.pdf

Summary of paper

Motivation

Past work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence)
in this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate.

Contribution

They show how explanation can be seen as a “model reconciliation problem” (MRP), where AI system in effect suggests changes to the human’s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human’s mindset about the domain and task model such that the plan generated from the AI system fits human’s expectation.

Some key terms
Definition of a classical planning problem" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017" property="og:title"/>
<meta content="[TOC]
Title: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy Author: Tathagata Chakraborti Publish Year: 30 May 2017 Review Date: Tue, Sep 19, 2023 url: https://arxiv.org/pdf/1701.08317.pdf Summary of paper Motivation Past work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence) in this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate. Contribution They show how explanation can be seen as a “model reconciliation problem” (MRP), where AI system in effect suggests changes to the human’s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human’s mindset about the domain and task model such that the plan generated from the AI system fits human’s expectation. Some key terms Definition of a classical planning problem" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/cover.png" name="twitter:image"/>
<meta content="Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017",
      "item": "https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017
    <a aria-label="RSS" href="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy</li>
<li>Author: Tathagata Chakraborti</li>
<li>Publish Year: 30 May 2017</li>
<li>Review Date: Tue, Sep 19, 2023</li>
<li>url: <a href="https://arxiv.org/pdf/1701.08317.pdf">https://arxiv.org/pdf/1701.08317.pdf</a></li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<p><img alt="image-20230920160252585" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/cover.png"/></p>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>Past work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence)</li>
<li>in this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>They show how explanation can be seen as a “model reconciliation problem” (MRP), where AI system in effect suggests changes to the human’s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human’s mindset about the domain and task model such that the plan generated from the AI system fits human’s expectation.</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>Definition of a classical planning problem</strong></p>
<p><img alt="image-20230920171507603" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920171507603.png"/></p>
<p><img alt="image-20230920171515175" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920171515175.png"/></p>
<p><strong>explicable</strong>: i.e., generate plans that also make sense with respect to the humans’ model.</p>
<ul>
<li>limitation: Such explicability requirement however puts additional constraints on the agent’s plans, and may not always be feasible</li>
</ul>
<p><strong>being called on to “explain” its plan</strong>: when the robot’s plan is different from what the human would expect given his model of the world, t he robot wil lbe called on to “explain” its plan</p>
<ul>
<li>the author posit that such explanations should be seen as the robot’s attempt to move the human’s model to be in conformance with its own.</li>
</ul>
<p><strong>model reconciliation problem (MRP)</strong></p>
<ul>
<li>which aims to make minimal changes to the human’s model to bring it closer to the robot’s model
<ul>
<li><u><em>comment: would this also apply to the domain fixing situation?</em></u></li>
</ul>
</li>
<li><em>this paper’s assumption: the human’s model is made available and is in PDDL format just like the robot’s one.</em></li>
<li>here, the planner does not change its own behavior, but rather corrects the human’s incorrect perception of its model via explanations.</li>
</ul>
<p><strong>multi model explanations</strong></p>
<ul>
<li>in the paper, the author gave an example how an AI system should explain their plan when the action definition of the robot’s PDDL model differs from ones of the human’s PDDL model
<ul>
<li>I think there is a implicit assumption that the robot’s PDDL model is self-justifying. (i.e., no semantic errors within its own system)</li>
</ul>
</li>
</ul>
<p><strong>formal mathematical terms</strong></p>
<ul>
<li>
<p>$\delta_{\mathcal M}(\mathcal I, \pi) \models G$: a sequential transition from initial state $\mathcal I$ to goal state $G$</p>
</li>
<li>
<p>$C(\pi^{<em>}, M^{R}) = C^{</em>}_{M^{R}}$ : the total cost of optimal policy $\pi^*$ under robot PDDL model $M$.</p>
</li>
<li>
<p>$c_a$ is a single cost of action $a$</p>
</li>
</ul>
<p><img alt="image-20230920173320742" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920173320742.png"/></p>
<ul>
<li>$\mathcal F$ is the fluent universe. fluent is different from state, a state is a compound of fluents</li>
<li>new action set $\Lambda$ containing unit model change actions such that there is only a single change to a domain at a time.</li>
<li>$s_1 \Delta s_2$ means the number of changes from state $s_1$ to $s_2$</li>
<li><strong>Objective</strong> of this work: transforming model space from human model $\mathcal M^{H}$ to $\hat{\mathcal M}$
<ul>
<li><img alt="image-20230920195040422" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920195040422.png"/></li>
<li>the difference between the cost of the robot-generated optimal policy $\pi^*$ in the transformed model and the cost of the local optimal policy should be smaller than the one between the cost in Human model and the cost of its local optimal policy.</li>
<li><img alt="image-20230920195405456" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920195405456.png"/></li>
</ul>
</li>
</ul>
<h2 id="good-things-about-the-paper-one-paragraph">Good things about the paper (one paragraph)<a aria-hidden="true" class="anchor" hidden="" href="#good-things-about-the-paper-one-paragraph">#</a></h2>
<ul>
<li>it provided a new paradigm about how to regard the explanation of PDDL models.</li>
<li>The math part is however, too heavy for layman.</li>
</ul>
<p><img alt="image-20230920200743126" loading="lazy" src="/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/image-20230920200743126.png"/></p>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<p>Limitation:</p>
<ul>
<li>this paper assumed that the expert’s domain model is available and is in PDDL format. However, usually expert’s model is encoded in natural language and there is no available ground truth in PDDL format.</li>
</ul>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
