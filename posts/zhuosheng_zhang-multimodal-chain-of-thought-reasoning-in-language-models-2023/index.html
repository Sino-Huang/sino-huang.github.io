<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 | Sukai Huang</title>
<meta content="language model reasoning, prompt learning, multimodal learning" name="keywords"/>
<meta content="[TOC]

Title: Multimodal Chain of Thought Reasoning in Language Models
Author: Zhuosheng Zhang et. al.
Publish Year: 2023
Review Date: Wed, Feb 8, 2023
url: https://arxiv.org/pdf/2302.00923.pdf

Summary of paper

Motivation

LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.
to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning.
The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference.

Contribution

We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework.
The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference.

Some key terms
Multimodal-CoT" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023" property="og:title"/>
<meta content="[TOC]
Title: Multimodal Chain of Thought Reasoning in Language Models Author: Zhuosheng Zhang et. al. Publish Year: 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2302.00923.pdf Summary of paper Motivation LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. Contribution We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference. Some key terms Multimodal-CoT" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2023-02-08T22:23:45+11:00" property="article:published_time"/>
<meta content="2023-02-08T22:23:45+11:00" property="article:modified_time"/>
<meta content="Language Model Reasoning" property="article:tag"/>
<meta content="Prompt Learning" property="article:tag"/>
<meta content="Multimodal Learning" property="article:tag"/>
<meta content="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png" name="twitter:image"/>
<meta content="Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023" name="twitter:title"/>
<meta content="[TOC]

Title: Multimodal Chain of Thought Reasoning in Language Models
Author: Zhuosheng Zhang et. al.
Publish Year: 2023
Review Date: Wed, Feb 8, 2023
url: https://arxiv.org/pdf/2302.00923.pdf

Summary of paper

Motivation

LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.
to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning.
The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference.

Contribution

We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework.
The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference.

Some key terms
Multimodal-CoT" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023",
      "item": "https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023",
  "name": "Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023",
  "description": "[TOC]\nTitle: Multimodal Chain of Thought Reasoning in Language Models Author: Zhuosheng Zhang et. al. Publish Year: 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2302.00923.pdf Summary of paper Motivation LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. Contribution We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference. Some key terms Multimodal-CoT\n",
  "keywords": [
    "language model reasoning", "prompt learning", "multimodal learning"
  ],
  "articleBody": "[TOC]\nTitle: Multimodal Chain of Thought Reasoning in Language Models Author: Zhuosheng Zhang et. al. Publish Year: 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2302.00923.pdf Summary of paper Motivation LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. Contribution We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference. Some key terms Multimodal-CoT\nMultimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the answer. In general, there are two ways to elicit Multimodal-CoT reasoning as follows prompting LLMs fintuning small models the most immediate way to perform Multimodal-CoT is to transform the input of different modalities into one modality and prompt LLMs to perform CoT. Specificaly, it is possible to extract the caption of an image by a captioning model and then concatenate the caption with the original language input to be fed into LLM However, there is severe information loss in the captioning process, with a lack of mutual synergy in the representation space of different modalities. another solution is to fine-tune language models by fusing multimodal features. As this approach does not rely on LLMs and allows the flexibilit of adjusting model architecture to incorporate multimodal features, this paper focuses on fine-tuning model. the key challenge is that language models under 100 billion parameters tend to generate hallucinated rationals that mislead the answer inference Method By incorporating the vision features in both stages, the model is able to generate more effective rationales. Our experiments are conducted on the ScienceQA benchmark (Lu et al., 2022a), which is the latest multimodal reasoning benchmark with annotated reasoning chains. how does CoT prompting work\nthe model takes the concatenation of tokens of the question text (Q), the context text (C), and multiple options (M) as the input. The key issue arises, the rationale information may not contribute to predicting the right answer. Improving Few-Shot-CoT studies are categorized into two major research lines optimizing the demonstrations the key is the diversity of demonstration questions Partition questions of a given dataset into a few clusters. optimising the reasoning chains. problem decomposition vote over multiple reasoning paths for a test question Why rationales might not help the plausible reason might be that the model exceeds the maximum token limits before obtaining the required answer or stops generating the prediction early. To dive into how the rationales affect the answer prediction, we decouple the CoT problem into two stages, rationale generation and answer inference. We speculate that such a phenomenon of hallucination is due to a lack of necessary vision contexts for performing effective Multimodal-CoT. Model Solution rather than convert the vision information to captions, we convert the vision info into vision features use gated fusion CONCLUSION minimise the conversion from one modality to another modality because there will be information loss. ",
  "wordCount" : "548",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png","datePublished": "2023-02-08T22:23:45+11:00",
  "dateModified": "2023-02-08T22:23:45+11:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023
    </h1>
<div class="post-meta"><span title="2023-02-08 22:23:45 +1100 AEDT">February 8, 2023</span> · 3 min · 548 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="Summary of paper" href="#summary-of-paper">Summary of paper</a><ul>
<li>
<a aria-label="Motivation" href="#motivation">Motivation</a></li>
<li>
<a aria-label="Contribution" href="#contribution">Contribution</a></li>
<li>
<a aria-label="Some key terms" href="#some-key-terms">Some key terms</a></li></ul>
</li>
<li>
<a aria-label="Method" href="#method">Method</a></li>
<li>
<a aria-label="Improving Few-Shot-CoT" href="#improving-few-shot-cot">Improving Few-Shot-CoT</a><ul>
<li>
<a aria-label="Why rationales might not help" href="#why-rationales-might-not-help">Why rationales might not help</a></li>
<li>
<a aria-label="Model" href="#model">Model</a></li>
<li>
<a aria-label="Solution" href="#solution">Solution</a></li></ul>
</li>
<li>
<a aria-label="CONCLUSION" href="#conclusion">CONCLUSION</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Multimodal Chain of Thought Reasoning in Language Models</li>
<li>Author: Zhuosheng Zhang et. al.</li>
<li>Publish Year: 2023</li>
<li>Review Date: Wed, Feb 8, 2023</li>
<li>url: <a href="https://arxiv.org/pdf/2302.00923.pdf">https://arxiv.org/pdf/2302.00923.pdf</a></li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<p><img alt="image-20230208222840588" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png"/></p>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.</li>
<li>to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning.</li>
<li>The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework.</li>
<li>The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference.</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>Multimodal-CoT</strong></p>
<ul>
<li>Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the answer.</li>
<li>In general, there are two ways to elicit Multimodal-CoT reasoning as follows
<ul>
<li>prompting LLMs</li>
<li>fintuning small models</li>
</ul>
</li>
<li>the most immediate way to perform Multimodal-CoT is to transform the input of different modalities into one modality and prompt LLMs to perform CoT. Specificaly, it is possible to extract the caption of an image by a <strong><u>captioning model</u></strong> and then concatenate the caption with the original language input to be fed into LLM
<ul>
<li>However, there is severe information loss in the captioning process, with a lack of mutual synergy in the representation space of different modalities.</li>
</ul>
</li>
<li>another solution is to fine-tune language models by fusing multimodal features. As this approach does not rely on LLMs and allows the flexibilit of adjusting model architecture to incorporate multimodal features, this paper focuses on fine-tuning model.
<ul>
<li>the key challenge is that language models under 100 billion parameters tend to generate hallucinated rationals that mislead the answer inference</li>
</ul>
</li>
</ul>
<h2 id="method">Method<a aria-hidden="true" class="anchor" hidden="" href="#method">#</a></h2>
<ul>
<li>By incorporating the vision features in both stages, the model is able to generate more effective rationales.</li>
<li>Our experiments are conducted on the ScienceQA benchmark (Lu et al., 2022a),
which is the latest multimodal reasoning benchmark with annotated reasoning chains.</li>
</ul>
<p><strong>how does CoT prompting work</strong></p>
<ul>
<li>the model takes the concatenation of tokens of the question text (Q), the context text (C), and multiple options (M) as the input.</li>
<li><img alt="image-20230209142712706" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/image-20230209142712706.png"/></li>
<li>The key issue arises, the rationale information may not contribute to predicting the right answer.</li>
</ul>
<h2 id="improving-few-shot-cot">Improving Few-Shot-CoT<a aria-hidden="true" class="anchor" hidden="" href="#improving-few-shot-cot">#</a></h2>
<ul>
<li>studies are categorized into two major research lines
<ul>
<li>optimizing the demonstrations
<ul>
<li>the key is the diversity of demonstration questions
<ul>
<li>Partition questions of a given dataset into a few clusters.</li>
</ul>
</li>
</ul>
</li>
<li>optimising the reasoning chains.
<ul>
<li>problem decomposition</li>
<li>vote over multiple reasoning paths for a test question</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="why-rationales-might-not-help">Why rationales might not help<a aria-hidden="true" class="anchor" hidden="" href="#why-rationales-might-not-help">#</a></h3>
<ul>
<li>the plausible reason might be that the model exceeds the maximum token limits before obtaining the required answer or stops generating the prediction early.</li>
<li>To dive into how the rationales affect the answer prediction, we decouple the CoT problem into two stages, rationale generation and answer inference.</li>
<li><img alt="image-20230209154511739" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/image-20230209154511739.png"/></li>
<li>We speculate that such a phenomenon of hallucination is due to a lack of necessary vision contexts for performing effective Multimodal-CoT.</li>
</ul>
<h3 id="model">Model<a aria-hidden="true" class="anchor" hidden="" href="#model">#</a></h3>
<p><img alt="image-20230209154905449" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/image-20230209154905449.png"/></p>
<h3 id="solution">Solution<a aria-hidden="true" class="anchor" hidden="" href="#solution">#</a></h3>
<ul>
<li>rather than convert the vision information to captions, we convert the vision info into vision features</li>
<li>use gated fusion
<ul>
<li><img alt="image-20230209154924769" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/image-20230209154924769.png"/></li>
</ul>
</li>
<li><img alt="image-20230209160626385" loading="lazy" src="/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/image-20230209160626385.png"/></li>
</ul>
<h2 id="conclusion">CONCLUSION<a aria-hidden="true" class="anchor" hidden="" href="#conclusion">#</a></h2>
<ul>
<li>minimise the conversion from one modality to another modality because there will be information loss.</li>
</ul>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/language-model-reasoning/">Language Model Reasoning</a></li>
<li><a href="https://sino-huang.github.io/tags/prompt-learning/">Prompt Learning</a></li>
<li><a href="https://sino-huang.github.io/tags/multimodal-learning/">Multimodal Learning</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/">
<span class="title">« Prev</span>
<br/>
<span>Jacob_andreas Lammp Language Models as Probabilistic Priors for Perception and Action 2023</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/">
<span class="title">Next »</span>
<br/>
<span>Siyuan_wang Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning 2023</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on x" href="https://x.com/intent/tweet/?text=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f&amp;hashtags=languagemodelreasoning%2cpromptlearning%2cmultimodallearning" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f&amp;title=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023&amp;summary=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f&amp;title=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on whatsapp" href="https://api.whatsapp.com/send?text=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on telegram" href="https://telegram.me/share/url?text=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Zhuosheng_zhang%20Multimodal%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models%202023&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2fzhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
