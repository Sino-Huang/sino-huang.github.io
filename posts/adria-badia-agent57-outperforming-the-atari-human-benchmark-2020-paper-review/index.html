<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review | Sukai Huang</title>
<meta content="reinforcement learning, Atari-2600" name="keywords"/>
<meta content="[TOC]

Title: Agent57: Outperforming the Atari Human Benchmark 2020
Author: Adria Badia et. al.
Publish Year: 2020
Review Date: Nov 2021

Summary of paper

This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.

Agent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like “Montezuma’s Revenge, “Pitfall”, “Solaris” and “Skiing”." name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review" property="og:title"/>
<meta content="[TOC]
Title: Agent57: Outperforming the Atari Human Benchmark 2020 Author: Adria Badia et. al. Publish Year: 2020 Review Date: Nov 2021 Summary of paper This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.
Agent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like “Montezuma’s Revenge, “Pitfall”, “Solaris” and “Skiing”." property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/cute_avatar.jpg" name="twitter:image"/>
<meta content="Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review",
      "item": "https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review
    <a aria-label="RSS" href="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Agent57: Outperforming the Atari Human Benchmark 2020</li>
<li>Author: Adria Badia et. al.</li>
<li>Publish Year: 2020</li>
<li>Review Date: Nov 2021</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<blockquote>
<p>This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.</p>
</blockquote>
<p>Agent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like “Montezuma’s Revenge, “Pitfall”, “Solaris” and “Skiing”.</p>
<p>Before we understand how Agent57 works, we shall look at its ancestry and how it evolves from DQN agent in 2015.</p>
<h3 id="recurrent-replay-distributed-dqn-r2d2">Recurrent Replay Distributed DQN (R2D2)<a aria-hidden="true" class="anchor" hidden="" href="#recurrent-replay-distributed-dqn-r2d2">#</a></h3>
<hr/>
<h4 id="dqn-efficiency-and-effectiveness-improvements">DQN efficiency and effectiveness improvements<a aria-hidden="true" class="anchor" hidden="" href="#dqn-efficiency-and-effectiveness-improvements">#</a></h4>
<p><strong>Double DQN</strong> (<a href="https://www.youtube.com/watch?v=ECV5yeigZIg&amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;index=8">https://www.youtube.com/watch?v=ECV5yeigZIg&amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;index=8</a>)</p>
<p>the Original DQN would often overestimate the Q value (make it higher). It’s true because the maximum Q value estimated by the network may be <strong>outdated</strong> very quickly.</p>
<p><img alt="image-20211122172240986" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122172240986.png"/></p>
<p><strong><u><em>But</em></u></strong> if we has this online learning and update the Network immidiately for every action, then we would have <em>Correlated Sampling in Online-Q-Learning</em> problem.</p>
<p><img alt="image-20211122172606979" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122172606979.png"/></p>
<p>This cause the network to always overfit to a small neighbourhood of the states.</p>
<p>Besides that, the target Q value is generated by our current network. (in fact, we do not have exact target Q value from online learning because we haven’t reached the end of the game)</p>
<p>So, the agent need a <em>experience memory</em> (<strong>Experience Replay</strong>) to collect the experience of the whole episodes. The reason is because otherwise we cannot get our fixed Q-value label. (our Q value label require we complete enough rounds of episodes)</p>
<p>Advantages:</p>
<ul>
<li>Breaks correlations between consecutive samples</li>
<li>Each experience step may influence multiple gradient updates (no need to drop)</li>
</ul>
<p><img alt="image-20211122173506886" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122173506886.png"/></p>
<p><strong><u><em>But again</em></u></strong> our target Q value is generated by our current network and thus no stable gradient through this target Q value because it also moves as our network improves.</p>
<p>Solution -&gt; force our target Q value (obtained by Bellman function) fixed for some duration.</p>
<p><img alt="image-20211122173905838" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122173905838.png"/></p>
<p><img alt="image-20211122174103188" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122174103188.png"/></p>
<p><u><em><strong>Now let’s solve the overestimation problem</strong></em></u></p>
<p><img alt="image-20211122174324097" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122174324097.png"/></p>
<p><img alt="image-20211122174952768" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122174952768.png"/></p>
<p><strong>prioritised experience replay</strong> (<a href="https://www.youtube.com/watch?v=MqZmwQoOXw4">https://www.youtube.com/watch?v=MqZmwQoOXw4</a>)</p>
<p>we do not want randomly sample the experience.</p>
<p><img alt="image-20211122175300200" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122175300200.png"/></p>
<p>However, we need to take care of the sample distribution problem and we do not want to overfit to the small neighbourhood of those hard samples, thus we introduce a importance weight.</p>
<p><img alt="image-20211122175633202" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122175633202.png"/></p>
<p>Thus, now we <strong>coarsely</strong> learn the easy scenes but <strong>finely</strong> learn the hard scenes</p>
<p><strong>Dueling architecture</strong></p>
<p>For some states, estimating Q value of all actions is not useful.</p>
<p>(I may believe that this is a similar improvement as ResNet to ResNeXt (divide and conquer))</p>
<p><img alt="image-20211122180413596" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122180413596.png"/></p>
<p><img alt="image-20211122180456880" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211122180456880.png"/></p>
<hr/>
<h4 id="distributed-agents">Distributed agents<a aria-hidden="true" class="anchor" hidden="" href="#distributed-agents">#</a></h4>
<p>distributed RL agent decouples the data collection and learning process.</p>
<p>Many <strong>actors</strong> interact with independent copies of the environment, feeding data to a central ‘memory bank’ in the form of a prioritized replay buffer.</p>
<p>A <strong>learner</strong> then samples training data from this replay buffer and get trained.</p>
<p>The <strong>learner</strong> weights are sent to the <strong>actors</strong> frequently, allowing actors to update their own weights in a manner determined by their <em>individual priorities</em>.</p>
<p><img alt="image-20211123120405436" loading="lazy" src="/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/image-assets/image-20211123120405436.png"/></p>
<h4 id="short-term-memory">Short-term memory<a aria-hidden="true" class="anchor" hidden="" href="#short-term-memory">#</a></h4>
<p>Memory allows agents to make decisions based on a sequence of observations, which can reveal more information about the environment as a whole.</p>
<p>Therefore the role of memor is to aggregate information from past observations to improve the decision making procecss. In deep RL and deep learning, RNN such as LSTM are used as short term memories.</p>
<p>But how to combine memories with off-policy learner?</p>
<p>(R2D2)</p>
<p>Instead of regular (s,a,r,s’) transition tuples in the replay buffer, we store fixed-length (m=80) sequence of (s,a,r), with adjacent sequence overlapping each other by 40 time steps, and never crossing episode boundaries. When training, we unroll bot online and target networks on the same sequence of states to generate value estimates and targets.</p>
<h3 id="never-give-up-rl-agent">Never Give Up RL agent<a aria-hidden="true" class="anchor" hidden="" href="#never-give-up-rl-agent">#</a></h3>
<h4 id="episodic-memory">Episodic memory<a aria-hidden="true" class="anchor" hidden="" href="#episodic-memory">#</a></h4>
<p>The episodic memory M is a <strong><u>dynamically-sized slot-based memory</u></strong> that stores the controllable states in an online fashion (add memory on the fly). At time t, the memory contains the controllable states of all the observations visited in the <strong><u>current episode</u></strong>, ${f(x_0), …, f(x_t-1)}$</p>
<p>this enables the agent to detect when new espisodes are encountered, so the agent can explore more when encountering unseen environments rather than exploit.</p>
<h4 id="exploration-improvement">Exploration improvement<a aria-hidden="true" class="anchor" hidden="" href="#exploration-improvement">#</a></h4>
<p><strong>Intrinsic motivation rewards</strong> encourages an agent to explore and visit as many states as possible by providing more dense “internal” rewards for novelty-seeking behaviours.</p>
<p>There are two types of rewards</p>
<ul>
<li>long-term novelty rewards that encourage visiting many states throughout training, across many episodes.
<ul>
<li>adjusted by how often the agent has seen a state similar to the current one relative to states seen overall.</li>
</ul>
</li>
<li>Short-term novelty rewards that encourage visiting many states within a single episode of a game.
<ul>
<li>use the episodic memory to recognise novel experiences.</li>
<li>the magnitude of the reward is determined by measuring the distance between the present state and previous state recorded in episodic memory.</li>
</ul>
</li>
</ul>
<p>One thing about measuring the novelty is that measuring the <strong>novelty features</strong> rather than the original observations would be a better approach.</p>
<p>After that, <strong>assigning</strong> actors with different policies based on the importance weighting on the total novelty reward would produce different experiences, ensuring more exploration</p>
<h3 id="meta-controller">Meta controller<a aria-hidden="true" class="anchor" hidden="" href="#meta-controller">#</a></h3>
<p>Use a traditional <em><u>sliding window UCB bandit</u></em> to control the <em><strong>exploration factor</strong></em> as well as the <em><strong>discount factor</strong></em></p>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>Extensive exploration problem</strong></p>
<p>For Montezuma’s Revenge and Pitfall, a extensive exploration is required for the agent to understand the environment.</p>
<p><strong>Long-term credit assignment problem</strong></p>
<p>For Skiing and Solaris Atari games, the current action may affect the far future reward and thus a low discount factor for the Q value is not suitable.</p>
<p><strong>On-policy learner</strong></p>
<p>which can only learn the value of its direct actions.</p>
<p><strong>Off-policy learner</strong></p>
<p>which can learn about optimal actions even when not performing those actions.</p>
<p>e.g., it might be taking random actions, but can still learn what the best possible action would be later.</p>
<p>Off-policy learning is therefore a desirable property for agents, helping them learn the best course of action to take while thoroughly exploring their environments.</p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
