<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 | Sukai Huang</title>
<meta content="natural language processing, multimodal, future work" name="keywords"/>
<meta content="[TOC]

Title: Flamingo: a Visual Language Model for Few-Shot Learning
Author: Jean-Baptiste Alayrac et. al.
Publish Year: Apr 2022
Review Date: May 2022

Summary of paper
Flamingo architecture
Pretrained vision encoder: from pixels to features
the model‚Äôs vision encoder is a pretrained Normalizer-Free ResNet (NFNet)
they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper ‚ÄúLearning Transferable Visual Models From Natural Language Supervision‚Äù" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022" property="og:title"/>
<meta content="[TOC]
Title: Flamingo: a Visual Language Model for Few-Shot Learning Author: Jean-Baptiste Alayrac et. al. Publish Year: Apr 2022 Review Date: May 2022 Summary of paper Flamingo architecture Pretrained vision encoder: from pixels to features
the model‚Äôs vision encoder is a pretrained Normalizer-Free ResNet (NFNet)
they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper ‚ÄúLearning Transferable Visual Models From Natural Language Supervision‚Äù" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2022-05-11T16:35:03+10:00" property="article:published_time"/>
<meta content="2022-05-11T16:35:03+10:00" property="article:modified_time"/>
<meta content="Natural Language Processing" property="article:tag"/>
<meta content="Multimodal" property="article:tag"/>
<meta content="Future Work" property="article:tag"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" name="twitter:image"/>
<meta content="Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022" name="twitter:title"/>
<meta content="[TOC]

Title: Flamingo: a Visual Language Model for Few-Shot Learning
Author: Jean-Baptiste Alayrac et. al.
Publish Year: Apr 2022
Review Date: May 2022

Summary of paper
Flamingo architecture
Pretrained vision encoder: from pixels to features
the model‚Äôs vision encoder is a pretrained Normalizer-Free ResNet (NFNet)
they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper ‚ÄúLearning Transferable Visual Models From Natural Language Supervision‚Äù" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022",
      "item": "https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022",
  "name": "Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022",
  "description": "[TOC]\nTitle: Flamingo: a Visual Language Model for Few-Shot Learning Author: Jean-Baptiste Alayrac et. al. Publish Year: Apr 2022 Review Date: May 2022 Summary of paper Flamingo architecture Pretrained vision encoder: from pixels to features\nthe model\u0026rsquo;s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)\nthey pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper \u0026ldquo;Learning Transferable Visual Models From Natural Language Supervision\u0026rdquo;\n",
  "keywords": [
    "natural language processing", "multimodal", "future work"
  ],
  "articleBody": "[TOC]\nTitle: Flamingo: a Visual Language Model for Few-Shot Learning Author: Jean-Baptiste Alayrac et. al. Publish Year: Apr 2022 Review Date: May 2022 Summary of paper Flamingo architecture Pretrained vision encoder: from pixels to features\nthe model‚Äôs vision encoder is a pretrained Normalizer-Free ResNet (NFNet)\nthey pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper ‚ÄúLearning Transferable Visual Models From Natural Language Supervision‚Äù\nHaving contrastive objective means that the visual encoder model is already stay in the same latent space of the BERT language model.\nThe output of the final stage is a 2D spatial grid of feature $X_f$, later 2D feature will be flattened to 1D\ntrainable Perceiver Resampler: from varying-size large feature maps to few visual tokens.\nthe Perceiver Resampler module connects the vision encoder to the frozen language model\ninput: a variable number of image features\noutput: a fixed number of visual outputs (in practical 64)\nmotivation: significantly reduce the computational complexity of vision text cross attention. (particularly important when dealing with multiple long videos)\nwhat do they need: in order to get the fixed number of output, we need to have a fixed number of query tokens\nso, we learn a predefined number of latent input queries. these latent queries are fed to a transformer stack and cross attend to the flattened visual features $X_f$\nThe keys and values computed from the learnt latents are concatenated to the keys and values obtained from $X_f$, which we found to perform slightly better.\nfrozen language model\nthe pretrained text-only model is a decoder-only model.\nbut in order to let the frozen language model fit the current situation, they introduced a gated xatten-dense layer\nthey also apply layer normalisation to the keys, values and queries\nmulti-image attention\nThey limit the number of visual tokens that a certain text token sees.\nTypically, they allow each token to attend to the tokens of the image that appeared just before it in the interleaved sequence. (this means we have temporal matching when we do cross attention between images and text)\nAlthough the model can only directly attend to a single image at any given point, there is still a causal dependency on all previous images in the sequence via causal self-attention in the text decoder.\nHow to train the model‚Äôs parameters They train the models by minimizing a weighted sum of dataset specific expected negative log likelihood of text given some visual inputs.\nIn practice, at each step of optimisation we go over each dataset Dùëö in turn, sample a batch of size ùêµùëö of visual language sequences from it, compute the gradient of the loss with respect to the minibatch and weight it by ùúÜùëö. We then accumulate the gradients over all ùëÄ datasets before triggering an update step. We found this gradient accumulation strategy to be crucial for high performance compared to a round-robin approach. (how to deal with multiple training dataset)\nbut how to flow the gradient given that we need to freeze some modules\nactually if we set requires_grad = False, everything still works ok\nhttps://discuss.pytorch.org/t/will-freezing-an-intermediate-block-influence-the-gradient-flow/88859/4\nPotential future work yes, we can use this model for any multi-modal tasks.\n",
  "wordCount" : "536",
  "inLanguage": "en",
  "image": "https://sino-huang.github.io/sukai_avatar.jpg","datePublished": "2022-05-11T16:35:03+10:00",
  "dateModified": "2022-05-11T16:35:03+10:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a>¬†¬ª¬†<a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022
    </h1>
<div class="post-meta"><span title="2022-05-11 16:35:03 +1000 AEST">May 11, 2022</span>¬†¬∑¬†3 min¬†¬∑¬†Sukai Huang¬†|¬†<a href="mailto:sukaih@student.unimelb.edu.au/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header> <div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="Summary of paper" href="#summary-of-paper">Summary of paper</a><ul>
<li>
<a aria-label="Flamingo architecture" href="#flamingo-architecture">Flamingo architecture</a></li>
<li>
<a aria-label="How to train the model‚Äôs parameters" href="#how-to-train-the-models-parameters">How to train the model‚Äôs parameters</a></li></ul>
</li>
<li>
<a aria-label="Potential future work" href="#potential-future-work">Potential future work</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Flamingo: a Visual Language Model for Few-Shot Learning</li>
<li>Author: Jean-Baptiste Alayrac et. al.</li>
<li>Publish Year: Apr 2022</li>
<li>Review Date: May 2022</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="flamingo-architecture">Flamingo architecture<a aria-hidden="true" class="anchor" hidden="" href="#flamingo-architecture">#</a></h3>
<p><strong>Pretrained</strong> <strong>vision encoder: from pixels to features</strong></p>
<p>the model‚Äôs vision encoder is a pretrained Normalizer-Free ResNet (NFNet)</p>
<p>they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper ‚ÄúLearning Transferable Visual Models From Natural Language Supervision‚Äù</p>
<p><img alt="image-20220511204113119" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511204113119.png"/></p>
<p>Having contrastive objective means that the visual encoder model is already stay in the same latent space of the BERT language model.</p>
<p>The output of the final stage is a 2D spatial grid of feature $X_f$, later 2D feature will be flattened to 1D</p>
<p><strong>trainable</strong> <strong>Perceiver Resampler</strong>: from varying-size large feature maps to few visual tokens.</p>
<p>the Perceiver Resampler module connects the vision encoder to the frozen language model</p>
<p><u>input</u>: a variable number of image features</p>
<p><u>output</u>: a fixed number of visual outputs (in practical 64)</p>
<p><u>motivation</u>: significantly reduce the computational complexity of vision text cross attention. (particularly important when dealing with multiple long videos)</p>
<p>what do they need: in order to get the fixed number of output, we need to have a fixed number of query tokens</p>
<p>so, we learn a predefined number of latent input queries. these latent queries are fed to a transformer stack and cross attend to the flattened visual features $X_f$</p>
<p>The keys and values computed from the learnt latents are concatenated to the keys and values obtained from $X_f$, which we found to perform slightly better.</p>
<p><img alt="image-20220511223348735" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511223348735.png"/></p>
<p><strong>frozen language model</strong></p>
<p>the pretrained text-only model is a decoder-only model.</p>
<p>but in order to let the frozen language model fit the current situation, they introduced a gated xatten-dense layer</p>
<p>they also apply layer normalisation to the keys, values and queries</p>
<p><img alt="image-20220511224452155" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511224452155.png"/></p>
<p><strong>multi-image attention</strong></p>
<p>They limit the number of visual tokens that a certain text token sees.</p>
<p>Typically, they allow each token to attend to the tokens of the image that appeared just before it in the interleaved sequence. (this means we have temporal matching when we do cross attention between images and text)</p>
<p><img alt="image-20220511230849495" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511230849495.png"/></p>
<p>Although the model can only directly attend to a single image at any given point, there is still a causal dependency on all previous images in the sequence via causal self-attention in the text decoder.</p>
<h3 id="how-to-train-the-models-parameters">How to train the model‚Äôs parameters<a aria-hidden="true" class="anchor" hidden="" href="#how-to-train-the-models-parameters">#</a></h3>
<p>They train the models by minimizing a weighted sum of dataset specific expected negative log likelihood of text given some visual inputs.</p>
<p><img alt="image-20220511231410644" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511231410644.png"/></p>
<p>In practice, at each step of optimisation we go over each dataset Dùëö in turn, sample a batch of size ùêµùëö of visual language sequences from it, compute the gradient of the loss with respect to the minibatch and weight it by ùúÜùëö. We then accumulate the gradients over all ùëÄ datasets before triggering an update step. We found this gradient accumulation strategy to be crucial for high performance compared to a round-robin approach. (how to deal with multiple training dataset)</p>
<p><strong>but how to flow the gradient given that we need to freeze some modules</strong></p>
<p><img alt="image-20220511232111835" loading="lazy" src="/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/image-assets/image-20220511232111835.png"/></p>
<p>actually if we set requires_grad = False, everything still works ok</p>
<p><a href="https://discuss.pytorch.org/t/will-freezing-an-intermediate-block-influence-the-gradient-flow/88859/4">https://discuss.pytorch.org/t/will-freezing-an-intermediate-block-influence-the-gradient-flow/88859/4</a></p>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<p>yes, we can use this model for any multi-modal tasks.</p>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/natural-language-processing/">Natural Language Processing</a></li>
<li><a href="https://sino-huang.github.io/tags/multimodal/">Multimodal</a></li>
<li><a href="https://sino-huang.github.io/tags/future-work/">Future Work</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/a-brief-overview-of-rank-based-prioritized-experience-replay-2022/">
<span class="title">¬´ Prev</span>
<br/>
<span>A Brief Overview of Rank Based Prioritized Experience Replay 2016</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/angela_fan-augmenting-transformer-with-knn-composite-memory-for-dialog-2021/">
<span class="title">Next ¬ª</span>
<br/>
<span>Angela_fan Augmenting Transformer With Knn Composite Memory for Dialog 2021</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on x" href="https://x.com/intent/tweet/?text=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f&amp;hashtags=naturallanguageprocessing%2cmultimodal%2cfuturework" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f&amp;title=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022&amp;summary=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f&amp;title=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on whatsapp" href="https://api.whatsapp.com/send?text=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on telegram" href="https://telegram.me/share/url?text=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Deepmind%20Flamingo%20a%20Visual%20Language%20Model%20for%20Few%20Shot%20Learning%202022&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2fdeepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>¬© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
