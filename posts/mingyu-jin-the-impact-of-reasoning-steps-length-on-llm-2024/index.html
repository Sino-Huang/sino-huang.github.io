<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024 | Sukai Huang</title>
<meta content="llm cot, llm empirical strategy" name="keywords"/>
<meta content="[TOC]

Title: The Impact of Reasoning Steps Length on Large Language Models
Author: Mingyu Jin et. al.
Publish Year: 20 Jan 2024
Review Date: Mon, Jan 29, 2024
url: arXiv:2401.04925v3

Summary of paper
Contribution
The study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:


Effect of Reasoning Step Length:" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024" property="og:title"/>
<meta content="[TOC]
Title: The Impact of Reasoning Steps Length on Large Language Models Author: Mingyu Jin et. al. Publish Year: 20 Jan 2024 Review Date: Mon, Jan 29, 2024 url: arXiv:2401.04925v3 Summary of paper Contribution The study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:
Effect of Reasoning Step Length:" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/image-assets/cover.png" name="twitter:image"/>
<meta content="Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024",
      "item": "https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024
    <a aria-label="RSS" href="/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: The Impact of Reasoning Steps Length on Large Language Models</li>
<li>Author: Mingyu Jin et. al.</li>
<li>Publish Year: 20 Jan 2024</li>
<li>Review Date: Mon, Jan 29, 2024</li>
<li>url: arXiv:2401.04925v3</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<p>The study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:</p>
<ol>
<li>
<p><strong>Effect of Reasoning Step Length</strong>:</p>
<ul>
<li>Lengthening reasoning steps in prompts, even without introducing new information, notably improves LLMs’ reasoning abilities across various datasets.</li>
<li>Conversely, shortening reasoning steps, while preserving key information, notably diminishes LLMs’ reasoning abilities.</li>
<li>This suggests the critical role of reasoning step length in CoT prompts and offers practical insights for leveraging LLMs in complex problem-solving scenarios.</li>
</ul>
</li>
<li>
<p><strong>Impact of Rationales</strong>:</p>
<ul>
<li>Surprisingly, even incorrect rationales can lead to favorable outcomes if they maintain the necessary length of inference.</li>
<li>This finding suggests that the length of reasoning steps may compensate for inaccuracies in rationales, emphasizing the importance of sequence length in CoT.</li>
</ul>
</li>
<li>
<p><strong>Task-Dependent Nature</strong>:</p>
<ul>
<li>The advantages of increasing reasoning steps vary depending on the complexity of tasks:
<ul>
<li>Simpler tasks require fewer steps.</li>
<li>Complex tasks benefit significantly from longer inference sequences.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The study underscores the significance of reasoning step length in CoT prompts for enhancing LLMs’ reasoning abilities and provides practical guidance for optimizing their performance in diverse problem-solving contexts.</p>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>incorrect but coherent rationales can improve reasoning performance</strong></p>
<ul>
<li>Interestingly, Wang et al. found that even incorrect but coherent rationales can improve reasoning performance, highlighting the value of logical continuity (Wang et al., 2023).</li>
</ul>
<p><strong>Strategies</strong></p>
<p>Few-shot setting</p>
<ul>
<li>think about the word. This process does not introduce new information.</li>
<li>Read the question again: Read the questions repeatedly to reduce the interference of other texts on the chain of thought.</li>
<li>Repeat State: we include a small summary of the current state after a long chain of reasoning</li>
<li>Self-Verification: before the model gets the answer, we add a self-verification process to judge whether the answer is reasonable based on some basic information.</li>
</ul>
<p>Zero-shot setting</p>
<ul>
<li>altered the initial prompt from “Let’s think step by step" to “Let’s think step by step, you must think more steps"</li>
</ul>
<h2 id="results">Results<a aria-hidden="true" class="anchor" hidden="" href="#results">#</a></h2>
<p>The study emphasizes the significance of the length of the thinking chain rather than its accuracy in improving Chain of Thought (CoT) performance. Here are the key findings:</p>
<ol>
<li>
<p><strong>Linear Correlation between Step Count and Accuracy</strong>:</p>
<ul>
<li>In few-shot CoT scenarios, there exists a direct linear correlation between the number of reasoning steps and accuracy.</li>
<li>Lengthening reasoning steps notably enhances Large Language Models’ (LLMs) reasoning abilities across multiple datasets.</li>
<li>Conversely, shortening reasoning steps significantly diminishes model performance, even when key information is preserved.</li>
</ul>
</li>
<li>
<p><strong>Role of Incorrect Rationales</strong>:</p>
<ul>
<li>Even incorrect rationales can produce favorable outcomes if they maintain the necessary length of inference.</li>
<li>Errors in intermediate numbers, particularly in process-oriented tasks like mathematical problems, have a minor impact on overall performance.</li>
</ul>
</li>
<li>
<p><strong>Task-Dependent Nature</strong>:</p>
<ul>
<li>The benefits of increasing reasoning steps depend on the complexity of tasks:
<ul>
<li>Simpler tasks require fewer steps.</li>
<li>More complex tasks benefit significantly from longer inference sequences.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Enhancement in Zero-Shot CoT</strong>:</p>
<ul>
<li>Increasing reasoning steps in zero-shot CoT notably improves LLM accuracy.</li>
<li>Altering the initial prompt to explicitly encourage more reasoning steps led to noticeable enhancements, particularly in datasets involving mathematical problems.</li>
</ul>
</li>
</ol>
<p>Overall, the findings suggest that optimizing CoT prompting involves prioritizing the length of the reasoning chain, which significantly impacts LLMs’ reasoning abilities across various tasks and scenarios.</p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
