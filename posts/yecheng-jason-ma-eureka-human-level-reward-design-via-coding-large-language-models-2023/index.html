<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 | Sukai Huang</title>
<meta content="reward design, large language model, llm for reward function" name="keywords"/>
<meta content="[TOC]

Title: Eureka Human Level Reward Design via Coding Large Language Models 2023
Author: Yecheng Jason Ma et. al.
Publish Year: 19 Oct 2023
Review Date: Fri, Oct 27, 2023
url: https://arxiv.org/pdf/2310.12931.pdf

Summary of paper

Motivation

harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem.
we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning.

Contribution

Eureka generate reward functions that outperform expert human-engineered rewards.
the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF)


Some key terms

given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling.
As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice

reward design problem" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023" property="og:title"/>
<meta content="[TOC]
Title: Eureka Human Level Reward Design via Coding Large Language Models 2023 Author: Yecheng Jason Ma et. al. Publish Year: 19 Oct 2023 Review Date: Fri, Oct 27, 2023 url: https://arxiv.org/pdf/2310.12931.pdf Summary of paper Motivation harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem. we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning. Contribution Eureka generate reward functions that outperform expert human-engineered rewards. the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) Some key terms given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling. As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice reward design problem" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2023-10-27T16:44:22+11:00" property="article:published_time"/>
<meta content="2023-10-27T16:44:22+11:00" property="article:modified_time"/>
<meta content="Reward Design" property="article:tag"/>
<meta content="Large Language Model" property="article:tag"/>
<meta content="Llm for Reward Function" property="article:tag"/>
<meta content="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png" name="twitter:image"/>
<meta content="Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023" name="twitter:title"/>
<meta content="[TOC]

Title: Eureka Human Level Reward Design via Coding Large Language Models 2023
Author: Yecheng Jason Ma et. al.
Publish Year: 19 Oct 2023
Review Date: Fri, Oct 27, 2023
url: https://arxiv.org/pdf/2310.12931.pdf

Summary of paper

Motivation

harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem.
we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning.

Contribution

Eureka generate reward functions that outperform expert human-engineered rewards.
the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF)


Some key terms

given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling.
As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice

reward design problem" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023",
      "item": "https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023",
  "name": "Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023",
  "description": "[TOC]\nTitle: Eureka Human Level Reward Design via Coding Large Language Models 2023 Author: Yecheng Jason Ma et. al. Publish Year: 19 Oct 2023 Review Date: Fri, Oct 27, 2023 url: https://arxiv.org/pdf/2310.12931.pdf Summary of paper Motivation harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem. we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning. Contribution Eureka generate reward functions that outperform expert human-engineered rewards. the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) Some key terms given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling. As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice reward design problem\n",
  "keywords": [
    "reward design", "large language model", "llm for reward function"
  ],
  "articleBody": "[TOC]\nTitle: Eureka Human Level Reward Design via Coding Large Language Models 2023 Author: Yecheng Jason Ma et. al. Publish Year: 19 Oct 2023 Review Date: Fri, Oct 27, 2023 url: https://arxiv.org/pdf/2310.12931.pdf Summary of paper Motivation harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem. we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning. Contribution Eureka generate reward functions that outperform expert human-engineered rewards. the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) Some key terms given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling. As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice reward design problem\nCurriculum learning\nthe paper mentioned that they used curriculum learning to train the model. here are the key aspects of Curriculum learning: gradual complexity increase Curriculum learning starts by training models on easier or simpler tasks before gradually increasing the complexity of the tasks improved learning efficiency and generalisation Structured learning path the curriculum provides a structured learning path, allowing the model to build upon previously learned concepts implementation implementing curriculum learning may involve designing a curriculum, which is a sequence of tasks of increasing complexity relation to other concepts curriculum learning shares similarities with concepts like transfer learning and multi-task learning, but with a focus on the structured, gradual increase in task complexity. Some insights for the practical implementation How does the model ensure the (semantic) correctness of the reward function ?\nEUREKA requires the environment specification to provided to the LLM. they directly feeding the raw environment code as context. reason: the environment source code typically reveals what the environment semantically entails and which variables can and should be used to compose a reward function for the specified task they used evolutionary search to address the execution error and sub-optimality challenges. a very big assumption behind the success of this method evolutionary search, i.e., refine the reward in the next iteration based on the performance in the current iteration. simply specifying the mutation operator as a text prompt that suggests a few general ways to modify an existing reward code based on a textual summary of policy training textual summary of policy training : after the policy training, we evaluate the policy performance basically it is a loop to refine the reward function. How does the model avoid the computer vision part ?\nthey directly access the source code to obtain “processed” observations. https://github.com/eureka-research/Eureka/blob/main/eureka/envs/isaac/shadow_hand_obs.py some important parameters are explicitly stated (e.g., position, velocity, angle velocity) maybe we shall also do this to avoid multimodal and CV part. Minor comments Potential future work having LLMs to generate reward function directly to train agents is something like replacing human experts with LLMs to design reward functions for learning agents.\nThe pipeline of this work is as follows:\nthe human users gave the task descriptions -\u003e the LLM convert the descriptions into a reward function -\u003e the reward function helps to train the agent that can accomplish the task -\u003e the human users can give further requirements in the next iteration -\u003e the LLM will further adjust the reward function to fit the users’ need. Hybrid model Nir suggested that we can have a hybrid model such that conditions and effects expressed partially in predicate logic, and partially specified through imperative programming languages\nI think the it really depends on the type of the tasks\nif the task is about planning or scheduling, e.g., Sudoku, then imperative programming has nothing to do with this if the task is about low-level controls that have no explicit discrete procedures, then defining a reward function (this work) is suitable if the task description explicitly contains the steps, then converting it to imperative programming language is suitable. so it really depends on what is the task description is\ne.g., cooking task provided with steps info -\u003e imperative programming e.g., “your task is to stack block A on top of B” -\u003e predicate logic as conditions and effects so the hybrid model is somehow like a big model containing multiple specialised models that handles various types of tasks.\nImperative programming version of the action definition both reward function generation for actions or direct imperative function for actions can be used as auxiliary information to tune the PDDL action definition\nReward function example from GPT4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def reward_function(state, action): reward = 0 # Assuming `is_on_ladder` and `is_moving_down` are functions that # determine whether the agent is on a ladder and moving down, respectively. if is_on_ladder(state) and is_moving_down(action): reward += 1 # Give a positive reward for moving down a ladder # Optionally, penalize the agent for not moving down a ladder while on it elif is_on_ladder(state) and not is_moving_down(action): reward -= 1 # Optionally, penalize or reward other behaviors # ... return reward Imperative function example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class Environment: def __init__(self): self.agent_position = (0, 0) # Assume the agent starts at the top-left corner self.ladder_position = (5, 5) # Assume there's a ladder at position (5, 5) def get_agent_position(self): return self.agent_position def move_agent(self, direction): x, y = self.agent_position if direction == \"down\" and self.is_ladder_below(): y += 1 elif direction == \"left\": x -= 1 elif direction == \"right\": x += 1 elif direction == \"up\": y -= 1 self.agent_position = (x, y) def is_ladder_below(self): x, y = self.agent_position return (x, y + 1) == self.ladder_position def is_on_ladder(self): return self.agent_position == self.ladder_position def climb_down_ladder(env): while not env.is_on_ladder(): # Move towards the ladder agent_position = env.get_agent_position() ladder_position = env.ladder_position if agent_position[0] \u003c ladder_position[0]: env.move_agent(\"right\") elif agent_position[0] \u003e ladder_position[0]: env.move_agent(\"left\") elif agent_position[1] \u003c ladder_position[1]: env.move_agent(\"down\") # Now on the ladder, climb down for _ in range(5): # Assume ladder is 5 cells tall env.move_agent(\"down\") def check_climb_down_complete(env): agent_position = env.get_agent_position() ladder_position = env.ladder_position # Check if the agent's y-coordinate is the same or below the ladder's bottom y-coordinate if agent_position[1] \u003e= ladder_position[1] and env.is_on_ladder(): return True # Climbing down the ladder is complete else: return False # Climbing down the ladder is not complete # Usage: env = Environment() climb_down_ladder(env) both reward and imperative action function contains state checking (i.e., is_on_ladder and is_moving_down etc. ) the imperative programming version of the “climb_down_ladder” action contains a while loop that controls the agent to move towards the ladder before climbing down. This is different from the PDDL version action definition, where is_on_ladder is the precondition of the action. ",
  "wordCount" : "1163",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png","datePublished": "2023-10-27T16:44:22+11:00",
  "dateModified": "2023-10-27T16:44:22+11:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023
    </h1>
<div class="post-meta"><span title="2023-10-27 16:44:22 +1100 AEDT">October 27, 2023</span> · 6 min · 1163 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="Summary of paper" href="#summary-of-paper">Summary of paper</a><ul>
<li>
<a aria-label="Motivation" href="#motivation">Motivation</a></li>
<li>
<a aria-label="Contribution" href="#contribution">Contribution</a></li>
<li>
<a aria-label="Some key terms" href="#some-key-terms">Some key terms</a></li>
<li>
<a aria-label="Some insights for the practical implementation" href="#some-insights-for-the-practical-implementation">Some insights for the practical implementation</a></li></ul>
</li>
<li>
<a aria-label="Minor comments" href="#minor-comments">Minor comments</a></li>
<li>
<a aria-label="Potential future work" href="#potential-future-work">Potential future work</a><ul>
<li>
<a aria-label="Hybrid model" href="#hybrid-model">Hybrid model</a></li>
<li>
<a aria-label="Imperative programming version of the action definition" href="#imperative-programming-version-of-the-action-definition">Imperative programming version of the action definition</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Eureka Human Level Reward Design via Coding Large Language Models 2023</li>
<li>Author: Yecheng Jason Ma et. al.</li>
<li>Publish Year: 19 Oct 2023</li>
<li>Review Date: Fri, Oct 27, 2023</li>
<li>url: <a href="https://arxiv.org/pdf/2310.12931.pdf">https://arxiv.org/pdf/2310.12931.pdf</a></li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<p><img alt="image-20231027164539472" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png"/></p>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem.</li>
<li>we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>Eureka generate reward functions that outperform expert human-engineered rewards.</li>
<li>the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF)</li>
<li><img alt="image-20231030132136067" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030132136067.png"/></li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<ul>
<li>given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling.</li>
<li>As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice</li>
</ul>
<p><strong>reward design problem</strong></p>
<p><img alt="image-20231027231538244" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231027231538244.png"/></p>
<p><img alt="image-20231027231736218" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231027231736218.png"/></p>
<p><strong>Curriculum learning</strong></p>
<p><img alt="image-20231030115254039" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030115254039.png"/></p>
<ul>
<li>the paper mentioned that they used curriculum learning to train the model.</li>
<li>here are the key aspects of Curriculum learning:
<ul>
<li>gradual complexity increase
<ul>
<li>Curriculum learning starts by training models on easier or simpler tasks before gradually increasing the complexity of the tasks</li>
</ul>
</li>
<li>improved learning efficiency and generalisation</li>
<li>Structured learning path
<ul>
<li>the curriculum provides a structured learning path, allowing the model to build upon previously learned concepts</li>
</ul>
</li>
<li>implementation
<ul>
<li>implementing curriculum learning may involve designing a curriculum, which is a sequence of tasks of increasing complexity</li>
</ul>
</li>
<li>relation to other concepts
<ul>
<li>curriculum learning shares similarities with concepts like transfer learning and multi-task learning, but with a focus on the structured, gradual increase in task complexity.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="some-insights-for-the-practical-implementation">Some insights for the practical implementation<a aria-hidden="true" class="anchor" hidden="" href="#some-insights-for-the-practical-implementation">#</a></h3>
<p><strong>How does the model ensure the (semantic) correctness of the reward function ?</strong></p>
<ol>
<li>EUREKA requires the environment specification to provided to the LLM. they directly feeding the raw environment code as context.
<ul>
<li>reason: the environment source code typically reveals what the environment semantically entails and which variables can and should be used to compose a reward function for the specified task</li>
</ul>
</li>
<li>they used evolutionary search to address the execution error and sub-optimality challenges.
<ul>
<li>a very big <strong>assumption</strong> behind the success of this method</li>
<li><img alt="image-20231030133130933" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030133130933.png"/></li>
<li>evolutionary search, i.e., refine the reward in the next iteration based on the performance in the current iteration.
<ul>
<li>simply specifying the mutation operator as a text prompt that suggests a few general ways to modify an existing reward code based on a textual summary of policy training</li>
<li>textual summary of policy training : after the policy training, we evaluate the policy performance
<ul>
<li><img alt="image-20231030133732964" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030133732964.png"/></li>
</ul>
</li>
<li>basically it is a loop to refine the reward function.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>How does the model avoid the computer vision part ?</strong></p>
<ul>
<li>they directly access the source code to obtain “processed” observations.
<ul>
<li><a href="https://github.com/eureka-research/Eureka/blob/main/eureka/envs/isaac/shadow_hand_obs.py">https://github.com/eureka-research/Eureka/blob/main/eureka/envs/isaac/shadow_hand_obs.py</a></li>
<li>some important parameters are explicitly stated (e.g., position, velocity, angle velocity)</li>
</ul>
</li>
<li>maybe we shall also do this to avoid multimodal and CV part.</li>
</ul>
<h2 id="minor-comments">Minor comments<a aria-hidden="true" class="anchor" hidden="" href="#minor-comments">#</a></h2>
<p><img alt="image-20231027171151010" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231027171151010.png"/></p>
<p><img alt="image-20231027171203317" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231027171203317.png"/></p>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<ul>
<li>
<p>having LLMs to generate reward function directly to train agents is something like replacing human experts with LLMs to design reward functions for learning agents.</p>
</li>
<li>
<p>The pipeline of this work is as follows:</p>
<ul>
<li>the human users gave the task descriptions</li>
<li>-&gt; the LLM convert the descriptions into a reward function</li>
<li>-&gt; the reward function helps to train the agent that can accomplish the task</li>
<li>-&gt; the human users can give further requirements in the next iteration</li>
<li>-&gt; the LLM will further adjust the reward function to fit the users’ need.</li>
</ul>
</li>
</ul>
<h3 id="hybrid-model">Hybrid model<a aria-hidden="true" class="anchor" hidden="" href="#hybrid-model">#</a></h3>
<p>Nir suggested that we can have a hybrid model such that conditions and effects expressed partially in predicate logic, and partially specified through imperative programming languages</p>
<ul>
<li>
<p>I think the it really depends on the type of the tasks</p>
<ul>
<li>if the task is about planning or scheduling, e.g., Sudoku, then imperative programming has nothing to do with this</li>
<li>if the task is about low-level controls that have no explicit discrete procedures, then defining a reward function (<em>this work</em>) is suitable</li>
<li>if the task description explicitly contains the steps, then converting it to imperative programming language is suitable.</li>
</ul>
</li>
<li>
<p>so it really depends on what is the task description is</p>
<ul>
<li>e.g., cooking task provided with steps info<img alt="image-20231030150628046" loading="lazy" src="/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030150628046.png"/> -&gt; imperative programming</li>
<li>e.g., “your task is to stack block A on top of B” -&gt; predicate logic as conditions and effects</li>
</ul>
</li>
<li>
<p>so the hybrid model is somehow like a big model containing multiple specialised <em>models</em> that handles various types of tasks.</p>
</li>
</ul>
<h3 id="imperative-programming-version-of-the-action-definition">Imperative programming version of the action definition<a aria-hidden="true" class="anchor" hidden="" href="#imperative-programming-version-of-the-action-definition">#</a></h3>
<p>both reward function generation for actions or direct imperative function for actions can be used as auxiliary information to tune the PDDL action definition</p>
<p>Reward function example from GPT4</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reward_function</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Assuming `is_on_ladder` and `is_moving_down` are functions that </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># determine whether the agent is on a ladder and moving down, respectively.</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_on_ladder</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_moving_down</span><span class="p">(</span><span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Give a positive reward for moving down a ladder</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Optionally, penalize the agent for not moving down a ladder while on it</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">is_on_ladder</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_moving_down</span><span class="p">(</span><span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Optionally, penalize or reward other behaviors</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">reward</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Imperative function example</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Assume the agent starts at the top-left corner</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ladder_position</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># Assume there's a ladder at position (5, 5)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">get_agent_position</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">move_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">direction</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">"down"</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ladder_below</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">"left"</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">"right"</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">direction</span> <span class="o">==</span> <span class="s2">"up"</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">-=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">is_ladder_below</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ladder_position</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">is_on_ladder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent_position</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">ladder_position</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">climb_down_ladder</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="ow">not</span> <span class="n">env</span><span class="o">.</span><span class="n">is_on_ladder</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Move towards the ladder</span>
</span></span><span class="line"><span class="cl">        <span class="n">agent_position</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_agent_position</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">ladder_position</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">ladder_position</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">agent_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">ladder_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="o">.</span><span class="n">move_agent</span><span class="p">(</span><span class="s2">"right"</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">agent_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">ladder_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="o">.</span><span class="n">move_agent</span><span class="p">(</span><span class="s2">"left"</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">agent_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">ladder_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="n">env</span><span class="o">.</span><span class="n">move_agent</span><span class="p">(</span><span class="s2">"down"</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Now on the ladder, climb down</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>  <span class="c1"># Assume ladder is 5 cells tall</span>
</span></span><span class="line"><span class="cl">        <span class="n">env</span><span class="o">.</span><span class="n">move_agent</span><span class="p">(</span><span class="s2">"down"</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">check_climb_down_complete</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">agent_position</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_agent_position</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">ladder_position</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">ladder_position</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Check if the agent's y-coordinate is the same or below the ladder's bottom y-coordinate</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">agent_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">ladder_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">env</span><span class="o">.</span><span class="n">is_on_ladder</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">True</span>  <span class="c1"># Climbing down the ladder is complete</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># Climbing down the ladder is not complete</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl"><span class="c1"># Usage:</span>
</span></span><span class="line"><span class="cl"><span class="n">env</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">climb_down_ladder</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>both reward and imperative action function contains state checking (i.e., <code>is_on_ladder</code> and <code>is_moving_down</code> etc.  )</li>
<li>the imperative programming version of the “climb_down_ladder” action contains a while loop that controls the agent to move towards the ladder before climbing down. This is different from the PDDL version action definition, where <code>is_on_ladder</code> is the precondition of the action.</li>
</ul>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/reward-design/">Reward Design</a></li>
<li><a href="https://sino-huang.github.io/tags/large-language-model/">Large Language Model</a></li>
<li><a href="https://sino-huang.github.io/tags/llm-for-reward-function/">Llm for Reward Function</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/pascal-bercher-detecting-ai-planning-modelling-mistakes-potential-errors-and-benchmark-domains-2023/">
<span class="title">« Prev</span>
<br/>
<span>Pascal Bercher Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains 2023</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/mark-chen-evaluating-large-language-models-trained-on-code-2021/">
<span class="title">Next »</span>
<br/>
<span>Mark Chen Evaluating Large Language Models Trained on Code 2021</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on x" href="https://x.com/intent/tweet/?text=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f&amp;hashtags=rewarddesign%2clargelanguagemodel%2cllmforrewardfunction" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f&amp;title=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023&amp;summary=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f&amp;title=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on whatsapp" href="https://api.whatsapp.com/send?text=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on telegram" href="https://telegram.me/share/url?text=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Yecheng%20Jason%20Ma%20Eureka%20Human%20Level%20Reward%20Design%20via%20Coding%20Large%20Language%20Models%202023&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2fyecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
