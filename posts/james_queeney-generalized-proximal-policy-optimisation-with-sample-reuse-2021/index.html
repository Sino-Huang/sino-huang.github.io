<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021 | Sukai Huang</title>
<meta content="policy gradient, reinforcement learning" name="keywords"/>
<meta content="[TOC]

Title: Generalized Proximal Policy Optimisation With Sample Reuse 2021
Author: James Queeney et. al.
Publish Year: 29 Oct 2021
Review Date: Wed, Dec 28, 2022

Summary of paper
Motivation


it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse.

Contribution

in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms.
We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO
this motivate an off-policy version of the popular algorithm that we call GePPO.
we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency

Some key terms
sample complexity" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021" property="og:title"/>
<meta content="[TOC]
Title: Generalized Proximal Policy Optimisation With Sample Reuse 2021 Author: James Queeney et. al. Publish Year: 29 Oct 2021 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. Contribution in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO this motivate an off-policy version of the popular algorithm that we call GePPO. we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency Some key terms sample complexity" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png" name="twitter:image"/>
<meta content="James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021",
      "item": "https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021
    <a aria-label="RSS" href="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Generalized Proximal Policy Optimisation With Sample Reuse 2021</li>
<li>Author: James Queeney et. al.</li>
<li>Publish Year: 29 Oct 2021</li>
<li>Review Date: Wed, Dec 28, 2022</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<p><img alt="image-20221228140752324" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png"/></p>
<ul>
<li>it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms.</li>
<li>We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO</li>
<li>this motivate an off-policy version of the popular algorithm that we call GePPO.</li>
<li>we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>sample complexity</strong></p>
<ul>
<li>Represents the number of training-samples that it needs in order to successfully learn a target function.</li>
</ul>
<p><strong>PPO theoretical guarantee</strong></p>
<ul>
<li>the method is motivated by a lower bound on the expected performance loss at every update, which can be approximated using sample generated by the current policy.</li>
<li>the <strong>theoretically supported stability</strong> of these methods is very attractive but need for on-policy data and the high-variance nature of reinforcement learning often require significant data to be collected between every update, resulting in <strong>high sample complexity</strong> and slow learning</li>
</ul>
<p><strong>reuse samples</strong></p>
<ul>
<li>allow data to be reused to calculate multiple policy updates</li>
<li>but also causes the distribution of data to shift away from the distribution generated by the current policy.</li>
<li>this distribution shift invalidates the standard performance guarantees used in on-policy methods, and can lead to instability in the training process.</li>
<li>popular off-policy algorithms often require various implementation tricks and extensive hyperparameter tuning to control the instability caused by off-policy data.</li>
</ul>
<p><strong>On-policy policy improvement methods</strong></p>
<ul>
<li>the goal of monotonic policy improvement was first introduced by   Kakade and Langford [14] in Conservative Policy Iteration, which maximise a lower bound on policy improvement that can be constructed using samples from the current policy.</li>
<li>This theory of policy improvement has served as a fundamental building block in the design of on-policy deep RL methods</li>
<li><u><strong>to best of our knowledge, we are the first to directly relate the clipping mechanism in PPO to the total variation distance between policies</strong></u></li>
</ul>
<p><strong>sample efficiency with off-policy data</strong></p>
<ul>
<li>a common approach to improving the sample efficiency of model-free reinforcement learning is to reuse samples collected under prior policies. Popular off-policy policy gradient approaches are DDPG, SAC.</li>
<li>these methods are not motivated by policy improvement guarantees, and do not directly control the bias introduced by off-policy data.</li>
</ul>
<p><strong>combining on-policy and off-policy</strong></p>
<ul>
<li>this is the goal of balancing the variance of on-policy methods and bias of off-policy methods</li>
<li>Gu et al. [10] demonstrated that the bias introduced by off-policy data is related to the KL divergence between the current policy and the behavior policy used to generate the data.</li>
<li>Some other methods are related to the penalty term that appears in our genralized policy improvement lower bound, which can be bounded by a penalty that depends on KL divergence</li>
</ul>
<p><strong>state visitation distribution</strong></p>
<p><img alt="image-20221228163740769" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228163740769.png"/></p>
<p><strong>Policy improvement lower bound</strong></p>
<p><img alt="image-20221228164207986" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228164207986.png"/></p>
<ul>
<li>we refer to the first term of the lower bound in lemma 1 as the surrogate objective, and the second term as the penalty term.</li>
<li>we can guarantee policy improvement at every step of the learning process by choosing the next policy $\pi_{k+1}$ to maximise this lower bound.</li>
<li>because the expectations in Lemma 1 depend on the current policy $\pi_k$, we can approximate this lower bound using samples generated by the current policy.</li>
</ul>
<p><strong>PPO why this is called proximal</strong></p>
<ul>
<li>it theoretically motivated by the policy improvement lower bound in Lemma 1. Rather than directly maximizing this lower bound, PPO considers the goal of maximizing the surrogate objective while constraint the next policy to be close to the current policy using CLIP</li>
<li>often the number of samples must be large in order for the empirical objective to be an accurate estimator for the true objective.
<ul>
<li>because these samples must be collected under the current policy between every policy update, PPO can be very sample intensive.</li>
</ul>
</li>
</ul>
<p><strong>Generalised policy improvement lower bound</strong></p>
<ul>
<li>in order to retain the stability benefits of PPO while reusing samples from prior policies, we must incorporate these off-policy samples in a principled way.</li>
<li><img alt="image-20221228200322392" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228200322392.png"/></li>
<li>proof: we generalise Lemma 1 to depend on expectations with respect to any reference policy, and we apply this result M times for the past policies. Then, the convex combination determined by the distribution determined by $\nu$ of the resulting M policy improvement lower bounds is also a lower bound.</li>
<li>this lower bound provides theoretical support for extending PPO to include off-policy samples</li>
<li>The penalty term is Theorem 1 suggests that we should control the expected total <strong>variance distance between the future policy $\pi$ and the last $M$ policies.</strong></li>
</ul>
<p><img alt="image-20221228201528123" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228201528123.png"/></p>
<ul>
<li>this means we can effectively control the expected performance loss at every policy update by controlling the expected total variance distance between consecutive policies.
<ul>
<li>the clipping mechanism in PPO approximately accomplish this task.</li>
</ul>
</li>
</ul>
<p><strong>The expected total variation distance between the current policy $\pi_k$ and the future policy pi</strong></p>
<ul>
<li><img alt="image-20221228202117593" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228202117593.png"/></li>
</ul>
<h2 id="sample-efficiency-analysis-important"><strong>Sample efficiency analysis</strong> (IMPORTANT)<a aria-hidden="true" class="anchor" hidden="" href="#sample-efficiency-analysis-important">#</a></h2>
<ul>
<li>in order to compare GePPO to PPO, we first must determine the appropriate choice of clipping parameter that results in the <strong><u>same worst-case expected performance loss</u></strong> at every update.</li>
<li><img alt="image-20221228202950131" loading="lazy" src="/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/image-20221228202950131.png"/></li>
<li>this shows that we must make smaller policy updates compared to PPO in terms of total variation distance, which is a result of utilizing samples from prior policies.</li>
<li>Assume we require N=Bn samples for the empirical objective to be a sufficiently accurate estimate of the true objective, where n is the smallest possible batch size we can collect and B is some positive integer</li>
<li>In this setting, PPO makes one policy update per N samples collected, while GePPO leverage data from prior policies to make B updates per N samples collected as long as M &gt;= B.</li>
<li>first we see that GePPO can <strong><u>increase the change in total variance distance of policy throughout training compared to PPO</u></strong> in same training sample number.</li>
</ul>
<h2 id="potential-future-work">Potential future work<a aria-hidden="true" class="anchor" hidden="" href="#potential-future-work">#</a></h2>
<p>give you some insight about how to theoretically analyse the learning efficiency</p>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
