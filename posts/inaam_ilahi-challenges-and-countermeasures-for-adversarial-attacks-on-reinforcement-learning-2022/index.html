<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022 | Sukai Huang</title>
<meta content="reinforcement learning, perturbed rewards" name="keywords"/>
<meta content="[TOC]

Title: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning
Author: Inaam Ilahi et. al.
Publish Year: 13 Sep 2021
Review Date: Sat, Dec 24, 2022

Summary of paper
Motivation

DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications.
Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks.

Contribution

we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms
we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures
we discuss the available benchmarks and metrics for the robustness of DRL
finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions .

Some key terms
organisation of this article" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022" property="og:title"/>
<meta content="[TOC]
Title: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning Author: Inaam Ilahi et. al. Publish Year: 13 Sep 2021 Review Date: Sat, Dec 24, 2022 Summary of paper Motivation DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications. Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks. Contribution we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures we discuss the available benchmarks and metrics for the robustness of DRL finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions . Some key terms organisation of this article" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/image-assets/cover.png" name="twitter:image"/>
<meta content="Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022",
      "item": "https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022
    <a aria-label="RSS" href="/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning</li>
<li>Author: Inaam Ilahi et. al.</li>
<li>Publish Year: 13 Sep 2021</li>
<li>Review Date: Sat, Dec 24, 2022</li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications.</li>
<li>Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms</li>
<li>we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures</li>
<li>we discuss the available benchmarks and metrics for the robustness of DRL</li>
<li>finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions .</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>organisation of this article</strong></p>
<p><img alt="image-20221224175537545" loading="lazy" src="/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/image-assets/image-20221224175537545.png"/></p>
<p><strong>Value function</strong></p>
<p><img alt="image-20221224182104800" loading="lazy" src="/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/image-assets/image-20221224182104800.png"/></p>
<p><strong>Non-exhaustive taxonomy of major DRL schemes as proposed</strong></p>
<p><img alt="image-20221224182342176" loading="lazy" src="/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/image-assets/image-20221224182342176.png"/></p>
<p><strong>why use policy based RL</strong></p>
<ul>
<li>advantages
<ul>
<li>better convergence properties</li>
<li>effective in high-dimensional or continuous action spaces. When the space is large, the usage of memory and computation consumption grows rapidly, the policy based RL avoids this because the objective is to learn a set of parameters that is far less than the space count.</li>
</ul>
</li>
<li>disadvantages
<ul>
<li>policy gradient methods tend to more stably converge to a good behaviour. but indeed being on-policy, makes them very sample inefficient.</li>
<li>Evaluating a policy is typically inefficient and high variance Policy based RL has high variance, but there are techniques to reduce this variance.</li>
</ul>
</li>
</ul>
<p><strong>attacks on DRL</strong></p>
<ul>
<li>reward space
<ul>
<li>adding perturbations</li>
<li>flipping the rewards</li>
</ul>
</li>
</ul>
<p><strong>adversarial training</strong></p>
<ul>
<li>adversarial training includes retraining of the ML model using the adversarial examples along with the legitimate examples. This increases the robustness of the ML against adversarial examples as the model is now able to learn a better distribution.</li>
</ul>
<h2 id="major-comments">Major comments<a aria-hidden="true" class="anchor" hidden="" href="#major-comments">#</a></h2>
<p><strong>Citation</strong></p>
<ul>
<li>RL algorithm also have some limitations to be utilised in practice mainly due to their slow learning process and inability to learn in complex environment.
<ul>
<li>ref: Ilahi, Inaam, et al. “Challenges and countermeasures for adversarial attacks on deep reinforcement learning.” <em>IEEE Transactions on Artificial Intelligence</em> 3.2 (2021): 90-109.</li>
</ul>
</li>
<li>recently, DRL has been vulnerable to adversarial attacks, where an imperceptible perturbation is added to the input to the DRL schemes with a predefined goal of causing a malfunction in the working of DRL
<ul>
<li>ref: Ilahi, Inaam, et al. “Challenges and countermeasures for adversarial attacks on deep reinforcement learning.” <em>IEEE Transactions on Artificial Intelligence</em> 3.2 (2021): 90-109.</li>
</ul>
</li>
<li>there is limited number of research focusing on the reward perturbation influence on the RL agent performance
<ul>
<li>ref: Ilahi, Inaam, et al. “Challenges and countermeasures for adversarial attacks on deep reinforcement learning.” <em>IEEE Transactions on Artificial Intelligence</em> 3.2 (2021): 90-109.</li>
</ul>
</li>
<li>With regard to handling reward perturbation, this work utilized a neural network to estimate the actual reward of the environment and therefore detect and filter out abnormal rewards.
<ul>
<li>ref: Kumar, Aashish. <em>Enhancing performance of reinforcement learning models in the presence of noisy rewards</em>. Diss. 2019.</li>
<li>Dogru, Oguzhan, Ranjith Chiplunkar, and Biao Huang. “Reinforcement  learning with constrained uncertain reward function through particle  filtering.” <em>IEEE Transactions on Industrial Electronics</em> 69.7 (2021): 7491-7499.</li>
</ul>
</li>
</ul>
<h2 id="minor-comments">Minor comments<a aria-hidden="true" class="anchor" hidden="" href="#minor-comments">#</a></h2>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
