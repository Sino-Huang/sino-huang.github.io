<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018 | Sukai Huang</title>
<meta content="reward machine" name="keywords"/>
<meta content="[TOC]

Title:  Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning
Author: Rodrigo Toro Icarte et. al.
Publish Year: PMLR 2018
Review Date: Thu, Aug 17, 2023
url: http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf

Summary of paper

Motivation

proposing a reward machine while exposing reward function structure to the learner and supporting decomposition.

Contribution

in contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case.

Some key terms
intro" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018" property="og:title"/>
<meta content="[TOC]
Title: Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning Author: Rodrigo Toro Icarte et. al. Publish Year: PMLR 2018 Review Date: Thu, Aug 17, 2023 url: http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf Summary of paper Motivation proposing a reward machine while exposing reward function structure to the learner and supporting decomposition. Contribution in contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case. Some key terms intro" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/cover.png" name="twitter:image"/>
<meta content="Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018",
      "item": "https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/"
    }
  ]
}
</script>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1>
    Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018
    <a aria-label="RSS" href="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title:  Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning</li>
<li>Author: Rodrigo Toro Icarte et. al.</li>
<li>Publish Year: PMLR 2018</li>
<li>Review Date: Thu, Aug 17, 2023</li>
<li>url: <a href="http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf">http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf</a></li>
</ol>
<h2 id="summary-of-paper">Summary of paper<a aria-hidden="true" class="anchor" hidden="" href="#summary-of-paper">#</a></h2>
<p><img alt="image-20230817111402194" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/cover.png"/></p>
<h3 id="motivation">Motivation<a aria-hidden="true" class="anchor" hidden="" href="#motivation">#</a></h3>
<ul>
<li>proposing a reward machine while exposing reward function structure to the learner and supporting decomposition.</li>
</ul>
<h3 id="contribution">Contribution<a aria-hidden="true" class="anchor" hidden="" href="#contribution">#</a></h3>
<ul>
<li>in contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case.</li>
</ul>
<h3 id="some-key-terms">Some key terms<a aria-hidden="true" class="anchor" hidden="" href="#some-key-terms">#</a></h3>
<p><strong>intro</strong></p>
<ul>
<li>there is less reason to hide the reward function from the agent.</li>
<li>this work our method is able to reward behaviours to varying degrees in manners that cannot be expressed by previous approaches.
<ul>
<li>i.e., reward the behaviours in advanced way</li>
</ul>
</li>
</ul>
<p><strong>reward machine</strong></p>
<ul>
<li>a RM allows for composing different reward functions in flexible ways, including concatenations, loops and conditional rules.</li>
<li>after every transition, the reward machine output the reward function the agent should use at that time.</li>
<li>the author claim that the structure of RM can give agent knowledge that the problem consists of multiple stages and thus can use the information to decompose the task.</li>
</ul>
<p><strong>Q-learning for reward machines (QRM)</strong></p>
<ul>
<li>can exploit a reward machine’s internal structure to decompose the problem and thereby improving sample efficiency.</li>
<li>it uses q learning to update each sub-task policy in parallel.</li>
<li>we show that QRM is guaranteed to converge to an optimal policy in the tabular case.</li>
</ul>
<p><strong>Bellman equation</strong></p>
<p><img alt="image-20230817152857864" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817152857864.png"/></p>
<h3 id="reward-machine-for-task-specification">Reward machine for task specification<a aria-hidden="true" class="anchor" hidden="" href="#reward-machine-for-task-specification">#</a></h3>
<ul>
<li>The intuition is that the agent will be rewarded by different reward functions at different times, depending on the <em>transitions</em> made in the RM.</li>
</ul>
<p><strong>definition of reward machine</strong></p>
<p><img alt="image-20230817153309923" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817153309923.png"/></p>
<ul>
<li>definition of simple reward machine
<ul>
<li>the reward only relies on the internal state of RM rather than the transitions in the world.</li>
</ul>
</li>
<li>Limitation
<ul>
<li>the RM has a clear understanding about the when the world state transits to their targeted high-level ones.</li>
<li>a ground-truth labelling function
<ul>
<li><img alt="image-20230817161908418" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817161908418.png"/></li>
</ul>
</li>
<li><img alt="image-20230817161703797" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817161703797.png"/></li>
</ul>
</li>
</ul>
<p><strong>A posible solution for LRS</strong></p>
<p><img alt="image-20230817162208206" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817162208206.png"/></p>
<ul>
<li>we need different q functions – meaning different sub policies</li>
<li><img alt="image-20230817162556151" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817162556151.png"/></li>
<li><img alt="image-20230817162611801" loading="lazy" src="/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/image-20230817162611801.png"/></li>
</ul>
<h2 id="major-comments">Major comments<a aria-hidden="true" class="anchor" hidden="" href="#major-comments">#</a></h2>
<ul>
<li>it is still very challenging to think about how to ground this method to simulated Atari games where there is no prepared labelling function for you.</li>
</ul>
</div>
</main>
<footer class="footer">
<span>© 2024 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
