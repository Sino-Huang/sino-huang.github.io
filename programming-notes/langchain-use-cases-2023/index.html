<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Langchain Use Cases 2023 | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="[TOC]

Title: Langchain Use Cases 2023
Review Date: Sat, Aug 26, 2023
url: https://python.langchain.com/docs/get_started/quickstart

Langchain quickstart

The core building block of LangChain applications is the LLMChain. This combines three things:

LLM: The language model  is the core reasoning engine here. In order to work with LangChain, you  need to understand the different types of language models and how to  work with them.
Prompt Templates: This provides instructions to  the language model. This controls what the language model outputs, so  understanding how to construct prompts and different prompting  strategies is crucial.
Output Parsers: These translate the raw  response from the LLM to a more workable format, making it easy to use  the output downstream.



PromptTemplate

modify prompt format easily

Chains: Combine LLMs and prompts in multi-step workflows


1
2


from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)


Agents: dynamically call chains based on user input


1
2
3


from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.llms import OpenAI


we can connect Google AI with ChatGPT" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/programming-notes/langchain-use-cases-2023/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/programming-notes/langchain-use-cases-2023/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/programming-notes/langchain-use-cases-2023/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Langchain Use Cases 2023" property="og:title"/>
<meta content="[TOC]
Title: Langchain Use Cases 2023 Review Date: Sat, Aug 26, 2023 url: https://python.langchain.com/docs/get_started/quickstart Langchain quickstart The core building block of LangChain applications is the LLMChain. This combines three things: LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream. PromptTemplate modify prompt format easily Chains: Combine LLMs and prompts in multi-step workflows 1 2 from langchain.chains import LLMChain chain = LLMChain(llm=llm, prompt=prompt) Agents: dynamically call chains based on user input 1 2 3 from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.llms import OpenAI we can connect Google AI with ChatGPT" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="programming-notes" property="article:section"/>
<meta content="2023-08-26T17:36:47+10:00" property="article:published_time"/>
<meta content="2023-08-26T17:36:47+10:00" property="article:modified_time"/>
<meta content="https://sino-huang.github.io/posts/langchain-use-cases-2023/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/langchain-use-cases-2023/image-assets/cover.png" name="twitter:image"/>
<meta content="Langchain Use Cases 2023" name="twitter:title"/>
<meta content="[TOC]

Title: Langchain Use Cases 2023
Review Date: Sat, Aug 26, 2023
url: https://python.langchain.com/docs/get_started/quickstart

Langchain quickstart

The core building block of LangChain applications is the LLMChain. This combines three things:

LLM: The language model  is the core reasoning engine here. In order to work with LangChain, you  need to understand the different types of language models and how to  work with them.
Prompt Templates: This provides instructions to  the language model. This controls what the language model outputs, so  understanding how to construct prompts and different prompting  strategies is crucial.
Output Parsers: These translate the raw  response from the LLM to a more workable format, making it easy to use  the output downstream.



PromptTemplate

modify prompt format easily

Chains: Combine LLMs and prompts in multi-step workflows


1
2


from langchain.chains import LLMChain
chain = LLMChain(llm=llm, prompt=prompt)


Agents: dynamically call chains based on user input


1
2
3


from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.llms import OpenAI


we can connect Google AI with ChatGPT" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Programming-Notes",
      "item": "https://sino-huang.github.io/programming-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Langchain Use Cases 2023",
      "item": "https://sino-huang.github.io/programming-notes/langchain-use-cases-2023/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Langchain Use Cases 2023",
  "name": "Langchain Use Cases 2023",
  "description": "[TOC]\nTitle: Langchain Use Cases 2023 Review Date: Sat, Aug 26, 2023 url: https://python.langchain.com/docs/get_started/quickstart Langchain quickstart The core building block of LangChain applications is the LLMChain. This combines three things: LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream. PromptTemplate modify prompt format easily Chains: Combine LLMs and prompts in multi-step workflows 1 2 from langchain.chains import LLMChain chain = LLMChain(llm=llm, prompt=prompt) Agents: dynamically call chains based on user input 1 2 3 from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.llms import OpenAI we can connect Google AI with ChatGPT\n",
  "keywords": [
    
  ],
  "articleBody": "[TOC]\nTitle: Langchain Use Cases 2023 Review Date: Sat, Aug 26, 2023 url: https://python.langchain.com/docs/get_started/quickstart Langchain quickstart The core building block of LangChain applications is the LLMChain. This combines three things: LLM: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them. Prompt Templates: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial. Output Parsers: These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream. PromptTemplate modify prompt format easily Chains: Combine LLMs and prompts in multi-step workflows 1 2 from langchain.chains import LLMChain chain = LLMChain(llm=llm, prompt=prompt) Agents: dynamically call chains based on user input 1 2 3 from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.llms import OpenAI we can connect Google AI with ChatGPT\nwe load language model, some tools to use and finally initialise an agent with the tools the language model the type of agent we want to use memory: add state to chains and agents conversation chain, it will save all the previous conversation. Langchain Schema Chat Messages like text, but specified with a message type (System, Human and AI) System, helpful background context that tell the AI what to do Human, messages that are intended to represent the user AI - messages that show what the AI responded with Document load documents to language model Langchain model Language model text in text out model Chat model a model takes a series of messages and returns a message output the memory is explicitly shown in chat schema Text embedding model change your text into a vector FAISS can be used as a retriever to get relevant documents Prompt prompt template: generate prompts based on the combination of user input (i.e., put place holders) example selector an easy way to select from a series of examples that allows for dynamically placing in-context information into your prompt SemanticSimilarityExampleSelector\nwe need a VectorStore class that is used to store the embeddings and do similarity check FAISS is the default Output Parsers a helpful way to format the output of a model. Usually used for structured output two big concepts Format instructions - A autogenerated prompt that tells LLM how to format its response based off your desired result Parser - A method which will extract output into a desired structure (usually json) Indexes – Structuring documents to LLMs Document loaders load documents from online source Text splitter often times your document is too long for your LLM, you need to split it into chunks Text splitters help with this Memory a common one is chathistory from langchain.memory import ChatMessageHistory Chains combine different LLMs calls and output Simple sequential chain decompose the task into each step feed the output of previous LLM to the following prompt Summarisation chain map reduce or different types of chain type Agents an agent is the language model that drives decision making agents are making automatic decisions Extraction extraction is the term for describing extract useful information from natural language information and parse it into structured format 1 2 3 4 5 6 7 8 9 10 11 12 # To help construct chat message from langchain.schema import HumanMessage from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate # to parse output and get structured data back from langchain.output_parsers import StructuredOutputParser, ResponseSchema output_parser = StructuredOutputParser.from_response_schemas(response_schema) # the format instructions are LangChain makes, format_instructions = output_parser.get_format_instructions() # the format_instruction will be refered as partial_variables in PromptTemplate Instead of parsing it into JSON, we can use Kor library to edit useful structured format https://eyurtsev.github.io/kor/index.html Question Answering sources allows you to return the source document that is essential for the output. 1 qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type='stuff', vectorstore=docsearch, return_source_document=True) NL Info to PDDL configs a Question-Answering case (predefine each statement chunk in PDDL) -\u003e an Extraction case (from NL info to each specific statement) -\u003e a Structured Parser and Autofix case \u003c-loop-\u003e send to planning domain api\npossible LLM\nhttps://huggingface.co/meta-llama/Llama-2-70b-chat-hf openai chat llama 2 variant https://huggingface.co/garage-bAInd/Platypus2-70B-instruct Question Answering: https://python.langchain.com/docs/use_cases/question_answering/\nExtraction: https://python.langchain.com/docs/use_cases/extraction\n",
  "wordCount" : "700",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/langchain-use-cases-2023/image-assets/cover.png","datePublished": "2023-08-26T17:36:47+10:00",
  "dateModified": "2023-08-26T17:36:47+10:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/programming-notes/langchain-use-cases-2023/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/programming-notes/">Programming-Notes</a></div>
<h1 class="post-title entry-hint-parent">
      Langchain Use Cases 2023
    </h1>
<div class="post-meta"><span title="2023-08-26 17:36:47 +1000 AEST">August 26, 2023</span> · 4 min · 700 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/programming-notes/langchain-use-cases-2023/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/langchain-use-cases-2023/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul><ul>
<li>
<a aria-label="Langchain quickstart" href="#langchain-quickstart">Langchain quickstart</a><ul>
<li>
<a aria-label="PromptTemplate" href="#prompttemplate">PromptTemplate</a></li>
<li>
<a aria-label="Chains: Combine LLMs and prompts in multi-step workflows" href="#chains-combine-llms-and-prompts-in-multi-step-workflows">Chains: Combine LLMs and prompts in multi-step workflows</a></li>
<li>
<a aria-label="Agents: dynamically call chains based on user input" href="#agents-dynamically-call-chains-based-on-user-input">Agents: dynamically call chains based on user input</a></li>
<li>
<a aria-label="memory: add state to chains and agents" href="#memory-add-state-to-chains-and-agents">memory: add state to chains and agents</a></li></ul>
</li>
<li>
<a aria-label="Langchain Schema" href="#langchain-schema">Langchain Schema</a><ul>
<li>
<a aria-label="Chat Messages" href="#chat-messages">Chat Messages</a></li>
<li>
<a aria-label="Document" href="#document">Document</a></li></ul>
</li>
<li>
<a aria-label="Langchain model" href="#langchain-model">Langchain model</a><ul>
<li>
<a aria-label="Language model" href="#language-model">Language model</a></li>
<li>
<a aria-label="Chat model" href="#chat-model">Chat model</a></li>
<li>
<a aria-label="Text embedding model" href="#text-embedding-model">Text embedding model</a></li></ul>
</li>
<li>
<a aria-label="Prompt" href="#prompt">Prompt</a><ul>
<li>
<a aria-label="example selector" href="#example-selector">example selector</a></li></ul>
</li>
<li>
<a aria-label="Output Parsers" href="#output-parsers">Output Parsers</a></li>
<li>
<a aria-label="Indexes – Structuring documents to LLMs" href="#indexes----structuring-documents-to-llms">Indexes – Structuring documents to LLMs</a><ul>
<li>
<a aria-label="Document loaders" href="#document-loaders">Document loaders</a></li>
<li>
<a aria-label="Text splitter" href="#text-splitter">Text splitter</a></li></ul>
</li>
<li>
<a aria-label="Memory" href="#memory">Memory</a></li>
<li>
<a aria-label="Chains" href="#chains">Chains</a><ul>
<li>
<a aria-label="Simple sequential chain" href="#simple-sequential-chain">Simple sequential chain</a></li>
<li>
<a aria-label="Summarisation chain" href="#summarisation-chain">Summarisation chain</a></li></ul>
</li>
<li>
<a aria-label="Agents" href="#agents">Agents</a></li>
<li>
<a aria-label="Extraction" href="#extraction">Extraction</a></li>
<li>
<a aria-label="Question Answering" href="#question-answering">Question Answering</a></li></ul>
<li>
<a aria-label="NL Info to PDDL configs" href="#nl-info-to-pddl-configs">NL Info to PDDL configs</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>[TOC]</p>
<ol>
<li>Title: Langchain Use Cases 2023</li>
<li>Review Date: Sat, Aug 26, 2023</li>
<li>url: <a href="https://python.langchain.com/docs/get_started/quickstart">https://python.langchain.com/docs/get_started/quickstart</a></li>
</ol>
<h2 id="langchain-quickstart">Langchain quickstart<a aria-hidden="true" class="anchor" hidden="" href="#langchain-quickstart">#</a></h2>
<ul>
<li>The core building block of LangChain applications is the LLMChain. This combines three things:
<ul>
<li>LLM: The language model  is the core reasoning engine here. In order to work with LangChain, you  need to understand the different types of language models and how to  work with them.</li>
<li>Prompt Templates: This provides instructions to  the language model. This controls what the language model outputs, so  understanding how to construct prompts and different prompting  strategies is crucial.</li>
<li>Output Parsers: These translate the raw  response from the LLM to a more workable format, making it easy to use  the output downstream.</li>
</ul>
</li>
</ul>
<h3 id="prompttemplate">PromptTemplate<a aria-hidden="true" class="anchor" hidden="" href="#prompttemplate">#</a></h3>
<ul>
<li>modify prompt format easily</li>
</ul>
<h3 id="chains-combine-llms-and-prompts-in-multi-step-workflows">Chains: Combine LLMs and prompts in multi-step workflows<a aria-hidden="true" class="anchor" hidden="" href="#chains-combine-llms-and-prompts-in-multi-step-workflows">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span>
</span></span><span class="line"><span class="cl"><span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="agents-dynamically-call-chains-based-on-user-input">Agents: dynamically call chains based on user input<a aria-hidden="true" class="anchor" hidden="" href="#agents-dynamically-call-chains-based-on-user-input">#</a></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">load_tools</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.agents</span> <span class="kn">import</span> <span class="n">initialize_agent</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>we can connect Google AI with ChatGPT</p>
<ul>
<li>we load language model, some tools to use and finally initialise an agent with
<ul>
<li>the tools</li>
<li>the language model</li>
<li>the type of agent we want to use</li>
</ul>
</li>
</ul>
<h3 id="memory-add-state-to-chains-and-agents">memory: add state to chains and agents<a aria-hidden="true" class="anchor" hidden="" href="#memory-add-state-to-chains-and-agents">#</a></h3>
<ul>
<li>conversation chain, it will save all the previous conversation.</li>
</ul>
<h2 id="langchain-schema">Langchain Schema<a aria-hidden="true" class="anchor" hidden="" href="#langchain-schema">#</a></h2>
<h3 id="chat-messages">Chat Messages<a aria-hidden="true" class="anchor" hidden="" href="#chat-messages">#</a></h3>
<ul>
<li>like text, but specified with a message type (System, Human and AI)
<ul>
<li>System, helpful background context that tell the AI what to do</li>
<li>Human, messages that are intended to represent the user</li>
<li>AI - messages that show what the AI responded with</li>
</ul>
</li>
</ul>
<h3 id="document">Document<a aria-hidden="true" class="anchor" hidden="" href="#document">#</a></h3>
<ul>
<li>load documents to language model</li>
</ul>
<h2 id="langchain-model">Langchain model<a aria-hidden="true" class="anchor" hidden="" href="#langchain-model">#</a></h2>
<h3 id="language-model">Language model<a aria-hidden="true" class="anchor" hidden="" href="#language-model">#</a></h3>
<ul>
<li>text in text out model</li>
</ul>
<h3 id="chat-model">Chat model<a aria-hidden="true" class="anchor" hidden="" href="#chat-model">#</a></h3>
<ul>
<li>a model takes a series of messages and returns a message output</li>
<li>the memory is explicitly shown in chat schema</li>
</ul>
<h3 id="text-embedding-model">Text embedding model<a aria-hidden="true" class="anchor" hidden="" href="#text-embedding-model">#</a></h3>
<ul>
<li>change your text into a vector</li>
<li>FAISS can be used as a <em>retriever</em> to get relevant documents</li>
</ul>
<h2 id="prompt">Prompt<a aria-hidden="true" class="anchor" hidden="" href="#prompt">#</a></h2>
<ul>
<li><u><em>prompt template</em></u>: generate prompts based on the combination of user input (i.e., put place holders)</li>
</ul>
<h3 id="example-selector">example selector<a aria-hidden="true" class="anchor" hidden="" href="#example-selector">#</a></h3>
<ul>
<li>an easy way to select from a series of examples that allows for dynamically placing in-context information into your prompt</li>
</ul>
<p><code>SemanticSimilarityExampleSelector</code></p>
<ul>
<li>we need a VectorStore class that is used to store the embeddings and do similarity check
<ul>
<li>FAISS is the default</li>
</ul>
</li>
</ul>
<p><img alt="image-20230826182346469" loading="lazy" src="/programming-notes/langchain-use-cases-2023/image-assets/image-20230826182346469.png"/></p>
<h2 id="output-parsers">Output Parsers<a aria-hidden="true" class="anchor" hidden="" href="#output-parsers">#</a></h2>
<ul>
<li>a helpful way to format the output of a model. Usually used for structured output</li>
<li>two big concepts
<ul>
<li>Format instructions - A <em>autogenerated</em> prompt that tells LLM how to format its response based off your desired result</li>
<li>Parser - A method which will extract output into a desired structure (usually json)</li>
</ul>
</li>
</ul>
<h2 id="indexes----structuring-documents-to-llms">Indexes – Structuring documents to LLMs<a aria-hidden="true" class="anchor" hidden="" href="#indexes----structuring-documents-to-llms">#</a></h2>
<h3 id="document-loaders">Document loaders<a aria-hidden="true" class="anchor" hidden="" href="#document-loaders">#</a></h3>
<ul>
<li>load documents from online source</li>
</ul>
<h3 id="text-splitter">Text splitter<a aria-hidden="true" class="anchor" hidden="" href="#text-splitter">#</a></h3>
<ul>
<li>often times your document is too long for your LLM, you need to split it into chunks</li>
<li>Text splitters help with this</li>
</ul>
<h2 id="memory">Memory<a aria-hidden="true" class="anchor" hidden="" href="#memory">#</a></h2>
<ul>
<li>a common one is chathistory</li>
<li><code>from langchain.memory import ChatMessageHistory</code></li>
</ul>
<p><img alt="image-20230827112610400" loading="lazy" src="/programming-notes/langchain-use-cases-2023/image-assets/image-20230827112610400.png"/></p>
<h2 id="chains">Chains<a aria-hidden="true" class="anchor" hidden="" href="#chains">#</a></h2>
<ul>
<li>combine different LLMs calls and output</li>
</ul>
<h3 id="simple-sequential-chain">Simple sequential chain<a aria-hidden="true" class="anchor" hidden="" href="#simple-sequential-chain">#</a></h3>
<ul>
<li>decompose the task into each step</li>
<li>feed the output of previous LLM to the following prompt</li>
</ul>
<h3 id="summarisation-chain">Summarisation chain<a aria-hidden="true" class="anchor" hidden="" href="#summarisation-chain">#</a></h3>
<ul>
<li>map reduce or different types of chain type</li>
</ul>
<h2 id="agents">Agents<a aria-hidden="true" class="anchor" hidden="" href="#agents">#</a></h2>
<p><img alt="image-20230827113142482" loading="lazy" src="/programming-notes/langchain-use-cases-2023/image-assets/image-20230827113142482.png"/></p>
<ul>
<li>an agent is the language model that drives decision making</li>
<li>agents are making automatic decisions</li>
</ul>
<h2 id="extraction">Extraction<a aria-hidden="true" class="anchor" hidden="" href="#extraction">#</a></h2>
<ul>
<li>extraction is the term for describing extract useful information from natural language information and parse it into structured format</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># To help construct chat message </span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.schema</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">HumanMessagePromptTemplate</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># to parse output and get structured data back</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">langchain.output_parsers</span> <span class="kn">import</span> <span class="n">StructuredOutputParser</span><span class="p">,</span> <span class="n">ResponseSchema</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">output_parser</span> <span class="o">=</span> <span class="n">StructuredOutputParser</span><span class="o">.</span><span class="n">from_response_schemas</span><span class="p">(</span><span class="n">response_schema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the format instructions are LangChain makes, </span>
</span></span><span class="line"><span class="cl"><span class="n">format_instructions</span> <span class="o">=</span> <span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># the format_instruction will be refered as partial_variables in PromptTemplate</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Instead of parsing it into JSON, we can use Kor library to edit useful structured format</li>
<li><a href="https://eyurtsev.github.io/kor/index.html">https://eyurtsev.github.io/kor/index.html</a></li>
</ul>
<h2 id="question-answering">Question Answering<a aria-hidden="true" class="anchor" hidden="" href="#question-answering">#</a></h2>
<ul>
<li>sources
<ul>
<li>allows you to return the source document that is essential for the output.</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma" tabindex="0"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma" tabindex="0"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">qa</span> <span class="o">=</span> <span class="n">VectorDBQA</span><span class="o">.</span><span class="n">from_chain_type</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">OpenAI</span><span class="p">(),</span> <span class="n">chain_type</span><span class="o">=</span><span class="s1">'stuff'</span><span class="p">,</span> <span class="n">vectorstore</span><span class="o">=</span><span class="n">docsearch</span><span class="p">,</span> <span class="n">return_source_document</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="nl-info-to-pddl-configs">NL Info to PDDL configs<a aria-hidden="true" class="anchor" hidden="" href="#nl-info-to-pddl-configs">#</a></h1>
<ul>
<li>
<p>a Question-Answering case (predefine each statement chunk in PDDL) -&gt; an Extraction case (from NL info to each specific statement) -&gt; a Structured Parser and Autofix case &lt;-loop-&gt; send to planning domain api</p>
</li>
<li>
<p>possible LLM</p>
<ul>
<li><a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">https://huggingface.co/meta-llama/Llama-2-70b-chat-hf</a></li>
<li>openai chat</li>
<li>llama 2 variant <a href="https://huggingface.co/garage-bAInd/Platypus2-70B-instruct">https://huggingface.co/garage-bAInd/Platypus2-70B-instruct</a></li>
</ul>
</li>
<li>
<p>Question Answering: <a href="https://python.langchain.com/docs/use_cases/question_answering/">https://python.langchain.com/docs/use_cases/question_answering/</a></p>
</li>
<li>
<p>Extraction: <a href="https://python.langchain.com/docs/use_cases/extraction">https://python.langchain.com/docs/use_cases/extraction</a></p>
</li>
</ul>
</div>
<footer class="post-footer">
<ul class="post-tags">
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/">
<span class="title">« Prev</span>
<br/>
<span>Peng_gao Llama Adapter V2 2023</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/rodrigo-reward-machines-exploiting-reward-function-structure-in-rl-2022/">
<span class="title">Next »</span>
<br/>
<span>Rodrigo Reward Machines Exploiting Reward Function Structure in Rl 2022</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Langchain Use Cases 2023 on x" href="https://x.com/intent/tweet/?text=Langchain%20Use%20Cases%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f&amp;hashtags=" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f&amp;title=Langchain%20Use%20Cases%202023&amp;summary=Langchain%20Use%20Cases%202023&amp;source=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f&amp;title=Langchain%20Use%20Cases%202023" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on whatsapp" href="https://api.whatsapp.com/send?text=Langchain%20Use%20Cases%202023%20-%20https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on telegram" href="https://telegram.me/share/url?text=Langchain%20Use%20Cases%202023&amp;url=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Langchain Use Cases 2023 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Langchain%20Use%20Cases%202023&amp;u=https%3a%2f%2fsino-huang.github.io%2fprogramming-notes%2flangchain-use-cases-2023%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
