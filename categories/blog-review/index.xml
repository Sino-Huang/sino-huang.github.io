<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog Review on Sukai Huang</title>
    <link>https://sino-huang.github.io/categories/blog-review/</link>
    <description>Recent content in Blog Review on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 26 Dec 2022 19:50:35 +1100</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/categories/blog-review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Proximal Policy Optimisation Explained Blog</title>
      <link>https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/</link>
      <pubDate>Mon, 26 Dec 2022 19:50:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Proximal Policy Optimisation Explained Blog&lt;/li&gt;
&lt;li&gt;Author: Xiao-Yang Liu; DI engine&lt;/li&gt;
&lt;li&gt;Publish Year: May 4, 2021&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Dec 26, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Highly recommend reading this blog
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&#34;&gt;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/487754664&#34;&gt;https://zhuanlan.zhihu.com/p/487754664&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Difference between on-policy and off-policy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221226195443427&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195443427.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For on-policy algorithms, they update the policy network based on the  transitions generated by the current policy network. The &lt;strong&gt;critic network&lt;/strong&gt; would make a more accurate value-prediction for the current policy  network in common environments.&lt;/li&gt;
&lt;li&gt;For off-policy algorithms, they allow to update the current policy  network using the transitions from old policies. Thus, the old  transitions could be &lt;strong&gt;reutilized&lt;/strong&gt;, as shown in Fig. 1 the points are  scattered on trajectories that are generated by different policies,  which &lt;strong&gt;improves the sample efficiency and reduces the total training steps&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;question-is-there-a-way-to-improve-the-sample-efficiency-of-on-policy-algorithms-without-losing-their-benefit&#34;&gt;Question: is there a way to improve the sample efficiency of on-policy algorithms without losing their benefit.&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PPO solves the problem of sample efficiency by utilizing surrogate  objectives to avoid the new policy changing too far from the old policy. The surrogate objective is the key feature of PPO since it both &lt;strong&gt;1. regularizes&lt;/strong&gt; the policy update and enables the &lt;strong&gt;2. reuse&lt;/strong&gt; of training data.&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20221226195751351&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195751351.png&#34;&gt;&lt;/li&gt;
&lt;li&gt;
&lt;img src=&#34;image-assets/image-20221226200007957.png&#34; alt=&#34;image-20221226200007957&#34; style=&#34;width:50%;&#34; /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20221226195936296&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226195936296.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221226200313414&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/proximal-policy-optimisation-explained-blog/image-assets/image-20221226200313414.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alex_petrekno Sample Factory Asynchronous Rl at Very High Fps 2020</title>
      <link>https://sino-huang.github.io/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/</link>
      <pubDate>Sun, 25 Sep 2022 16:34:09 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/alex_petrekno-sample-factory-asynchronous-rl-at-very-high-fps-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Sample Factory: Asynchronous Rl at Very High FPS&lt;/li&gt;
&lt;li&gt;Author: Alex Petrenko&lt;/li&gt;
&lt;li&gt;Publish Year: Oct, 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Sep 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Identifying performance bottlenecks&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;RL involves three workloads:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;environment simulation&lt;/li&gt;
&lt;li&gt;inference&lt;/li&gt;
&lt;li&gt;backpropagation&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;overall performance depends on the &lt;strong&gt;lowest&lt;/strong&gt; workload&lt;/li&gt;
&lt;li&gt;In existing methods (A2C/PPO/IMPALA) the computational workloads are dependent -&amp;gt; under-utilisation of the system resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Existing high-throughput methods focus on distributed training, therefore introducing a lot of overhead such as networking serialisation, etc.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sergios_karagiannakos Vision Language Models Towards Multimodal Dl 2022</title>
      <link>https://sino-huang.github.io/posts/sergios_karagiannakos-vision-language-models-towards-multimodal-dl-2022/</link>
      <pubDate>Tue, 09 Aug 2022 07:37:30 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/sergios_karagiannakos-vision-language-models-towards-multimodal-dl-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Vision Language Models Towards Multimodal Deep Learning&lt;/li&gt;
&lt;li&gt;Author: Sergios Karagiannakos&lt;/li&gt;
&lt;li&gt;Publish Year: 03 Mar 2022&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Aug 9, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://theaisummer.com/vision-language-models/&#34;&gt;https://theaisummer.com/vision-language-models/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
