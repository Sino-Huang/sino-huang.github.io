<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Paper Review | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/categories/paper-review/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/categories/paper-review/index.xml" rel="alternate" type="application/rss+xml"/>
<link href="https://sino-huang.github.io/categories/paper-review/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/categories/paper-review/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Paper Review" property="og:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="website" property="og:type"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" name="twitter:image"/>
<meta content="Paper Review" name="twitter:title"/>
<meta content="Sukai's academic blog - storing weekly reports and research paper reviews" name="twitter:description"/>
</head>
<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<header class="page-header"><div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/categories/">Categories</a></div>
<h1>
    Paper Review
    <a aria-label="RSS" href="/categories/paper-review/index.xml" title="RSS">
<svg fill="none" height="23" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
<path d="M4 11a9 9 0 0 1 9 9"></path>
<path d="M4 4a16 16 0 0 1 16 16"></path>
<circle cx="5" cy="19" r="1"></circle>
</svg>
</a>
</h1>
</header>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Siyuan_wang Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning Author: Siyuan Wang et. al. Publish Year: 21 Jan 2023 Review Date: Wed, Feb 8, 2023 url: https://arxiv.org/pdf/2301.08913.pdf Summary of paper Motivation language models still suffer from a heterogeneous information alignment problem and a noisy knowledge injection problem. for complex reasoning, the context contains rich knowledge that typically exists in complex and sparse form. Contribution we propose to unify structure reasoning and language model pre-training identifies four types of elementary knowledge structures from contexts to construct structured queries utilise box embedding method to conduct explicit structure reasoning along query during language modeling Some key terms What is the problem
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-08 22:17:31 +1100 AEDT">February 8, 2023</span> · 2 min · 281 words · Sukai Huang</footer>
<a aria-label="post link to Siyuan_wang Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning 2023" class="entry-link" href="https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Ekin_akyurek Towards Tracing Factual Knowledge in Language Models Back to the Training Data 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Towards Tracing Factual Knowledge in Language Models Back to the Training Data Author: Ekin Akyurek et. al. Publish Year: EMNLP 2022 Review Date: Wed, Feb 8, 2023 url: https://aclanthology.org/2022.findings-emnlp.180.pdf Summary of paper Motivation LMs have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true. Contribution we propose the problem of fact tracing identifying which training examples taught an LM to generate a particular factual assertion. prior work on training data distribution (TDA) may offer effective tools for identifying such examples, known as “proponent”. We present the first quantitative benchmark to evaluate this we compare two popular families of TDA methods gradient based embedding based Some key terms Training data distribution method (TDA)
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-08 22:16:28 +1100 AEDT">February 8, 2023</span> · 2 min · 363 words · Sukai Huang</footer>
<a aria-label="post link to Ekin_akyurek Towards Tracing Factual Knowledge in Language Models Back to the Training Data 2022" class="entry-link" href="https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Danijar_hafner Mastering Diverse Domains Through World Models 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Mastering Diverse Domains Through World Models Author: Danijar Hafner et. al. Publish Year: 10 Jan 2023 Review Date: Tue, Feb 7, 2023 url: https://www.youtube.com/watch?v=vfpZu0R1s1Y Summary of paper Motivation general intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but held back by the resources and knowledge required tune them for new task. Contribution we present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. we observe favourable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Some key terms World Model learning
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-07 18:18:37 +1100 AEDT">February 7, 2023</span> · 2 min · 291 words · Sukai Huang</footer>
<a aria-label="post link to Danijar_hafner Mastering Diverse Domains Through World Models 2023" class="entry-link" href="https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yuanhan_zhang What Makes Good Examples for Visual in Context Learning 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: What Makes Good Examples for Visual in Context Learning Author: Yuan Zhang et. al. Publish Year: 1 Feb 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.13670.pdf Summary of paper Motivation in this paper, the main focus is on an emergent ability in large vision models, known. as in-context learning this concept has been well-known in natural language processing but has only been studied very recently for large vision models. Contribution we for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples. exposing a critical issue that different in-context examples could lead to drastically different results. Our methods obtain significant improvements over random selection under various problem settings, showing the potential of using prompt retrieval in vision applications with a Model-as-a-Service (MaaS) business structure. we show that a good in-context example should be semantically similar to the query and closer in context. A model that can better balance spatial and se- mantic closedness in feature space would be more ideal for visual in-context learning. yeah, it is because the model is not that smart in a way that it can directly tell the semantic regardless of what the spatial structure looks like Some key terms existing issue of using LLM
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-06 22:38:35 +1100 AEDT">February 6, 2023</span> · 3 min · 427 words · Sukai Huang</footer>
<a aria-label="post link to Yuanhan_zhang What Makes Good Examples for Visual in Context Learning 2023" class="entry-link" href="https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Jing_yu_koh Grounding Language Models to Images for Multimodal Generation 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Grounding Language Models to Images for Multimodal Generation Author: Jing Yu Koh et. al. Publish Year: 31 Jan 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.13823.pdf Summary of paper Motivation we propose an efficient method to ground pre-trained text-only language models to the visual domain How we keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved Contribution our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pre-trained language models in visually grounded settings. Related work LLMs for vision-and-language
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-06 22:37:53 +1100 AEDT">February 6, 2023</span> · 2 min · 239 words · Sukai Huang</footer>
<a aria-label="post link to Jing_yu_koh Grounding Language Models to Images for Multimodal Generation 2023" class="entry-link" href="https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Zhenfang_chen See Think Confirm Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning Author: Zhenfang Chen et. al. Publish Year: 12 Jan 2023 Review Date: Mon, Feb 6, 2023 url: https://arxiv.org/pdf/2301.05226.pdf Summary of paper Motivation Solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect external world knowledge, and perform step-by-step reasoning to answer the questions correctly. Contribution We propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge based visual reasoning. IPVR contains three stages, see, think, and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rational to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. Some key terms human process to handle knowledge-based visual reasoning
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-06 22:36:41 +1100 AEDT">February 6, 2023</span> · 2 min · 405 words · Sukai Huang</footer>
<a aria-label="post link to Zhenfang_chen See Think Confirm Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning 2023" class="entry-link" href="https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xiaotian_liu a Planning Based Neural Symbolic Approach for Embodied Instruction Following 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: A Planning Based Neural Symbolic Approach for Embodied Instruction Following Author: Xiaotian Liu et. al. Publish Year: 2022 Review Date: Thu, Feb 2, 2023 url: https://embodied-ai.org/papers/2022/15.pdf Summary of paper Motivation end-to-end deep learning methods struggle at these tasks due to long-horizon and sparse rewards. Contribution Our main innovation relies on combining DL models for perception and NLP with a new egocentric planner based on successive planning problems formulated using the PDDL syntax, both for exploration and task accomplishment. our planning framework can naturally recover from action failures at any stage of the planned trajectory. Some key terms Embodied Instruction Following
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-02 13:28:19 +1100 AEDT">February 2, 2023</span> · 2 min · 226 words · Sukai Huang</footer>
<a aria-label="post link to Xiaotian_liu a Planning Based Neural Symbolic Approach for Embodied Instruction Following 2022" class="entry-link" href="https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">So_yeon_min Film Following Instructions in Language With Modular Methods 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: FILM: Following Instructions in Language With Modular Methods Author: So Yeon Min et. al. Publish Year: 16 Mar 2022 Review Date: Wed, Feb 1, 2023 url: https://arxiv.org/pdf/2110.07342.pdf Summary of paper Motivation current approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. in contrast, we propose a modular method with structured representation that build a semantic map of scene and perform exploration with a semantic search policy, to achieve natural language goal. Contribution FILM consists of several modular components that each processes language instructions into structured forms (language processing) converts egocentric visual input into a semantic metric map (Semantic Mapping) predicts a search goal location (Semantic Search Policy) ? subgoal will be plotted as a dot on the semantic top-down map outputs subsequent navigation/interaction actions (Deterministic Policy) Some key terms embodied instruction following
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-01 18:32:24 +1100 AEDT">February 1, 2023</span> · 3 min · 430 words · Sukai Huang</footer>
<a aria-label="post link to So_yeon_min Film Following Instructions in Language With Modular Methods 2022" class="entry-link" href="https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yuki_inoue Prompter Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following Author: Yuki Inoue et. al. Publish Year: 7 Nov 2022 Review Date: Wed, Feb 1, 2023 url: https://arxiv.org/pdf/2211.03267.pdf Summary of paper Motivation we propose FILM++ which extends the existing work FILM with modifications that do not require extra data. furthermore, we propose Prompter, which replace FILM++’s semantic search module with language model prompting. no training is needed for our prompting based implementation while achieving better or least comparable performance. Contribution FILM++ to fill the role of the data efficient baseline. we propose Prompter, which replaces the semantic search module of FILM++ with language prompting, making it even more data efficient. Some key terms Difficulty in converting language into robot controls
...</p>
</div>
<footer class="entry-footer"><span title="2023-02-01 17:22:35 +1100 AEDT">February 1, 2023</span> · 3 min · 526 words · Sukai Huang</footer>
<a aria-label="post link to Yuki_inoue Prompter Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following 2022" class="entry-link" href="https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Kyle_mahowald Dissociating Language and Thought in Large Language Models a Cognitive Perspective 2023
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Dissociating Language and Thought in Large Language Models a Cognitive Perspective Author: Kyle Mahowald et. al. Publish Year: 16 Jan 2023 Review Date: Tue, Jan 31, 2023 url: https://arxiv.org/pdf/2301.06627.pdf Summary of paper Motivation the author tried to challenge the “good at language $\implies$ good at thought” fallacy. the second fallacy is “bad at thought $\implies$ bad at language” Contribution the author argued that LLMs have promise as scientific models of one piece of the human cognitive toolbox – formal language processing – but fall short of modelling human thought. in section 4, we consider several domains required for functional linguistic competence – formal reasoning, world knowledge, situation modelling and social cognitive abilities Some key terms deep learning models in linguistics
...</p>
</div>
<footer class="entry-footer"><span title="2023-01-31 18:47:45 +1100 AEDT">January 31, 2023</span> · 4 min · 776 words · Sukai Huang</footer>
<a aria-label="post link to Kyle_mahowald Dissociating Language and Thought in Large Language Models a Cognitive Perspective 2023" class="entry-link" href="https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Michael_janner Planning With Diffusion for Flexible Behaviour Synthesis 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Planning With Diffusion for Flexible Behaviour Synthesis Author: Michael Janner et. al. Publish Year: 21 Dec 2022 Review Date: Mon, Jan 30, 2023 Summary of paper Motivation use the diffusion model to learn the dynamics tight coupling of the modelling and planning our goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical. Some key terms ideal model-based RL
...</p>
</div>
<footer class="entry-footer"><span title="2023-01-30 13:43:20 +1100 AEDT">January 30, 2023</span> · 2 min · 317 words · Sukai Huang</footer>
<a aria-label="post link to Michael_janner Planning With Diffusion for Flexible Behaviour Synthesis 2022" class="entry-link" href="https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022 Author: Publish Year: Review Date: Fri, Jan 20, 2023 Summary of paper Motivation reasoning about actions &amp; changes has been widely studies in the knowledge representation community, it has recently piqued the interest of NLP and computer vision researchers. Contribution Some key terms Six most frequent types of commonsense knowledge
tasks that involve language-based reasoning about actions
...</p>
</div>
<footer class="entry-footer"><span title="2023-01-20 13:59:00 +1100 AEDT">January 20, 2023</span> · 3 min · 524 words · Sukai Huang</footer>
<a aria-label="post link to Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022" class="entry-link" href="https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xin_wang Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019 Author: Xin Wang et. al. Publish Year: Review Date: Wed, Jan 18, 2023 Summary of paper Motivation Visual Language Navigation (VLN) presents some unique challenges
first, reasoning over images and natural language instructions can be difficult. secondly, except for strictly following expert demonstrations, the feedback is rather coarse, since the “Success” feedback is provided only when the agent reaches a target position (sparse reward) A good “instruction following” trajectory may ended up just stop before you reaching the goal state and then receive zero rewards. existing work suffer from generalisation problem. (need to retrain the agent in new environment) Implementation agent can infer which sub-instruction to focus on and where to look at. (automatic splitting long instruction) with a matching critic that evaluates an executed path by the probability of reconstructing the original instruction from the executed path. P(original instruction | past trajectory) cycle reconstruction: we have P(target trajectory | the instruction) = 1, and we want to measure P(original instruction | past trajectory) this will enhance the interpretability as now you understand how the robot was thinking about </p>
</div>
<footer class="entry-footer"><span title="2023-01-18 09:48:14 +1100 AEDT">January 18, 2023</span> · 1 min · 195 words · Sukai Huang</footer>
<a aria-label="post link to Xin_wang Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019" class="entry-link" href="https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Alekh_agarwal PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning Author: Alekh Agarwal et. al. Publish Year: Review Date: Wed, Dec 28, 2022 Summary of paper Motivation The primary drawback of direct policy gradient methods is that, by being local in nature, they fail to adequately explore the environment. In contrast, while model-based approach and Q-learning directly handle exploration through the use of optimism. Contribution Policy Cover-Policy Gradient algorithm (PC-PG), a direct, model-free, policy optimisation approach which addresses exploration through the use of a learned ensemble of policies, the latter provides a policy cover over the state space. the use of a learned policy cover address exploration, and also address what is the catastrophic forgetting problem in policy gradient approaches (which use reward bonuses); the on-policy algorithm, where approximation errors due to model mispecification amplify (see [Lu et al., 2018] for discussion) Some key terms suffering from sparse reward
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 14:39:25 +1100 AEDT">December 28, 2022</span> · 2 min · 271 words · Sukai Huang</footer>
<a aria-label="post link to Alekh_agarwal PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning 2020" class="entry-link" href="https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Alekh_agarwal on the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: On the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020 Author: Alekh Agarwal et. al. Publish Year: 14 Oct 2020 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution and how they cope with approximation error due to using a restricted class of parametric policies. Contribution One central contribution of this work is in providing approximation guarantees that are average case - which avoid explicit worst-case dependencies on the size of state space – by making a formal connection to supervised learning under distribution shift. This characterisation shows an important between estimation error, approximation error and exploration (as characterised through a precisely defined condition number) Some key terms basic theoretical convergence questions
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 14:36:20 +1100 AEDT">December 28, 2022</span> · 3 min · 557 words · Sukai Huang</footer>
<a aria-label="post link to Alekh_agarwal on the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020" class="entry-link" href="https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Chloe_ching_yun_hsu Revisiting Design Choices in Proximal Policy Optimisation 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Revisiting Design Choices in Proximal Policy Optimisation Author: Chloe Ching-Yun Hsu et. al. Publish Year: 23 Sep 2020 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation Contribution on discrete action space with sparse high rewards, standard PPO often gets stuck at suboptimal actions. Why analyze the reason fort these failure modes and explain why they are not exposed by standard benchmarks In summary, our study suggests that Beta policy parameterization and KL-regularized objectives should be reconsidered for PPO, especially when alternatives improves PPO in all settings. The author proved the convergence guarantee for PPO-KL penalty version, as it inherits convergence guarantees of mirror descent for policy families that are closed under mixture Some key terms design choices
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 14:32:15 +1100 AEDT">December 28, 2022</span> · 3 min · 467 words · Sukai Huang</footer>
<a aria-label="post link to Chloe_ching_yun_hsu Revisiting Design Choices in Proximal Policy Optimisation 2020" class="entry-link" href="https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Generalized Proximal Policy Optimisation With Sample Reuse 2021 Author: James Queeney et. al. Publish Year: 29 Oct 2021 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. Contribution in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO this motivate an off-policy version of the popular algorithm that we call GePPO. we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency Some key terms sample complexity
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 14:00:32 +1100 AEDT">December 28, 2022</span> · 5 min · 1033 words · Sukai Huang</footer>
<a aria-label="post link to James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021" class="entry-link" href="https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/lun_wang-backdoorl-backdoor-attack-against-competitive-reinforcement-learning-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Lun_wang Backdoorl Backdoor Attack Against Competitive Reinforcement Learning 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: BackdooRL Backdoor Attack Against Competitive Reinforcement Learning 2021 Author: Lun Wang et. al Publish Year: 12 Dec 2021 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation in this paper, we propose BACKDOORL, a backdoor attack targeted at two player competitive reinforcement learning systems. first the adversary agent has to lead the victim to take a series of wrong actions instead of only one to prevent it from winning. Additionally, the adversary wants to exhibit the trigger action in as few steps as possible to avoid detection. Contribution we propose backdoorl, the first backdoor attack targeted at competitive reinforcement learning systems. The trigger is the action of another agent in the environment. We propose a unified method to design fast-failing agent for different environment We prototype BACKDOORL and evaluate it in four environments. The results validate the feasibility of backdoor attacks in competitive environment We study the possible defenses for backdoorl. The results show that fine-tuning cannot completely remove the backdoor. Some key terms backdoorl workflow
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 03:57:59 +1100 AEDT">December 28, 2022</span> · 1 min · 202 words · Sukai Huang</footer>
<a aria-label="post link to Lun_wang Backdoorl Backdoor Attack Against Competitive Reinforcement Learning 2021" class="entry-link" href="https://sino-huang.github.io/posts/lun_wang-backdoorl-backdoor-attack-against-competitive-reinforcement-learning-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/sandy_huang-adversarial-attacks-on-neural-network-policies-2017/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Sandy_huang Adversarial Attacks on Neural Network Policies 2017
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Adversarial Attacks on Neural Network Policies Author: Sandy Huang et. al. Publish Year: 8 Feb 2017 Review Date: Wed, Dec 28, 2022 Summary of paper Motivation in this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Contribution we characterise the degree of vulnerability across tasks and training algorithm, for a subclass of adversarial example attacks in white-box and black-box settings.
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-28 00:08:22 +1100 AEDT">December 28, 2022</span> · 2 min · 346 words · Sukai Huang</footer>
<a aria-label="post link to Sandy_huang Adversarial Attacks on Neural Network Policies 2017" class="entry-link" href="https://sino-huang.github.io/posts/sandy_huang-adversarial-attacks-on-neural-network-policies-2017/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/yinglun_xu-efficient-reward-poisoning-attacks-on-online-deep-reinforcement-learning-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Yinglun_xu Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning Author: Yinglun Xu et. al. Publish Year: 30 May 2022 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation we study data poisoning attacks on online deep reinforcement learning (DRL) where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment. we instantiate our framework to construct several attacks which only corrupts the rewards for a small fraction of the total training timesteps and make the agent learn a low performing policy Contribution result show that the reward attack efficiently poison agent learning with a variety of SOTA DRL algorithm such as DQN, PPO our attack can work on model-free DRL algorithm for all popular learning paradigms, and only assume the learning algorithm to be efficient. large enough reward poisoning attack in the right direction is able to disrupt the DRL algorithm. limitation
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 23:14:19 +1100 AEDT">December 27, 2022</span> · 2 min · 302 words · Sukai Huang</footer>
<a aria-label="post link to Yinglun_xu Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning 2022" class="entry-link" href="https://sino-huang.github.io/posts/yinglun_xu-efficient-reward-poisoning-attacks-on-online-deep-reinforcement-learning-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/young_wu-reward-poisoning-attacks-on-offline-multi-agent-reinforcement-learning-2022/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Young_wu Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning 2022
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning Author: Young Wu et. al. Publish Year: 1 Dec 2022 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Contribution unlike attacks on single-agent RL, we show that the attacker can install the target poilcy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow. This attack can be significantly cheaper than separate single-agent attacks. Limitation
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 22:50:14 +1100 AEDT">December 27, 2022</span> · 1 min · 146 words · Sukai Huang</footer>
<a aria-label="post link to Young_wu Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning 2022" class="entry-link" href="https://sino-huang.github.io/posts/young_wu-reward-poisoning-attacks-on-offline-multi-agent-reinforcement-learning-2022/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xuezhou_zhang Robust Policy Gradient Against Strong Data Corruption 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Robust Policy Gradient Against Strong Data Corruption Author: Xuezhou Zhang et. al. Publish Year: 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Abstract Contribution the author utilised a SVD-denoising technique to identify and remove the possible reward perturbations this approach gives a robust RL algorithm Limitation
This approach only solve the attack perturbation that is not consistent. (i.e. not stealthy) Some key terms Policy gradient methods
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 20:35:10 +1100 AEDT">December 27, 2022</span> · 2 min · 317 words · Sukai Huang</footer>
<a aria-label="post link to Xuezhou_zhang Robust Policy Gradient Against Strong Data Corruption 2021" class="entry-link" href="https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/kiarash_banihashem-defense-against-reward-poisoning-attacks-in-reinforcement-learning-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Kiarash_banihashem Defense Against Reward Poisoning Attacks in Reinforcement Learning 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Defense Against Reward Poisoning Attacks in Reinforcement Learning Author: Kiarash Banihashem et. al. Publish Year: 20 Jun 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true unpoisoned rewards while computing their policies under the poisoned rewards. Contribution we formalise this reasoning and characterize the utility of our novel framework for designing defense policies. In summary, the key contributions include
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 18:27:17 +1100 AEDT">December 27, 2022</span> · 2 min · 303 words · Sukai Huang</footer>
<a aria-label="post link to Kiarash_banihashem Defense Against Reward Poisoning Attacks in Reinforcement Learning 2021" class="entry-link" href="https://sino-huang.github.io/posts/kiarash_banihashem-defense-against-reward-poisoning-attacks-in-reinforcement-learning-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/amin_rakhsha-reward-poisoning-in-reinforcement-learning-attacks-against-unknown-learners-in-unknown-environments-2021/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Amin_rakhsha Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments 2021
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments Author: Amin Rakhsha et. al. Publish Year: 16 Feb 2021 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Our attack makes minimum assumptions on the prior knowledge of the environment or the learner’s learning algorithm. most of the prior work makes strong assumptions on the knowledge of adversary – it often assumed that the adversary has full knowledge of the environment or the agent’s learning algorithm or both. under such assumptions, attack strategies have been proposed that can mislead the agent to learn a nefarious policy with minimal perturbation to the rewards. Contribution We design a novel black-box attack, U2, that can provably achieve a near-matching performance to the SOTA white-box attack, demonstrating the feasibility of reward poisoning even in the most challenging black-box setting. limitation
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 15:50:22 +1100 AEDT">December 27, 2022</span> · 2 min · 233 words · Sukai Huang</footer>
<a aria-label="post link to Amin_rakhsha Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments 2021" class="entry-link" href="https://sino-huang.github.io/posts/amin_rakhsha-reward-poisoning-in-reinforcement-learning-attacks-against-unknown-learners-in-unknown-environments-2021/"></a>
</article>
<article class="post-entry tag-entry">
<figure class="entry-cover"><img alt="" loading="lazy" src="https://sino-huang.github.io/posts/xuezhou_zhang-adaptive-reward-poisoning-attacks-against-reinforcement-learning-2020/image-assets/cover.png"/>
</figure>
<header class="entry-header">
<h2 class="entry-hint-parent">Xuezhou_zhang Adaptive Reward Poisoning Attacks Against Reinforcement Learning 2020
    </h2>
</header>
<div class="entry-content">
<p>[TOC]
Title: Adaptive Reward Poisoning Attacks Against Reinforcement Learning Author: Xuezhou Zhang et. al. Publish Year: 22 Jun, 2020 Review Date: Tue, Dec 27, 2022 Summary of paper Motivation Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$ whereas non-adaptive attacks require exponential steps. Contribution we provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe. similar to this paper, it shows that reward attack has its limit we provide a corresponding upper threshold above which the attack is feasible. we characterise conditions under which such attacks are guaranteed to fail (thus RL is safe), and vice versa in the case where attack is feasible, we provide upper bounds on the attack cost in the processing of achieving bad poliy we show that effective attacks can be found empirically using deep RL techniques. Some key terms feasible attack category
...</p>
</div>
<footer class="entry-footer"><span title="2022-12-27 00:21:15 +1100 AEDT">December 27, 2022</span> · 2 min · 283 words · Sukai Huang</footer>
<a aria-label="post link to Xuezhou_zhang Adaptive Reward Poisoning Attacks Against Reinforcement Learning 2020" class="entry-link" href="https://sino-huang.github.io/posts/xuezhou_zhang-adaptive-reward-poisoning-attacks-against-reinforcement-learning-2020/"></a>
</article>
<footer class="page-footer">
<ul class="post-tags">
</ul>
<nav class="pagination">
<a class="prev" href="https://sino-huang.github.io/categories/paper-review/page/4/">
      « Prev 4/8
    </a>
<a class="next" href="https://sino-huang.github.io/categories/paper-review/page/6/">Next 6/8 »
    </a>
</nav>
</footer>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>
</html>
