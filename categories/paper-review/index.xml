<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Review on Sukai Huang</title>
    <link>https://sino-huang.github.io/categories/paper-review/</link>
    <description>Recent content in Paper Review on Sukai Huang</description>
    <image>
      <title>Sukai Huang</title>
      <url>https://sino-huang.github.io/sukai_avatar.jpg</url>
      <link>https://sino-huang.github.io/sukai_avatar.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Sun, 09 Feb 2025 21:07:11 +1100</lastBuildDate>
    <atom:link href="https://sino-huang.github.io/categories/paper-review/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Awesome LLMs Reasoning Abilities Papers</title>
      <link>https://sino-huang.github.io/posts/awesome_llm_reasoning_capability_papers/</link>
      <pubDate>Sun, 09 Feb 2025 21:07:11 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/awesome_llm_reasoning_capability_papers/</guid>
      <description>&lt;h3 id=&#34;demystifying-long-chain-of-thought-reasoning-in-llms&#34;&gt;Demystifying Long Chain-of-Thought Reasoning in LLMs&lt;/h3&gt;
&lt;p&gt;This study systematically investigate the mechanics of long CoT reasoning,  identifying the key factors that enable models to generate long CoT  trajectories and providing practical guidance for optimizing training  strategies to enhance long CoT reasoning in LLMs&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.03373.pdf&#34;&gt;https://arxiv.org/pdf/2502.03373.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning&#34;&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;This work introduces first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, which incorporates multi-stage training and cold-start  data before RL and achieves performance comparable to OpenAI-o1-1217 on  reasoning tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pallagani Plansformer Generating Plans 2023</title>
      <link>https://sino-huang.github.io/posts/pallagani-plansformer-generating-plans-2023/</link>
      <pubDate>Tue, 24 Dec 2024 22:25:58 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/pallagani-plansformer-generating-plans-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Pallagani Plansformer Generating Plans 2023&lt;/li&gt;
&lt;li&gt;Author: Pallagani, Vishal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: GenPlan 2023 Workshop&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 24, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiw.org/pdf/2212.08681&#34;&gt;https://arxiw.org/pdf/2212.08681&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;# input bibtex here&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@InProceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;pallagani2023plansformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Pallagani, Vishal and Muppasani, Bharath and Murugesan, Keerthiram and Rossi, Francesca and Horesh, Lior and Srivastava, Biplav and Fabiano, Francesco and Loreggia, Andrea}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Plansformer: Generating Symbolic Plans using Transformers}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Seventh Workshop on Generalization in Planning (GenPlan 2023)}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;      &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2023}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;month&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{December}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;address&lt;/span&gt;   &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{New Orleans, USA}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;venue&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Room 238-239, New Orleans Ernest N. Morial Convention Center}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c&#34;&gt;Pallagani, Vishal, et al. &amp;#34;Plansformer: Generating Symbolic Plans using Transformers.&amp;#34; NeurIPS 2023 Workshop on Generalization in Planning.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;[!Note]&lt;/p&gt;</description>
    </item>
    <item>
      <title>Damai Dai Deepseekmoe 2024</title>
      <link>https://sino-huang.github.io/posts/damai-dai-deepseekmoe-2024/</link>
      <pubDate>Sat, 22 Jun 2024 11:13:50 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/damai-dai-deepseekmoe-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture of Experts Language Models&lt;/li&gt;
&lt;li&gt;Author: Damai Dai et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 11 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Jun 22, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2401.06066&#34;&gt;https://arxiv.org/pdf/2401.06066&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240622111425099&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/damai-dai-deepseekmoe-2024/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;conventional MoE architecture like GShard, which avtivate top-k out of N experts, face challenges in ensuring expert specialization, i.e., each expert acquires non-overlapping and focused knowledge,&lt;/li&gt;
&lt;li&gt;in response, we propose DeepSeekMoE architecture towards ultimate expert specialization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;segmenting expert into mN ones and activating mK from them&lt;/li&gt;
&lt;li&gt;isolating K_s, experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;MoE architecture&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jessy Lin Learning to Model the World With Language 2024</title>
      <link>https://sino-huang.github.io/posts/jessy-lin-learning-to-model-the-world-with-language-2024/</link>
      <pubDate>Fri, 21 Jun 2024 11:47:25 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jessy-lin-learning-to-model-the-world-with-language-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Learning to Model the World With Language 2024&lt;/li&gt;
&lt;li&gt;Author: Jessy Lin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: ICML 2024&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Jun 21, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2308.01399&#34;&gt;https://arxiv.org/abs/2308.01399&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240621173424831&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jessy-lin-learning-to-model-the-world-with-language-2024/image-assets/image-20240621173424831.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this work, we propose that agents can ground diverse kinds of language by using it to predict the future&lt;/li&gt;
&lt;li&gt;in contrast to directly predicting what to do with a language-conditioned policy, Dynalang decouples learning to model the world with language (supervised learning with prediction objectives) from learning to act given that model (RL with task rewards)&lt;/li&gt;
&lt;li&gt;Future prediction provides a rich grounding signal for learning what language utterances mean, which in turn equip the agent with a richer understanding of the world to solve complex tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;investigate whether learning language-conditioned world models enable agents to scale to more diverse language use, compared to language-conditioned policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;related work&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiuzhou Reward Engineering for Generating Semi Structured Explan 2023</title>
      <link>https://sino-huang.github.io/posts/jiuzhou-reward-engineering-for-generating-semi-structured-explan-2023/</link>
      <pubDate>Thu, 20 Jun 2024 14:11:32 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jiuzhou-reward-engineering-for-generating-semi-structured-explan-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Engineering for Generating Semi-Structured Explanation&lt;/li&gt;
&lt;li&gt;Author: Jiuzhou Han et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: EACL2024&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jun 20, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG&#34;&gt;https://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the objective is to equip moderately-sized LMs with the ability to not only provide answers but also generate structured explanations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Intro&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the author talked about some background on Cui et al. incorporate a generative pre-training mechanism over synthetic graphs by aligning inputs pairs of text-graph to improve the model&amp;rsquo;s capability in generating semi-structured explanation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiuzhou Towards Uncertainty Aware Lang Agent 2024</title>
      <link>https://sino-huang.github.io/posts/jiuzhou-towards-uncertaintty-aware-lang-agent-2024/</link>
      <pubDate>Thu, 20 Jun 2024 11:15:18 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jiuzhou-towards-uncertaintty-aware-lang-agent-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Towards Uncertainty Aware Language Agent&lt;/li&gt;
&lt;li&gt;Author: Jiuzhou Han et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 30 May 2024&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jun 20, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.14016v3&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240620111719451&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jiuzhou-towards-uncertaintty-aware-lang-agent-2024/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The existing approaches neglect the notion of uncertainty during these interactions&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Related work 1: lang agent&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the author define what is language agent and discuss it &amp;ndash; the prominent work of ReAct propose a general language agent framework to combine reasoning and acting with LLMs for solving diverse language reasoning tasks.&lt;/li&gt;
&lt;li&gt;continue the track of ReAct &amp;ndash; introducing Reflexion, use the history failure trials as input to ask for reflection and can gain better results&lt;/li&gt;
&lt;li&gt;FireAct &amp;ndash; add more diverse fine-tuning data to improve the performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Later the author mention Toolformer, Gorilla and other lang agent that is not start from ReAct&lt;/p&gt;</description>
    </item>
    <item>
      <title>Silviu Pitis Failure Modes of Learning Reward Models for Sequence Model 2023</title>
      <link>https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/</link>
      <pubDate>Fri, 10 May 2024 22:23:31 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Failure Modes of Learning Reward Models for LLMs and other Sequence Models&lt;/li&gt;
&lt;li&gt;Author: Silviu Pitis&lt;/li&gt;
&lt;li&gt;Publish Year: ICML workshop 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, May 10, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://openreview.net/forum?id=NjOoxFRZA4&amp;amp;noteId=niZsZfTPPt&#34;&gt;https://openreview.net/forum?id=NjOoxFRZA4&amp;noteId=niZsZfTPPt&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510222642292&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;c3-preference-cannot-represented-as-numbers&#34;&gt;C3. Preference cannot represented as numbers&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510222758050&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510222758050.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;m1-rationality-level-of-human-preference&#34;&gt;M1. rationality level of human preference&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510223017797&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510223017797.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-if-the-conditioncontext-changes-the-preference-may-change-rapidly-and-this-cannot-reflect-on-the-reward-machine&#34;&gt;3.2, if the condition/context changes, the preference may change rapidly, and this cannot reflect on the reward machine&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510224002585&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/silviu-pitis-failure-modes-of-learning-reward-models-for-sequence-model-2023/image-assets/image-20240510224002585.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;a2-preference-should-be-expressed-with-respect-to-state-policy-pairs-rather-than-just-outcomes&#34;&gt;A2. Preference should be expressed with respect to state-policy pairs, rather than just outcomes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A state-policy pair includes both the current state of the system and the strategy (policy) being employed. This approach avoids the complication of unresolved stochasticity (randomness that hasn&amp;rsquo;t yet been resolved), focusing instead on scenarios where the outcomes of policies are already known.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example with Texas Hold’em&lt;/strong&gt;: The author uses an example  from poker to illustrate these concepts. In the example, a player  holding a weaker hand (72o) wins against a stronger hand (AA) after both commit to large bets pre-flop. Traditional reward modeling would prefer the successful trajectory of the weaker hand due to the positive  outcome. However, a rational analysis (ignoring stochastic outcomes)  would prefer the decision-making associated with the stronger hand (AA), even though it lost, as it&amp;rsquo;s typically the better strategy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gaurav Ghosal the Effect of Modeling Human Rationality Level 2023</title>
      <link>https://sino-huang.github.io/posts/gaurav-ghosal-the-effect-of-modeling-human-rationality-level-2023/</link>
      <pubDate>Fri, 10 May 2024 19:35:03 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/gaurav-ghosal-the-effect-of-modeling-human-rationality-level-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types&lt;/li&gt;
&lt;li&gt;Author: Gaurav R. Ghosal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 9 Mar 2023 AAAI 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, May 10, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2208.10687v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510211346583&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/gaurav-ghosal-the-effect-of-modeling-human-rationality-level-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We find that overestimating human rationality can have dire effects on reward learning accuracy and regret&lt;/li&gt;
&lt;li&gt;We also find that fitting the rationality coefficient to human data enables better reward learning, even when the human deviates significantly from the noisy-rational choice model due to systematic biases&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What is Boltzmann Rationality coefficient $\beta$&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nate Rahn Policy Optimization in Noisy Neighbourhood 2023</title>
      <link>https://sino-huang.github.io/posts/nate-rahn-policy-optimization-in-noisy-neighbourhood-2023/</link>
      <pubDate>Fri, 10 May 2024 14:16:56 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/nate-rahn-policy-optimization-in-noisy-neighbourhood-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Policy Optimization in Noisy Neighborhood&lt;/li&gt;
&lt;li&gt;Author: Nate Rahn et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: NeruIPS 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, May 10, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2309.14597&#34;&gt;https://arxiv.org/abs/2309.14597&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240510141939305&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/nate-rahn-policy-optimization-in-noisy-neighbourhood-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we demonstrate that high-frequency discontinuities in the mapping from policy parameters $\theta$ to return $R(\theta)$​ are an important cause of return variation.&lt;/li&gt;
&lt;li&gt;As a consequence of these discontinuities, a single gradient step or perturbation to the policy parameters often causes important changes in the return, even in settings where both the policy and the dynamics are deterministic.
&lt;ul&gt;
&lt;li&gt;unstable learning in some sense&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;based on this observation, we demonstrate the usefulness of studying the landscape through the distribution of returns obtained from small perturbation of $\theta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Evidence that noisy reward signal leads to substantial variance in performance&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ademi Adeniji Language Reward Modulation for Pretraining Rl 2023</title>
      <link>https://sino-huang.github.io/posts/ademi-adeniji-language-reward-modulation-for-pretraining-rl-2023/</link>
      <pubDate>Thu, 09 May 2024 21:18:00 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/ademi-adeniji-language-reward-modulation-for-pretraining-rl-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language Reward Modulation for Pretraining Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Ademi Adeniji et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: ICLR 2023 reject&lt;/li&gt;
&lt;li&gt;Review Date: Thu, May 9, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://openreview.net/forum?id=SWRFC2EupO&#34;&gt;https://openreview.net/forum?id=SWRFC2EupO&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240509213653815&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ademi-adeniji-language-reward-modulation-for-pretraining-rl-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Learned reward function (LRF) are notorious for noise and reward misspecification errors&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which can render them highly unreliable for learning robust policies with RL&lt;/li&gt;
&lt;li&gt;due to issues of reward exploitation and noisy models that these LRF’s are ill-suited for directly learning downstream tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Generalization ability issue of multi-modal vision and language model (VLM)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Thomas Coste Reward Model Ensembles Help Mitigate Overoptimization 2024</title>
      <link>https://sino-huang.github.io/posts/thomas-coste-reward-model-ensembles-help-mitigate-overoptimization-2024/</link>
      <pubDate>Thu, 09 May 2024 14:06:33 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/thomas-coste-reward-model-ensembles-help-mitigate-overoptimization-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Model Ensembles Help Mitigate Overoptimization&lt;/li&gt;
&lt;li&gt;Author: Thomas Coste et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Mar 2024&lt;/li&gt;
&lt;li&gt;Review Date: Thu, May 9, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2310.02743v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240509140808445&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/thomas-coste-reward-model-ensembles-help-mitigate-overoptimization-2024/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;however, as imperfect representation of the &amp;ldquo;true&amp;rdquo; reward, these learned reward models are susceptible to over-optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author conducted a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specially worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization&lt;/li&gt;
&lt;li&gt;the author additionally extend the setup to include 25% label noise to better mirror real-world conditions&lt;/li&gt;
&lt;li&gt;For PPO, ensemble-based conservative optimization always reduce overoptimization and outperforms single reward model optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Overoptimization&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mengdi Li Internally Rewarded Rl 2023</title>
      <link>https://sino-huang.github.io/posts/mengdi-li-internally-rewarded-rl-2023/</link>
      <pubDate>Wed, 08 May 2024 14:59:15 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/mengdi-li-internally-rewarded-rl-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Internally Rewarded Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Mengdi Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023 PMLR&lt;/li&gt;
&lt;li&gt;Review Date: Wed, May 8, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://proceedings.mlr.press/v202/li23ax.html&#34;&gt;https://proceedings.mlr.press/v202/li23ax.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;img src=&#34;image-assets/cover.png&#34; alt=&#34;image-20240508150740997&#34; style=&#34;zoom: 50%;&#34; /&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author studied a class o RL problem where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy (parallel training on both the policy and the reward model)&lt;/li&gt;
&lt;li&gt;this leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning , and conversely, an under-optimized policy impedes discriminator learning&lt;/li&gt;
&lt;li&gt;we call this learning setting Internally Rewarded RL (IRRL) as the reward is not provided directly by the environment but internally by the discriminator.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;proposed the clipped linear reward function. Results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance.&lt;/li&gt;
&lt;li&gt;we formulate a class of RL problems as IRRL, and formulate the inherent issues of noisy rewards that leads to an unstable training loop in IRRL&lt;/li&gt;
&lt;li&gt;we empirically characterize the noise in the discriminator and derive &lt;em&gt;the&lt;/em&gt; &lt;em&gt;effect of the reward function in reducing the bias of the estimated reward and the variance of reward noise from an underdeveloped discriminator&lt;/em&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Comment&lt;/strong&gt;: the author tried to express the bias and variance of reward noises in Taylor approximation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;propose clipped linear reward function&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Simultaneous optimization causes suboptimal training&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xuran Pan on the Integration of Self Attention and Convolution 2022</title>
      <link>https://sino-huang.github.io/posts/xuran-pan-on-the-integration-of-self-attention-and-convolution-2022/</link>
      <pubDate>Thu, 25 Apr 2024 17:53:46 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/xuran-pan-on-the-integration-of-self-attention-and-convolution-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: On the Integration of Self-Attention and Convolution&lt;/li&gt;
&lt;li&gt;Author: Xuran Pan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022 IEEE&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Apr 25, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2111.14556&#34;&gt;https://arxiv.org/abs/2111.14556&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;there exists a strong underlying relation between convolution and self-attention.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240425231916932&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/xuran-pan-on-the-integration-of-self-attention-and-convolution-2022/image-assets/image-20240425231916932.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related work&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Convolution NN&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it uses convolution kernels to extract local features, have become the most powerful and conventional technique for various vision tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Self-attention only&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recently, vision transformer shows that given enough data, we can treat an image as a sequence of 256 tokens and leverage Transformer models to achieve competitive results in image recognition.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Attention enhanced convolution&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Recent Language Model Technique 2024</title>
      <link>https://sino-huang.github.io/posts/recent-language-model-technique-2024/</link>
      <pubDate>Thu, 25 Apr 2024 12:49:03 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/recent-language-model-technique-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Recent Language Model Technique 2024&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Apr 25, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.youtube.com/watch?v=kzB23CoZG30&#34;&gt;https://www.youtube.com/watch?v=kzB23CoZG30&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;url2: &lt;a href=&#34;https://www.youtube.com/watch?v=iH-wmtxHunk&#34;&gt;https://www.youtube.com/watch?v=iH-wmtxHunk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;url3: &lt;a href=&#34;https://www.youtube.com/watch?v=o68RRGxAtDo&#34;&gt;https://www.youtube.com/watch?v=o68RRGxAtDo&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;llama-3&#34;&gt;LLama 3&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240425125031837&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/recent-language-model-technique-2024/image-assets/image-20240425125031837.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;key modification: grouped query attention (GQA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;key instruction-tuning process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Their approach to post-training is a combination of supervised fine-tuning (SFT), rejection sampling, proximal policy optimization (PPO), and direct preference optimization (DPO).&lt;/li&gt;
&lt;li&gt;The quality of the prompts that are used in SFT and the preference rankings that are used in PPO and DPO has an &lt;strong&gt;&lt;u&gt;&lt;em&gt;outsized influence&lt;/em&gt;&lt;/u&gt;&lt;/strong&gt; on the performance of aligned models.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fine-tuning tool: &lt;a href=&#34;https://github.com/pytorch/torchtune&#34;&gt;torchtune&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Thomas Carta Grounding Llms in Rl 2023</title>
      <link>https://sino-huang.github.io/posts/thomas-carta-grounding-llms-in-rl-2023/</link>
      <pubDate>Tue, 23 Apr 2024 13:20:22 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/thomas-carta-grounding-llms-in-rl-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Thomas Carta el. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 6 Sep 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Apr 23, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2302.02662v3&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240423132057110&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/thomas-carta-grounding-llms-in-rl-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The author considered an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online reinforcement learning to improve its performance to solve goals (under the RL paradigm environment (MDP))&lt;/p&gt;</description>
    </item>
    <item>
      <title>Daniel Hierarchies of Reward Machines 2023</title>
      <link>https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/</link>
      <pubDate>Fri, 12 Apr 2024 15:12:54 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Hierarchies of Reward Machines&lt;/li&gt;
&lt;li&gt;Author: Daniel Furelos-Blanco et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 4 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Apr 12, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2205.15752&#34;&gt;https://arxiv.org/abs/2205.15752&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240412151420609&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/daniel-hierarchies-of-reward-machines-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Finite state machine are a simple yet powerful formalism for abstractly representing temporal tasks in a structured manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The work introduces Hierarchies of Reinforcement Models (HRMs) to enhance the abstraction power of existing models. Key contributions include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;HRM Abstraction Power&lt;/strong&gt;: HRMs allow for the creation of hierarchies of Reinforcement Models (RMs), enabling constituent RMs to call other RMs. It&amp;rsquo;s proven that any HRM can be converted into an equivalent flat HRM with identical behavior. However, the equivalent flat HRM can have significantly more states and edges, especially under specific conditions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shanchuan Efficient N Robust Exploration Through Discriminative Ir 2023</title>
      <link>https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/</link>
      <pubDate>Fri, 12 Apr 2024 15:07:58 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards&lt;/li&gt;
&lt;li&gt;Author: Shanchuan Wan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Apr 12, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2304.10770&#34;&gt;https://arxiv.org/abs/2304.10770&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240412151513920&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shanchuan-efficient-n-robust-exploration-through-discriminative-ir-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recent studies have shown the effectiveness of encouraging exploration with intrinsic rewards estimated from novelties in observations&lt;/li&gt;
&lt;li&gt;However, there is a gap between the novelty of an observation and an exploration, as both the stochasticity in the environment and agent&amp;rsquo;s behaviour may affect the observation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose DEIR, a novel method in which we theoretically derive an intrinsic reward with a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and then implement the reward
with a discriminative forward model.&lt;/li&gt;
&lt;li&gt;want to design a novel intrinsic reward design that considers not only the observed novelty but also the effective contribution brought by the agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;internal rewards&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Discover Hierarchical Achieve in Rl via Cl 2023</title>
      <link>https://sino-huang.github.io/posts/seungyong-discover-hierarchical-achieve-in-rl-via-cl-2023/</link>
      <pubDate>Tue, 02 Apr 2024 21:02:37 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/seungyong-discover-hierarchical-achieve-in-rl-via-cl-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning&lt;/li&gt;
&lt;li&gt;Author: Seungyong Moon et. al.&lt;/li&gt;
&lt;li&gt;Publish Year:  2 Nov 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Apr 2, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2307.03486&#34;&gt;https://arxiv.org/abs/2307.03486&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240402210833949&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/seungyong-discover-hierarchical-achieve-in-rl-via-cl-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PPO agents demonstrate some ability to predict future achievements. Leveraging this observation, a novel contrastive learning method called achievement distillation is introduced, enhancing the agent&amp;rsquo;s predictive abilities. This approach excels at discovering hierarchical achievements,&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model based and explicit module in previous studies are not that good&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jia Li Structured Cot Prompting for Code Generation 2023</title>
      <link>https://sino-huang.github.io/posts/jia-li-structured-cot-prompting-for-code-generation-2023/</link>
      <pubDate>Wed, 28 Feb 2024 19:59:38 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jia-li-structured-cot-prompting-for-code-generation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Structured Chaint of Thought Prompting for Code Generation 2023&lt;/li&gt;
&lt;li&gt;Author: Jia Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 7 Sep 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 28, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.06599.pdf&#34;&gt;https://arxiv.org/pdf/2305.06599.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240229114924548&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jia-li-structured-cot-prompting-for-code-generation-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The paper introduces Structured CoTs (SCoTs) and a novel prompting  technique called SCoT prompting for improving code generation with Large Language Models (LLMs) like ChatGPT and Codex. Unlike the previous  Chain-of-Thought (CoT) prompting, which focuses on natural language  reasoning steps, SCoT prompting leverages the structural information  inherent in source code. By incorporating program structures (sequence,  branch, and loop structures) into intermediate reasoning steps (SCoTs),  LLMs are guided to generate more structured and accurate code.  Evaluation on three benchmarks demonstrates that SCoT prompting  outperforms CoT prompting by up to 13.79% in Pass@1, is preferred by  human developers in terms of program quality, and exhibits robustness to various examples, leading to substantial improvements in code  generation performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stephanie Teaching Models to Express Their Uncertainty in Words 2022</title>
      <link>https://sino-huang.github.io/posts/stephanie-teaching-models-to-express-their-uncertainty-in-words-2022/</link>
      <pubDate>Wed, 28 Feb 2024 16:12:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/stephanie-teaching-models-to-express-their-uncertainty-in-words-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Teaching Models to Express Their Uncertainty in Words&lt;/li&gt;
&lt;li&gt;Author: Stephanie Lin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 13 Jun 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 28, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2205.14334.pdf&#34;&gt;https://arxiv.org/pdf/2205.14334.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240228162359685&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/stephanie-teaching-models-to-express-their-uncertainty-in-words-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The study demonstrates that a GPT-3 model can articulate uncertainty  about its answers in natural language without relying on model logits.  It generates both an answer and a confidence level (e.g., &amp;ldquo;90%  confidence&amp;rdquo; or &amp;ldquo;high confidence&amp;rdquo;), which map to well-calibrated  probabilities. The model maintains moderate calibration even under  distribution shift and shows sensitivity to uncertainty in its answers  rather than mimicking human examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gwenyth Estimating Confidence of Llm by Prompt Agreement 2023</title>
      <link>https://sino-huang.github.io/posts/gwenyth-estimating-confidence-of-llm-by-prompt-agreement-2023/</link>
      <pubDate>Tue, 27 Feb 2024 15:44:06 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/gwenyth-estimating-confidence-of-llm-by-prompt-agreement-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement&lt;/li&gt;
&lt;li&gt;Author: Gwenyth Portillo Wightman et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: TrustNLP 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Feb 27, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://aclanthology.org/2023.trustnlp-1.28.pdf&#34;&gt;https://aclanthology.org/2023.trustnlp-1.28.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240227154510805&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/gwenyth-estimating-confidence-of-llm-by-prompt-agreement-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;while traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated.&lt;/li&gt;
&lt;li&gt;the authors proposed a method that involves comparing generated outputs across diverse prompts to create confidence score. By utilizing multiple prompts, they aim to obtain more precise  confidence estimates, using response diversity as a measure of  confidence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The results show that this method produces more calibrated confidence  estimates compared to the log probability of the answer to a single  prompt, which could be valuable for users relying on prediction  confidence in larger systems or decision-making processes.&lt;/li&gt;
&lt;li&gt;in one sentence: try multiple times, get the mean, mean is more robust and consistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;calibrated confidence score&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sudhir Agarwal Translate Infer Compile for Accurate Text to Plan 2024</title>
      <link>https://sino-huang.github.io/posts/sudhir-agarwal-translate-infer-compile-for-accurate-text-to-plan-2024/</link>
      <pubDate>Sat, 17 Feb 2024 12:56:25 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/sudhir-agarwal-translate-infer-compile-for-accurate-text-to-plan-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  TIC: Translate-Infer-Compile for accurate &amp;ldquo;text to plan&amp;rdquo; using LLMs and logical intermediate representations&lt;/li&gt;
&lt;li&gt;Author: Sudhir Agarwal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Feb 17, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2402.06608.pdf&#34;&gt;https://arxiv.org/pdf/2402.06608.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240217125703052&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/sudhir-agarwal-translate-infer-compile-for-accurate-text-to-plan-2024/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;using an LLM to generate the task PDDL from a natural language planning task descriptions is challenging. One of the primary reasons for failure is that the LLM often make errors generating information that must abide by the constraints specified in the domain knowledge or the task descriptions&lt;/p&gt;</description>
    </item>
    <item>
      <title>Philip Cohen Intention Is Choice With Commitment 1990</title>
      <link>https://sino-huang.github.io/posts/philip-cohen-intention-is-choice-with-commitment-1990/</link>
      <pubDate>Tue, 30 Jan 2024 23:17:51 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/philip-cohen-intention-is-choice-with-commitment-1990/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Intention Is Choice With Commitment&lt;/li&gt;
&lt;li&gt;Author: Philip Cohen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1990&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Jan 30, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0004370290900555&#34;&gt;https://www.sciencedirect.com/science/article/pii/0004370290900555&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;This paper delves into the principles governing the rational balance  between an agent&amp;rsquo;s beliefs, goals, actions, and intentions, offering  valuable insights for both artificial agents and a theory of human  action. It focuses on clarifying when an agent can abandon their goals  and how strongly they are committed to these goals. The formalism used  in the paper captures several crucial aspects of intentions, including  an analysis of Bratman&amp;rsquo;s three characteristic functional roles of  intentions and how agents can avoid intending all the unintended  consequences of their actions. Furthermore, the paper discusses how  intentions can be shaped based on an agent&amp;rsquo;s relevant beliefs and other  intentions or goals. It also introduces a preliminary concept of  interpersonal commitments by relating one agent&amp;rsquo;s intentions to their  beliefs about another agent&amp;rsquo;s intentions or beliefs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Christian Muise Planning for Goal Oriented Dialgue Systems 2019</title>
      <link>https://sino-huang.github.io/posts/christian-muise-planning-for-goal-oriented-dialgue-systems-2019/</link>
      <pubDate>Tue, 30 Jan 2024 16:58:06 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/christian-muise-planning-for-goal-oriented-dialgue-systems-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Christian Muise Planning for Goal Oriented Dialgue Systems 2019&lt;/li&gt;
&lt;li&gt;Author:&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Jan 30, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:1910.08137v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;there is increasing demand for dialogue agents capable of handling specific tasks and interactions in a business context&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author propose a new approach that eliminates the need for manual specification of dialogue trees, a common practice in existing systems.&lt;/li&gt;
&lt;li&gt;they suggest using a declarative representation of the dialogue agent, which can be processed by advanced planning tech (tree -&amp;gt; planning)&lt;/li&gt;
&lt;li&gt;The paper introduces a paradigm shift in specifying complex dialogue  agents by recognizing that many aspects of these agents share  similarities or identical underlying processes. Instead of manually  creating and maintaining entire dialogue graphs, the authors propose a  declarative approach where behavior is specified compactly, and the  complete implicit graphs are generated from this specification.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of end to end trained machine learning architectures&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vishal Pallagani Llm N Planning Survey 2024</title>
      <link>https://sino-huang.github.io/posts/vishal-pallagani-llm-n-planning-survey-2024/</link>
      <pubDate>Mon, 29 Jan 2024 23:02:47 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/vishal-pallagani-llm-n-planning-survey-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  &amp;ldquo;On the Prospects of Incorporating Large  Language Models (LLMs) in Automated Planning and Scheduling (APS).&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Author: Pallagani, Vishal, et al.&lt;/li&gt;
&lt;li&gt;Publish Year:  &lt;em&gt;arXiv preprint arXiv:2401.02500&lt;/em&gt; (2024).&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url:&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The paper provides a comprehensive review of 126 papers focusing on the integration of Large Language Models (LLMs) within Automated Planning and Scheduling, a growing area in Artificial Intelligence (AI). It identifies eight categories where LLMs are applied in addressing various aspects of planning problems:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ishika Singh Progprompt Program Generation for Robot Task Planning 2023</title>
      <link>https://sino-huang.github.io/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/</link>
      <pubDate>Mon, 29 Jan 2024 20:45:59 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  ProgPrompt: program generation for situated robot task planning using large language models&lt;/li&gt;
&lt;li&gt;Author: Ishika Singh et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 28 August 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://progprompt.github.io/&#34;&gt;https://progprompt.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Classical Task planning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;requires myriad domain knowledge&lt;/li&gt;
&lt;li&gt;large serach space, hard toscale&lt;/li&gt;
&lt;li&gt;domain specific&lt;/li&gt;
&lt;li&gt;require concrete goal specification&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Planning with LLMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240129205334711&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/image-assets/image-20240129205334711.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM is not situated in the scene&lt;/li&gt;
&lt;li&gt;Plan steps using unavailable actions and objects&lt;/li&gt;
&lt;li&gt;Text-to-robot action mapping may not be trivial&lt;/li&gt;
&lt;li&gt;combinatorial admissible action space.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;present a programmatic LLM prompt structure that enables plan generation function across situated environments, robot capabilities and tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240129205546614&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ishika-singh-progprompt-program-generation-for-robot-task-planning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Avichai Levy Understanding Natural Language in Context 2023</title>
      <link>https://sino-huang.github.io/posts/avichai-levy-understanding-natural-language-in-context-2023/</link>
      <pubDate>Mon, 29 Jan 2024 20:25:43 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/avichai-levy-understanding-natural-language-in-context-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Understanding Natural Language in Context&lt;/li&gt;
&lt;li&gt;Author: Avichai Levy et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: ICAPS 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://ojs.aaai.org/index.php/ICAPS/article/view/27248&#34;&gt;https://ojs.aaai.org/index.php/ICAPS/article/view/27248&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240129203153924&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/avichai-levy-understanding-natural-language-in-context-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The paper discusses the increasing prevalence of applications with natural language interfaces, such as chatbots and personal assistants like Alexa, Google Assistant, Siri, and Cortana. While current dialogue systems mainly involve static robots, the challenge intensifies with cognitive robots capable of movement and object manipulation in home environments. The focus is on cognitive robots equipped with knowledge-based models of the world, enabling reasoning and planning. The paper proposes an approach to translate natural language directives into the robot&amp;rsquo;s formalism, leveraging state-of-the-art large language models, planning tools, and the robot&amp;rsquo;s knowledge of the world and its own model. This approach enhances the interpretation of directives in natural language, facilitating the completion of complex household tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mingyu Jin the Impact of Reasoning Steps Length on Llm 2024</title>
      <link>https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/</link>
      <pubDate>Mon, 29 Jan 2024 17:44:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/mingyu-jin-the-impact-of-reasoning-steps-length-on-llm-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: The Impact of Reasoning Steps Length on Large Language Models&lt;/li&gt;
&lt;li&gt;Author: Mingyu Jin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 20 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.04925v3&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The study investigates the impact of the length of reasoning steps in prompts on the reasoning abilities of Large Language Models (LLMs), focusing on Chain of Thought (CoT). Here are the key findings:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Effect of Reasoning Step Length&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision</title>
      <link>https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/</link>
      <pubDate>Mon, 29 Jan 2024 15:32:21 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Weak-To-Strong-Generalization: Eliciting Strong Capabilities with Weak Supervision&lt;/li&gt;
&lt;li&gt;Author: Collin Burns et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 14 Dec 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 29, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2312.09390v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240129161843295&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/collin-burns-weak-to-strong-generalisation-weak-supervision-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Superalignment: OPENAI believe that RLHF is essentially use human to supervise the model (RM is trained by human annotation). One day when superhuman models come out, human are no longer to annotate the good / bad of the model&amp;rsquo;s output. e.g., superhuman model generate a 1M lines complex code and human cannot review it.&lt;/li&gt;
&lt;li&gt;How to do the alignment in for this case?&lt;/li&gt;
&lt;li&gt;thus the research question is can we use a weak teacher model to improve strong student model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they used weak model to generate annotations and fine tune the strong model, they empirically did a lot of experiments&lt;/li&gt;
&lt;li&gt;note: although they use the term teacher and student, the alignment task is not about &amp;ldquo;teaching&amp;rdquo;, alignment is to elicit learnt stuffs from strong foundation model (something like finetuning), rather than asking strong model to follow weak teacher model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ziwei Xu Hallucination Is Inevitable an Innate Limitation Llm 2024</title>
      <link>https://sino-huang.github.io/posts/ziwei-xu-hallucination-is-inevitable-an-innate-limitation-llm-2024/</link>
      <pubDate>Sun, 28 Jan 2024 23:11:28 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ziwei-xu-hallucination-is-inevitable-an-innate-limitation-llm-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Hallucination Is Inevitable an Innate Limitation Llm 2024&lt;/li&gt;
&lt;li&gt;Author: Ziwei Xu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 28, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.11817v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;The paper formalizes the issue of hallucination in large language models (LLMs) and argues that it is impossible to completely eliminate hallucination. It defines hallucination as inconsistencies between a computable LLM and a computable ground truth function. By drawing from learning theory, the paper demonstrates that LLMs cannot learn all computable functions, thus always prone to hallucination. The formal world is deemed a simplified representation of the real world, implying that hallucination is inevitable for real-world LLMs. Additionally, for real-world LLMs with provable time complexity constraints, the paper identifies tasks prone to hallucination and provides empirical validation. Finally, the paper evaluates existing hallucination mitigators using the formal world framework and discusses practical implications for the safe deployment of LLMs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zhiwei He Improving Machine Translation Use Quality Estimation as a Reward Model 2024</title>
      <link>https://sino-huang.github.io/posts/zhiwei-he-improving-machine-translation-use-quality-estimation-as-a-reward-model-2024/</link>
      <pubDate>Sun, 28 Jan 2024 22:53:41 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zhiwei-he-improving-machine-translation-use-quality-estimation-as-a-reward-model-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Improving Machine Translation Use Quality Estimation as a Reward Model 2024&lt;/li&gt;
&lt;li&gt;Author: Zhiwei He et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 23 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 28, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.12873v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;In this research, the authors explore using Quality Estimation (QE) models as a basis for reward systems in translation quality improvement through human feedback. They note that while QE has shown promise aligning with human evaluations, there&amp;rsquo;s a risk of overoptimization where translations receive high rewards despite declining quality. The study addresses this by introducing heuristic rules to identify and penalize incorrect translations, resulting in improved training outcomes. Experimental results demonstrate consistent enhancements across various setups, validated by human preference studies. Additionally, the approach proves highly data-efficient, outperforming systems relying on larger parallel corpora with only a small amount of monolingual data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Krishan Rana Sayplan Grounding Llm for Scalable Task Planning 2023</title>
      <link>https://sino-huang.github.io/posts/krishan-rana-sayplan-grounding-llm-for-scalable-task-planning-2023/</link>
      <pubDate>Sun, 28 Jan 2024 21:37:21 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/krishan-rana-sayplan-grounding-llm-for-scalable-task-planning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: SayPlan: Grounding Large Language Models using 3D Scene for for Scalable Task Planning&lt;/li&gt;
&lt;li&gt;Author: Krishan Rana&lt;/li&gt;
&lt;li&gt;Publish Year: CoRL 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 28, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2307.06135&#34;&gt;https://arxiv.org/abs/2307.06135&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this is a pipeline introduction paper&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hierarchical Exploration&lt;/strong&gt;: SayPlan leverages the hierarchical structure of 3DSGs to enable LLMs to conduct semantic searches for task-relevant subgraphs from a condensed representation of the full graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Path Planning Integration&lt;/strong&gt;: It integrates a classical path planner to reduce the planning horizon for the LLM, thus improving efficiency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterative Replanning Pipeline&lt;/strong&gt;: An iterative replanning pipeline refines initial plans by incorporating feedback from a scene graph simulator, correcting infeasible actions and preventing planning failures.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240128220929621&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/krishan-rana-sayplan-grounding-llm-for-scalable-task-planning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Luigi Bonassi Planning With Qualitative Constraints Pddl3 2022</title>
      <link>https://sino-huang.github.io/posts/luigi-bonassi-planning-with-qualitative-constraints-pddl3-2022/</link>
      <pubDate>Sun, 28 Jan 2024 21:28:51 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/luigi-bonassi-planning-with-qualitative-constraints-pddl3-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Planning With Qualitative Constraints Pddl3 2022&lt;/li&gt;
&lt;li&gt;Author: Luigi Bonassi et. al.&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 28, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.ijcai.org/proceedings/2022/0639.pdf&#34;&gt;https://www.ijcai.org/proceedings/2022/0639.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper introduces a formalism to express trajectory constraints over actions in plans, complementing the state-trajectory constraints of PDDL3. This new formalism retains PDDL3&amp;rsquo;s temporal modal operators and adds two modalities. The authors then explore compilation-based methods for dealing with action-trajectory constraints in propositional planning, proposing a new, simple, and effective method. Experimental results demonstrate the utility of action-trajectory constraints for expressing control knowledge, showing significant performance improvements in classical planners when leveraging knowledge expressed through action constraints. Conversely, the same knowledge specified as state constraints and handled by two state-of-the-art systems yields less beneficial results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parsa Mahmoudieh Zero Shot Reward Specification via Grounded Natural Language 2022</title>
      <link>https://sino-huang.github.io/posts/parsa-mahmoudieh-zero-shot-reward-specification-via-grounded-natural-language-2022/</link>
      <pubDate>Sun, 28 Jan 2024 09:31:05 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/parsa-mahmoudieh-zero-shot-reward-specification-via-grounded-natural-language-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Zero Shot Reward Specification via Grounded Natural Language&lt;/li&gt;
&lt;li&gt;Author: Parsa Mahnoudieh et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: PMLR 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 28, 2024&lt;/li&gt;
&lt;li&gt;url:&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240128113716178&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/parsa-mahmoudieh-zero-shot-reward-specification-via-grounded-natural-language-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;reward signals in RL are expensive to design and often require access to the true state.&lt;/li&gt;
&lt;li&gt;common alternatives are usually demonstrations or goal images which can be label intensive&lt;/li&gt;
&lt;li&gt;on the other hand, text descriptions provide a general low-effect way of communicating.&lt;/li&gt;
&lt;li&gt;previous work rely on true state or labelled expert demonstration match,
&lt;ul&gt;
&lt;li&gt;this work directly use CLIP to convert the observation to semantic embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Difference&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Allen Z Ren Robots That Ask for Help Uncertainty Alignment 2023</title>
      <link>https://sino-huang.github.io/posts/allen-z-ren-robots-that-ask-for-help-uncertainty-alignment-2023/</link>
      <pubDate>Fri, 26 Jan 2024 17:29:29 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/allen-z-ren-robots-that-ask-for-help-uncertainty-alignment-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Robots That Ask for Help:  Uncertainty Alignment for Large Language Model Planners&lt;/li&gt;
&lt;li&gt;Author: Allen Z. Ren et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 4 Sep 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Jan 26, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2307.01928v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240127222901220&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/allen-z-ren-robots-that-ask-for-help-uncertainty-alignment-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LLMs have various capabilities but often make overly confident yet  incorrect predictions. KNOWNO aims to measure and align this  uncertainty, enabling LLM-based planners to recognize their limitations  and request assistance when necessary.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;built on theory of conformal prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Ambiguity in NL&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Marta Skreta Replan Robotic Replanning 2024</title>
      <link>https://sino-huang.github.io/posts/marta-skreta-replan-robotic-replanning-2024/</link>
      <pubDate>Thu, 25 Jan 2024 00:55:05 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/marta-skreta-replan-robotic-replanning-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: RePlan: Robotic Replanning with Perception and Language Models&lt;/li&gt;
&lt;li&gt;Author: Marta Skreta et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 8 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jan 25, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.04157v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240126170742457&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/marta-skreta-replan-robotic-replanning-2024/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;However, the challenge remains that even with syntac- tically correct plans, robots can still fail to achieve their intended goals. This failure can be attributed to imperfect plans proposed by LLMs or to unforeseeable environmental circumstances that hinder the execution of planned subtasks due to erroneous assumptions about the state of objects.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Robotic Replanning with Perception and Language Models that enables &lt;strong&gt;real-time replanning&lt;/strong&gt; capabilities for long-horizon tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Address the challenge of multi-stage long-horizon tasks&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Binghai Wang Secrets of Rlhf Reward Modelling 2024</title>
      <link>https://sino-huang.github.io/posts/binghai-wang-secrets-of-rlhf-reward-modelling-2024/</link>
      <pubDate>Wed, 24 Jan 2024 23:31:28 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/binghai-wang-secrets-of-rlhf-reward-modelling-2024/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Secrets of RLHF in Large Language Models Part II: Reward Modelling&lt;/li&gt;
&lt;li&gt;Author: Binghai Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 12 Jan 2024&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Jan 24, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2401.06080v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;a crucial technology for aligning language models with human values. Two main issues are tackled: (1) Incorrect and ambiguous preference pairs  in the dataset hindering reward model accuracy, and (2) Difficulty in  generalization for reward models trained on specific distributions.&lt;/li&gt;
&lt;li&gt;a method measuring preference strength within the data is proposed,  utilizing a voting mechanism of multiple reward models. Novel techniques are introduced to mitigate the impact of incorrect preferences and  leverage high-quality preference data. For the second issue, contrastive learning is introduced to enhance the reward models&amp;rsquo; ability to  distinguish between chosen and rejected responses, improving  generalization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;noisy data&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rui Zheng Secrets of Rlhf in Llm Part Ppo 2023</title>
      <link>https://sino-huang.github.io/posts/rui-zheng-secrets-of-rlhf-in-llm-part-ppo-2023/</link>
      <pubDate>Mon, 22 Jan 2024 20:26:18 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/rui-zheng-secrets-of-rlhf-in-llm-part-ppo-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Secrets of RLHF in Large Language Models Part1: PPO&lt;/li&gt;
&lt;li&gt;Author: Rui Zheng et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 Jul 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 22, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2307.04964v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Current approaches involve creating reward models to measure human  preferences, using Proximal Policy Optimization (PPO) to improve policy  models, and enhancing step-by-step reasoning through process  supervision. However, challenges in reward design, interaction with the  environment, and agent training, along with the high trial and error  costs of LLMs, make it difficult for researchers to develop technically  aligned and safe LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;finding that LLMs trained using their algorithm can better understand  query meanings and provide responses that resonate with people.&lt;/li&gt;
&lt;li&gt;A new PPO algorithm called PPO-max is introduced, which incorporates effective implementations and addresses stability issues.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RLHF limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zhiting Hu Language Agent and World Models 2023</title>
      <link>https://sino-huang.github.io/posts/zhiting-hu-language-agent-and-world-models-2023/</link>
      <pubDate>Mon, 22 Jan 2024 16:01:20 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zhiting-hu-language-agent-and-world-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Zhiting Hu Language Agent and World Models 2023&lt;/li&gt;
&lt;li&gt;Author:&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 22, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2312.05230v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240122201639803&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/zhiting-hu-language-agent-and-world-models-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LAW proposes that world and agent models, which encompass beliefs about  the world, anticipation of consequences, goals/rewards, and strategic  planning, provide a better abstraction of reasoning. In this framework,  language models play a crucial role as a backend&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Limitation of Language&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gautier Dagan Dynamic Planning With a Llm 2023</title>
      <link>https://sino-huang.github.io/posts/gautier-dagan-dynamic-planning-with-a-llm-2023/</link>
      <pubDate>Sun, 21 Jan 2024 01:42:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/gautier-dagan-dynamic-planning-with-a-llm-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Dynamic Planning With a LLM&lt;/li&gt;
&lt;li&gt;Author: Gautier Dagan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 11 Aug 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 21, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2308.06391v1&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240122133428294&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/gautier-dagan-dynamic-planning-with-a-llm-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Traditional symbolic planners can find optimal solutions quickly but  need complete and accurate problem representations. In contrast, LLMs  can handle noisy data and uncertainty but struggle with planning tasks.  The LLM-DP framework combines LLMs and traditional planners to solve  embodied tasks efficiently.&lt;/li&gt;
&lt;li&gt;Traditional Planner need maximal information&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jun Wang Conformal Temporal Logic Planning Using Llm 2023</title>
      <link>https://sino-huang.github.io/posts/jun-wang-conformal-temporal-logic-planning-using-llm-2023/</link>
      <pubDate>Sun, 21 Jan 2024 00:34:56 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jun-wang-conformal-temporal-logic-planning-using-llm-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Conformal Temporal Logic Planning Using Llm 2023&lt;/li&gt;
&lt;li&gt;Author: Jun Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 19 Dec, 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jan 21, 2024&lt;/li&gt;
&lt;li&gt;url: arXiv:2309.10092v2&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240121004104715&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jun-wang-conformal-temporal-logic-planning-using-llm-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unlike previous methods that focus on low-level system configurations, this approach focuses on NL-based atomic propositions.
&lt;ul&gt;
&lt;li&gt;now the LTL tasks are defined over NL-based atomic propositions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Robots are required to perform high-level sub tasks specified in natural language.
&lt;ul&gt;
&lt;li&gt;To formally define the overarching mission, they leverage LTL defined over atomic predicates modelling these NL-based sub-tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To address the challenge of ensuring the correctness of robot plans with respect to these LTL-encoded tasks, the authors propose HERACLEs, a  hierarchical conformal natural language planner. HERACLEs employs  automata theory to determine the next NL-specified sub-tasks for mission progress, employs Large Language Models to design robot plans to  fulfill these sub-tasks, and uses conformal prediction to assess the  probabilistic correctness of the plans, deciding whether external  assistance is needed. The paper provides theoretical probabilistic  guarantees for mission satisfaction and presents extensive comparative  experiments on mobile manipulation tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Limitation for previous work&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gerevini Plan Constraints and Preferences in Pddl3 2005</title>
      <link>https://sino-huang.github.io/posts/gerevini-plan-constraints-and-preferences-in-pddl3-2005/</link>
      <pubDate>Thu, 11 Jan 2024 19:54:29 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/gerevini-plan-constraints-and-preferences-in-pddl3-2005/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Gerevini Plan Constraints and Preferences in PDDL3&lt;/li&gt;
&lt;li&gt;Author: Alfonso Gerevini, Derek Long&lt;/li&gt;
&lt;li&gt;Publish Year: 2005&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jan 11, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;http://www.cs.yale.edu/~dvm/papers/pddl-ipc5.pdf&#34;&gt;http://www.cs.yale.edu/~dvm/papers/pddl-ipc5.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240111195650738&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/gerevini-plan-constraints-and-preferences-in-pddl3-2005/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the notion of plan quality in automated planning is a practically very important issue.&lt;/li&gt;
&lt;li&gt;it is important to generate plans of good or optimal quality and we need to express the plan quality&lt;/li&gt;
&lt;li&gt;the proposed extended language allows us to express strong and soft constraints on plan trajectories
&lt;ul&gt;
&lt;li&gt;i.e., constraints over possible actions in the plan and intermediate states reached by the plan&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;as well as strong and soft problem goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;some scenarios&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nir Lipo Planning With Perspectives Using Functional Strips 2022</title>
      <link>https://sino-huang.github.io/posts/nir-lipo-planning-with-perspectives-using-functional-strips-2022/</link>
      <pubDate>Thu, 11 Jan 2024 19:41:55 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/nir-lipo-planning-with-perspectives-using-functional-strips-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Planning With Perspectives &amp;ndash; Using Decomposing Epistemic Planning using Functional STRIPS&lt;/li&gt;
&lt;li&gt;Author: Guang Hu, Nir Lipovetzky&lt;/li&gt;
&lt;li&gt;Publish Year: 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jan 11, 2024&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://nirlipo.github.io/publication/hu-2022-planning/&#34;&gt;https://nirlipo.github.io/publication/hu-2022-planning/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20240111194239165&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/nir-lipo-planning-with-perspectives-using-functional-strips-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we present a novel approach to epistemic planning called planning with perspectives (PWP) that is both more expressive and computationally more efficient than existing state of the art epistemic planning tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we decompose epistemic planning by delegating reasoning about epistemic formulae to an external solver, i.e., Functional STRIPS&lt;/li&gt;
&lt;li&gt;F-STRIPS supports the user of external, black-box functions within action models.&lt;/li&gt;
&lt;li&gt;Building on recent work that demonstrates the relationship between what an agent &amp;lsquo;sees&amp;rsquo; and what it knows, we define the perspective of each agent using an external function, and build a solver for epistemic logic around this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;external functions (black-box)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alex_coulter Theory Alignment via a Classical Encoding of Regular Bismulation 2022</title>
      <link>https://sino-huang.github.io/posts/alex_coulter-theory-alignment-via-a-classical-encoding-of-regular-bismulation-2022/</link>
      <pubDate>Wed, 29 Nov 2023 17:24:08 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alex_coulter-theory-alignment-via-a-classical-encoding-of-regular-bismulation-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Theory Alignment via a Classical Encoding of Regular Bismulation 2022&lt;/li&gt;
&lt;li&gt;Author: Alex Coulter et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: KEPS 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Nov 29, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://icaps22.icaps-conference.org/workshops/KEPS/KEPS-22_paper_7781.pdf&#34;&gt;https://icaps22.icaps-conference.org/workshops/KEPS/KEPS-22_paper_7781.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20231129172526275&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alex_coulter-theory-alignment-via-a-classical-encoding-of-regular-bismulation-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the main question we seek to answer is how we can test if two models align (where the fluents and action implementations may differ), and if not, where that misalignment occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the work is built on a foundation of regular bisimulation&lt;/li&gt;
&lt;li&gt;found that the proposed alignment was not only viable, with many submissions having &amp;ldquo;solutions&amp;rdquo; to the merged model showing where a modelling error occurs, but several cases demonstrated errors with the submitted domains that were subtle and detected only by this added approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Bisimulation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pascal Bercher Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains 2023</title>
      <link>https://sino-huang.github.io/posts/pascal-bercher-detecting-ai-planning-modelling-mistakes-potential-errors-and-benchmark-domains-2023/</link>
      <pubDate>Mon, 13 Nov 2023 22:33:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/pascal-bercher-detecting-ai-planning-modelling-mistakes-potential-errors-and-benchmark-domains-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Detecting Ai Planning Modelling Mistakes Potential Errors and Benchmark Domains&lt;/li&gt;
&lt;li&gt;Author: Pascal Bercher et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Nov 13, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://bercher.net/publications/2023/Sleath2023PossibleModelingErrors.pdf&#34;&gt;https://bercher.net/publications/2023/Sleath2023PossibleModelingErrors.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author provided a compilation of potential modelling errors&lt;/li&gt;
&lt;li&gt;the author supply a public repository of 56 (flawed) benchmark domains&lt;/li&gt;
&lt;li&gt;conducted an evaluation of well-known AI planning tools for their ability to diagnose those errors, showing that not a single tool is able to spot all errors, with no tool being strictly stronger than another.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;list of errors&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yecheng Jason Ma Eureka Human Level Reward Design via Coding Large Language Models 2023</title>
      <link>https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/</link>
      <pubDate>Fri, 27 Oct 2023 16:44:22 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Eureka Human Level Reward Design via Coding Large Language Models 2023&lt;/li&gt;
&lt;li&gt;Author: Yecheng Jason Ma et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 19 Oct 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Oct 27, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2310.12931.pdf&#34;&gt;https://arxiv.org/pdf/2310.12931.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20231027164539472&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;harnessing LLMs to learn complex low-level manipulation tasks, remains an open problem.&lt;/li&gt;
&lt;li&gt;we bridge this fundamental gap by using LLMs to produce rewards that can be used to acquire conplex skill via reinforcement learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Eureka generate reward functions that outperform expert human-engineered rewards.&lt;/li&gt;
&lt;li&gt;the generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF)&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20231030132136067&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yecheng-jason-ma-eureka-human-level-reward-design-via-coding-large-language-models-2023/image-assets/image-20231030132136067.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;given detailed environmental code and natural language description about the task, the LLMs can generate reward function candidate sampling.&lt;/li&gt;
&lt;li&gt;As many real-world RL tasks admit sparse rewards that are difficult for learning, reward shaping that provides incremental learning signals is necessary in practice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;reward design problem&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mark Chen Evaluating Large Language Models Trained on Code 2021</title>
      <link>https://sino-huang.github.io/posts/mark-chen-evaluating-large-language-models-trained-on-code-2021/</link>
      <pubDate>Mon, 16 Oct 2023 07:24:26 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/mark-chen-evaluating-large-language-models-trained-on-code-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Evaluating Large Language Models Trained on Code&lt;/li&gt;
&lt;li&gt;Author: Mark Chen et. al. OPENAI&lt;/li&gt;
&lt;li&gt;Publish Year: 14 Jul 2021&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Oct 16, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2107.03374.pdf&#34;&gt;https://arxiv.org/pdf/2107.03374.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;it is the research paper behind Github Copilot tech&lt;/li&gt;
&lt;li&gt;more recently, language models have also fueled progress towards the longstanding challenge of program synthesis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;limitation&#34;&gt;limitation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;difficulty with docstrings describing long chain of operations and with binding operations to variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;HumanEval&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Baptiste Roziere Code Llama Open Foundation Model for Code 2023</title>
      <link>https://sino-huang.github.io/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/</link>
      <pubDate>Mon, 16 Oct 2023 02:58:20 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Code Llama Open Foundation Model for Code&lt;/li&gt;
&lt;li&gt;Author: Baptiste Roziere et. al. META AI&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Oct 16, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://scontent.fmel13-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&amp;amp;ccb=1-7&amp;amp;_nc_sid=3c67a6&amp;amp;_nc_ohc=Hcg6QsYJx1wAX_okEZO&amp;amp;_nc_ht=scontent.fmel13-1.fna&amp;amp;oh=00_AfAYtfHJfYeomAQWiMUTRo96iP8d4sZrlIfD_KAeYlYaDQ&amp;amp;oe=6531E8CF&#34;&gt;https://scontent.fmel13-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=Hcg6QsYJx1wAX_okEZO&amp;_nc_ht=scontent.fmel13-1.fna&amp;oh=00_AfAYtfHJfYeomAQWiMUTRo96iP8d4sZrlIfD_KAeYlYaDQ&amp;oe=6531E8CF&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20231016025929381&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CODE Llama, support for large input contexts, and zero-shot instruction following ability for programming tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CODE llama reaches SOTA performance among open models on several code benchmarks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20231016032328670&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/baptiste-roziere-code-llama-open-foundation-model-for-code-2023/image-assets/image-20231016032328670.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;By training on domain-specific datasets, LLM have proved effective more broadly on applications that require advanced natural language understanding.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Haotian Liu Improved Baselines With Visual Instruction Tuning 2023</title>
      <link>https://sino-huang.github.io/posts/haotian-liu-improved-baselines-with-visual-instruction-tuning-2023/</link>
      <pubDate>Sun, 08 Oct 2023 10:37:37 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/haotian-liu-improved-baselines-with-visual-instruction-tuning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Improved Baselines With Visual Instruction Tuning&lt;/li&gt;
&lt;li&gt;Author: Haotian Liu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Oct 5 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Oct 8, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2310.03744.pdf&#34;&gt;https://arxiv.org/pdf/2310.03744.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20231008103914399&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/haotian-liu-improved-baselines-with-visual-instruction-tuning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;with simple modifications to LLaVA, namely, using CLIP-ViT with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, they establish stronger baseline.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Improvement one: MLP cross modal connector&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Christabel Wayllace Goal Recognition Design With Stochastic Agent Action Outcomes 2016</title>
      <link>https://sino-huang.github.io/posts/christabel-wayllace-goal-recognition-design-with-stochastic-agent-action-outcomes-2016/</link>
      <pubDate>Fri, 06 Oct 2023 18:16:28 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/christabel-wayllace-goal-recognition-design-with-stochastic-agent-action-outcomes-2016/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Christabel Wayllace Goal Recognition Design With Stochastic Agent Action Outcomes 2016&lt;/li&gt;
&lt;li&gt;Author: Christable Wayllace et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: IJCAI 2016&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Oct 6, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.ijcai.org/Proceedings/16/Papers/464.pdf&#34;&gt;https://www.ijcai.org/Proceedings/16/Papers/464.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, they generalize the Goal Recognition Design (GRD) problem to Stochastic GRD (S-GRD) problems, which handle stochastic action outcomes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Plan and goal recognition problem&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it aims to identify the actual plan or goal of an agent given its behaviour.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Goal Recognition Design&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alba Gragera Pddl Domain Repair Fixing Domains With Incomplete Action Effects 2023</title>
      <link>https://sino-huang.github.io/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/</link>
      <pubDate>Wed, 20 Sep 2023 23:17:51 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: PDDL Domain Repair Fixing Domains With Incomplete Action Effects&lt;/li&gt;
&lt;li&gt;Author: Alba Gragera et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: ICAPS 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Sep 20, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://icaps23.icaps-conference.org/demos/papers/2791_paper.pdf&#34;&gt;https://icaps23.icaps-conference.org/demos/papers/2791_paper.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230920232918668&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, they present a tool to repair planning models where the effects of some actions are incomplete. The received input is compiled to a new extended planning task, in which actions are permitted to insert possible missing effects. The solution is a plan that achieves the goals of the original problem while also alerting users of the modification made.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230920233613949&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alba-gragera-pddl-domain-repair-fixing-domains-with-incomplete-action-effects-2023/image-assets/image-20230920233613949.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alba Gragera Exploring the Limitations of Using LLMs to Fix Planning Tasks 2023</title>
      <link>https://sino-huang.github.io/posts/alba-gragera-exploring-the-limitations-of-using-llms-to-fix-planning-tasks-2023/</link>
      <pubDate>Wed, 20 Sep 2023 20:22:32 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/alba-gragera-exploring-the-limitations-of-using-llms-to-fix-planning-tasks-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Exploring the Limitations of Using LLMs to Fix Planning Tasks&lt;/li&gt;
&lt;li&gt;Author: Alba Gragera et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: icaps23.icaps-conference&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Sep 20, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf&#34;&gt;https://icaps23.icaps-conference.org/program/workshops/keps/KEPS-23_paper_3645.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230920210538214&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alba-gragera-exploring-the-limitations-of-using-llms-to-fix-planning-tasks-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In this work, the authors present ongoing efforts on exploring the limitations of LLMs in task requiring reasoning and planning competences: that of assisting humans in the process of fixing planning tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;investigate how good LLMs are at repairing planning tasks when the prompt is given in PDDL and when it is given in natural language.&lt;/li&gt;
&lt;li&gt;also they tested on incomplete initial state and also incomplete domains which lack a necessary action effect to achieve the goals.&lt;/li&gt;
&lt;li&gt;in all cases, LLMs are used as stand-alone, and they directly assess the correctness of the solutions it generates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;conclusion:&lt;/strong&gt; they demonstrate that although LLMs can in principle facilitate iterative refinement of PDDL models through user interaction, their limited reasoning abilities render them insufficient for identifying meaningful changes to ill-defined planning models that result into solvable planning tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tathagata Chakraborti Plan Explanations as Model Reconciliation 2017</title>
      <link>https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/</link>
      <pubDate>Tue, 19 Sep 2023 22:04:06 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Plan Explanations as Model Reconciliation: Moving beyond explanation as soliloquy&lt;/li&gt;
&lt;li&gt;Author: Tathagata Chakraborti&lt;/li&gt;
&lt;li&gt;Publish Year: 30 May 2017&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Sep 19, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/1701.08317.pdf&#34;&gt;https://arxiv.org/pdf/1701.08317.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230920160252585&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tathagata-chakraborti-plan-explanations-as-model-reconciliation-2017/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Past work on plan explanations primarily involved AI system explaining the correctness of its plan and t he rationale for its decision in terms of its own model. Such soliloquy is inadequate (think about the case where GPT4 cannot find errors in PDDL domain file due to over confidence)&lt;/li&gt;
&lt;li&gt;in this work, the author said that due to the domain and task model difference between human and AI system, the soliloquy is inadequate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They show how explanation can be seen as a &amp;ldquo;model reconciliation problem&amp;rdquo; (MRP), where AI system in effect suggests changes to the human&amp;rsquo;s model, so as to make its plan be optimal with respected to that changed human model. In other words, they need to update human&amp;rsquo;s mindset about the domain and task model such that the plan generated from the AI system fits human&amp;rsquo;s expectation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Definition of a classical planning problem&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vishal Pallagani Plansformer Tool Demonstrating Generation of Symbolic Plans Using Transformers 2023</title>
      <link>https://sino-huang.github.io/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/</link>
      <pubDate>Sat, 16 Sep 2023 00:46:56 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Plansformer &amp;ndash; Tool Demonstrating Generation of Symbolic Plans Using Transformers&lt;/li&gt;
&lt;li&gt;Author: Vishal Pallagani et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: IJCAI-23&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Sep 16, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.ijcai.org/proceedings/2023/0839.pdf&#34;&gt;https://www.ijcai.org/proceedings/2023/0839.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;making a bridge between planning in LLM and planning in traditional automatic planner&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;design-of-plansformer&#34;&gt;Design of Plansformer&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230916142131826&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/image-assets/image-20230916142131826.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;in the evaluation phase, planner testing helps to validate the plan (both the syntax validation and plan optimality validation), model testing helps to force a linguistic consistency (in this case it supervise the semantics).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;function-of-this-plansformer&#34;&gt;Function of this Plansformer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Plansformer operates as an AI planner designed for plan generation,  not for creating PDDLs from natural language descriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230916144140801&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/vishal-pallagani-plansformer-tool-demonstrating-generation-of-symbolic-plans-using-transformers-2023/image-assets/image-20230916144140801.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Junnan_li Blip2 Boostrapping Language Image Pretraining 2023</title>
      <link>https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/</link>
      <pubDate>Mon, 28 Aug 2023 18:48:08 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/junnan_li-blip2-boostrapping-language-image-pretraining-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  BLIP2 - Boostrapping Language Image Pretraining 2023&lt;/li&gt;
&lt;li&gt;Author: Junnan Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 15 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Aug 28, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.12597.pdf&#34;&gt;https://arxiv.org/pdf/2301.12597.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper titled &amp;ldquo;BLIP-2&amp;rdquo; proposes a new and efficient pre-training strategy for vision-and-language models. The cost of training such models has been increasingly prohibitive due to the large scale of the models. BLIP-2 aims to address this issue by leveraging off-the-shelf, pre-trained image encoders and large language models (LLMs) that are kept frozen during the pre-training process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Peng_gao Llama Adapter V2 2023</title>
      <link>https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/</link>
      <pubDate>Mon, 28 Aug 2023 18:47:05 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/peng_gao-llama-adapter-v2-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Llama Adapter V2&lt;/li&gt;
&lt;li&gt;Author: Peng Gao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 28 Apr 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Aug 28, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2304.15010.pdf&#34;&gt;https://arxiv.org/pdf/2304.15010.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper presents LLaMA-Adapter V2, an enhanced version of the original LLaMA-Adapter designed for multi-modal reasoning and instruction following. The paper aims to address the limitations of the original LLaMA-Adapter, which could not generalize well to open-ended visual instructions and lagged behind GPT-4 in performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rodrigo Reward Machines Exploiting Reward Function Structure in Rl 2022</title>
      <link>https://sino-huang.github.io/posts/rodrigo-reward-machines-exploiting-reward-function-structure-in-rl-2022/</link>
      <pubDate>Thu, 17 Aug 2023 16:32:09 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/rodrigo-reward-machines-exploiting-reward-function-structure-in-rl-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning 2022&lt;/li&gt;
&lt;li&gt;Author: Rodrigo Toro Icarte et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022 AI Access Foundation&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Aug 17, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2010.03950&#34;&gt;https://arxiv.org/abs/2010.03950&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in most RL applications, however, users have to program the reward function and hence, there is the opportunity to make the reward function visible and RL  agent can exploit the function&amp;rsquo;s internal structure to learn optimal policies in a more sample efficient manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;different methodology of RL for Reward Machines&lt;/li&gt;
&lt;li&gt;compared to their previous studies, this work tested a collection of RL methods that can exploit a reward machine&amp;rsquo;s internal structure to improve sample efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;counterfactual experiences for reward machines (CRM)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rodrigo Using Reward Machines for High Level Task Specification and Decomposition in Rl 2018</title>
      <link>https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/</link>
      <pubDate>Thu, 17 Aug 2023 11:13:24 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Reward Machines for High Level Task Specification and Decomposition in Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Rodrigo Toro Icarte et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: PMLR 2018&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Aug 17, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf&#34;&gt;http://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230817111402194&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/rodrigo-using-reward-machines-for-high-level-task-specification-and-decomposition-in-rl-2018/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;proposing a reward machine while exposing reward function structure to the learner and supporting decomposition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in contrast to hierarchical RL methods which might converge to suboptimal policies. We prove that QRM is guaranteed to converge to an optimal policy in the tabular case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;intro&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>William_berrios Towards Language Models That Can See 2023</title>
      <link>https://sino-huang.github.io/posts/william_berrios-towards-language-models-that-can-see-2023/</link>
      <pubDate>Mon, 03 Jul 2023 19:33:22 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/william_berrios-towards-language-models-that-can-see-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language&lt;/li&gt;
&lt;li&gt;Author: William Berrios et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 28 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jul 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2306.16410.pdf&#34;&gt;https://arxiv.org/pdf/2306.16410.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230703193548354&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/william_berrios-towards-language-models-that-can-see-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;proposing LENS, a modular approach that addresses computer vision tasks by harnessing the few-shot, in-context learning abilities of language models through natural language descriptions of visual inputs&lt;/li&gt;
&lt;li&gt;LENS enables any off-the-shelf LLM to have visual capabilities without auxiliary training or data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lens-framework&#34;&gt;LENS framework&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230703195306870&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/william_berrios-towards-language-models-that-can-see-2023/image-assets/image-20230703195306870.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lionel_wong From Word Models to World Models 2023</title>
      <link>https://sino-huang.github.io/posts/lionel_wong-from-word-models-to-world-models-2023/</link>
      <pubDate>Sun, 02 Jul 2023 21:24:50 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/lionel_wong-from-word-models-to-world-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought&lt;/li&gt;
&lt;li&gt;Author: Lionel Wong et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 23 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jul 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2306.12672.pdf&#34;&gt;https://arxiv.org/pdf/2306.12672.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;leverage a theory of linguistic meaning to build machines that think in more human-like ways.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we frame linguistic meaning as a context-sensitive mapping from NL into a probabilistic language of thought (PLoT) &amp;ndash; a general-purpose &lt;strong&gt;symbolic substrate for probabilistic, generative world modelling&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jianning_wang Boosting Language Models Reasoning With Chain of Knowledge Prompting 2023</title>
      <link>https://sino-huang.github.io/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/</link>
      <pubDate>Sun, 02 Jul 2023 16:09:58 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Boosting Language Models Reasoning With Chain of Knowledge Prompting&lt;/li&gt;
&lt;li&gt;Author: Jianing Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Jun 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jul 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2306.06427.pdf&#34;&gt;https://arxiv.org/pdf/2306.06427.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230702162535381&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Chain of Thought (CoT)&amp;rdquo; aims at designing a simple prompt like &amp;ldquo;Let&amp;rsquo;s think step by step&amp;rdquo;&lt;/li&gt;
&lt;li&gt;however, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chain&lt;/li&gt;
&lt;li&gt;To mitigate this brittleness, we propose a novel Chain-of-Knowlege knowledge evidence in the form of structure triple&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Benefiting from CoK, we additional introduce a F^2 -Verification method to estimate the reliable response, the wrong evidence can be indicated to prompt the LLM to rethink.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230702174714043&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jianning_wang-boosting-language-models-reasoning-with-chain-of-knowledge-prompting-2023/image-assets/image-20230702174714043.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lin_guan Leveraging Pretrained Llm to Construct and Utilise World Models for Model Based Task Planning 2023</title>
      <link>https://sino-huang.github.io/posts/lin_guan-leveraging-pretrained-llm-to-construct-and-utilise-world-models-for-model-based-task-planning-2023/</link>
      <pubDate>Sun, 04 Jun 2023 12:01:46 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/lin_guan-leveraging-pretrained-llm-to-construct-and-utilise-world-models-for-model-based-task-planning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Leveraging Pretrained Large Language Models to Construct and Utilise World Models for Model Based Task Planning&lt;/li&gt;
&lt;li&gt;Author: Lin Guan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 24 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Jun 4, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.14909.pdf&#34;&gt;https://arxiv.org/pdf/2305.14909.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230604120324310&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/lin_guan-leveraging-pretrained-llm-to-construct-and-utilise-world-models-for-model-based-task-planning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;However, methods that use LLMs directly as planners
are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;introduce a alternative paradigm that construct an explicit world (domain) model in planning domain definition language (PDDL) and then use it to plan with sound domain-independent planners.&lt;/li&gt;
&lt;li&gt;users can correct the PDDL before the real planning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;findings&#34;&gt;Findings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-4 can readily correct all the errors according to natural language feedback from PDDL validators and humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;approach&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dharma_kc Neural Machine Translation for Code Generation 2023</title>
      <link>https://sino-huang.github.io/posts/dharma_kc-neural-machine-translation-for-code-generation-2023/</link>
      <pubDate>Sun, 28 May 2023 09:52:32 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/dharma_kc-neural-machine-translation-for-code-generation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Neural Machine Translation for Code Generation&lt;/li&gt;
&lt;li&gt;Author: Dharma KC et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, May 28, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.13504.pdf&#34;&gt;https://arxiv.org/pdf/2305.13504.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Recently, NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;NMT-based architecture are getting quite popular for source generation from various input. The NMT-based code generation is useful in multiple domains such as code generation from input binary or assembly (decompilation), code-to-code translation, code repair, bug fixing, and many more.&lt;/li&gt;
&lt;li&gt;some open problems
&lt;ul&gt;
&lt;li&gt;source code has long dependencies in multiple places
&lt;ul&gt;
&lt;li&gt;next-token prediction technique may lost the dependency information&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Methods that can break down a problem into small problems, generate code for such subprograms, and evaluate them are good potential research direction&lt;/li&gt;
&lt;li&gt;sample efficiency&lt;/li&gt;
&lt;li&gt;Current code generation does not combine code abstraction to higher-level abstractions as human do.&lt;/li&gt;
&lt;li&gt;Execution-guided synthesis currently works with DSLs, but extending them to real-world source code generation is a research direction.&lt;/li&gt;
&lt;li&gt;Retrieve-and-Edit framework&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Jiannan_xiang Language Models Meet World Models 2023</title>
      <link>https://sino-huang.github.io/posts/jiannan_xiang-language-models-meet-world-models-2023/</link>
      <pubDate>Fri, 26 May 2023 01:00:02 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jiannan_xiang-language-models-meet-world-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language Models Meet World Models: Embodied Experiences Enhance Language Models&lt;/li&gt;
&lt;li&gt;Author: Jiannan Xiang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, May 26, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.10626v2.pdf&#34;&gt;https://arxiv.org/pdf/2305.10626v2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LLM often struggle with simple reasoning and planning in physical environment&lt;/li&gt;
&lt;li&gt;the limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities.&lt;/li&gt;
&lt;li&gt;the experiments in a virtual physical world simulation environment will be used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking etc.&lt;/li&gt;
&lt;li&gt;to preserve the generalisation ability of LM models, we use elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230527163509701&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jiannan_xiang-language-models-meet-world-models-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ryan_yang PG3 Policy Guided Planning for Generalised Policy Generation 2022</title>
      <link>https://sino-huang.github.io/posts/ryan_yang-pg3-policy-guided-planning-for-generalised-policy-generation-2022/</link>
      <pubDate>Wed, 24 May 2023 19:57:16 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/ryan_yang-pg3-policy-guided-planning-for-generalised-policy-generation-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: PG3 Policy Guided Planning for Generalised Policy Generation&lt;/li&gt;
&lt;li&gt;Author: Ryan Yang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 21 Apr 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, May 24, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2204.10420.pdf&#34;&gt;https://arxiv.org/pdf/2204.10420.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230524195832214&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ryan_yang-pg3-policy-guided-planning-for-generalised-policy-generation-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;a longstanding objective in classical planning is to synthesise policies that generalise across multiple problems from the same domain&lt;/li&gt;
&lt;li&gt;this work, we study generalised policy search-based methods with a focus on the score function used to guide the search over policies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we study a specific instantiation of policy search where planning problems are PDDL-based and policies are lifted decision lists.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;what is generalised planning and generalised policy search (GPS)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shunyu_yao Tree of Thoughts 2023</title>
      <link>https://sino-huang.github.io/posts/shunyu_yao-tree-of-thoughts-2023/</link>
      <pubDate>Wed, 24 May 2023 16:35:10 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/shunyu_yao-tree-of-thoughts-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Tree of Thoughts: Deliberate Problem Solving with LLM&lt;/li&gt;
&lt;li&gt;Author: Shunyu Yao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 17 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, May 24, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.10601.pdf&#34;&gt;https://arxiv.org/pdf/2305.10601.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230524163530127&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shunyu_yao-tree-of-thoughts-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions.&lt;/li&gt;
&lt;li&gt;search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt=&#34;image-20230524164011231&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shunyu_yao-tree-of-thoughts-2023/image-assets/image-20230524164011231.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tom_silver Generalised Planning in PDDL Domains With Pretrained Large Language Models 2023</title>
      <link>https://sino-huang.github.io/posts/tom_silver-generalised-planning-in-pddl-domains-with-pretrained-large-language-models-2023/</link>
      <pubDate>Tue, 23 May 2023 21:27:15 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/tom_silver-generalised-planning-in-pddl-domains-with-pretrained-large-language-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Generalised Planning in Pddl Domains With Pretrained Large Language Models&lt;/li&gt;
&lt;li&gt;Author: Tom Silver et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, May 23, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.11014.pdf&#34;&gt;https://arxiv.org/pdf/2305.11014.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in particular, we consider PDDL domains and use GPT-4 to synthesize Python programs,&lt;/li&gt;
&lt;li&gt;we also consider Chain of Thought (CoT) summarisation, where the LLM is prompted to summarize the domain and propose a strategy in words before synthesizing the program&lt;/li&gt;
&lt;li&gt;we consider automated debugging, where the program is validated with respect to the training tasks, and in case of errors, the LLM is re-prompted with four types of feedback.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we find that GPT4 is a surprisingly powerful generalised planner.&lt;/li&gt;
&lt;li&gt;we also conclude that automated debugging is very important, that CoT summarisation has non-uniform impact, that GPT4 is far superior to GPT3.5, and that just two training tasks are often sufficient for strong generalisation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;the problem&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yongliang Hugginggpt 2023</title>
      <link>https://sino-huang.github.io/posts/yongliang-hugginggpt-2023/</link>
      <pubDate>Tue, 23 May 2023 11:57:02 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/yongliang-hugginggpt-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: HuggingGPT: Solving AI tasks with ChatGPT and its Friends in Hugging Face&lt;/li&gt;
&lt;li&gt;Author: Yongliang Shen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2 Apr 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, May 23, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2303.17580.pdf&#34;&gt;https://arxiv.org/pdf/2303.17580.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;while there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks.&lt;/li&gt;
&lt;li&gt;we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging face, execute each subtask with the selected AI model, and summarize the response according to the execution results.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yaqi_xie Translating Natural Language to Planning Goals With Llm 2023</title>
      <link>https://sino-huang.github.io/posts/yaqi_xie-translating-natural-language-to-planning-goals-with-llm-2023/</link>
      <pubDate>Mon, 22 May 2023 12:30:25 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/yaqi_xie-translating-natural-language-to-planning-goals-with-llm-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Translating Natural Language to Planning Goals With LLM&lt;/li&gt;
&lt;li&gt;Author: Yaqi Xie et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 22, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.05128.pdf&#34;&gt;https://arxiv.org/pdf/2302.05128.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problem&lt;/li&gt;
&lt;li&gt;LLM can act as a natural interface between the planner and human users&lt;/li&gt;
&lt;li&gt;Our empirical results on GPT 3.5 variants show that LLMs are much
better suited towards translation rather than planning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We find that LLMs are able to leverage commonsense knowledge and
reasoning to furnish missing details from under-specified goals (as
is often the case in natural language)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bo_liu Llmp Empowering Large Language Models With Optimal Planning Proficiency 2023</title>
      <link>https://sino-huang.github.io/posts/bo_liu-llmp-empowering-large-language-models-with-optimal-planning-proficiency-2023/</link>
      <pubDate>Mon, 22 May 2023 11:56:15 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/bo_liu-llmp-empowering-large-language-models-with-optimal-planning-proficiency-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: LLM+P Empowering Large Language Models With Optimal Planning Proficiency&lt;/li&gt;
&lt;li&gt;Author: Bo Liu&lt;/li&gt;
&lt;li&gt;Publish Year: 5 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 22, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2304.11477.pdf&#34;&gt;https://arxiv.org/pdf/2304.11477.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal plans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;introduce LLM+P, it takes in a natural language description of a planning problem, then return a correct plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;limitation of the paper&lt;/strong&gt;: In this paper, we do not ask the LLM to recognize that it has been posed a prompt that is suitable for processing using the proposed LLM+P pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of LLMs&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Siyu_yuan Distilling Script Knowledge From Large Language Models for Constrainted Language Planning 2023</title>
      <link>https://sino-huang.github.io/posts/siyu_yuan-distilling-script-knowledge-from-large-language-models-for-constrainted-language-planning-2023/</link>
      <pubDate>Mon, 22 May 2023 11:31:39 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/siyu_yuan-distilling-script-knowledge-from-large-language-models-for-constrainted-language-planning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Distilling Script Knowledge From Large Language Models for Constrainted Language Planning&lt;/li&gt;
&lt;li&gt;Author: Siyu Yuan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 22, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.05252.pdf&#34;&gt;https://arxiv.org/pdf/2305.05252.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;to accomplish everyday goals, human usually plan their actions in accordance with step-by-step instructions, such instruction are discovered as goal-oriented scripts.&lt;/li&gt;
&lt;li&gt;In this paper, we define the task of constrained language planning for
the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, &lt;em&gt;CoScript&lt;/em&gt;, which consists of 55,000
scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the dataset&lt;/li&gt;
&lt;li&gt;Experiments show that, when trained on CoScript, smaller models such as T5 (Raffel et al., 2020) can achieve good performance, even surpassing that of LLMs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of previous work&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Junnan_li BLIP Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation 2022</title>
      <link>https://sino-huang.github.io/posts/junnan_li-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-understanding-and-generation-2022/</link>
      <pubDate>Mon, 22 May 2023 11:17:28 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/junnan_li-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-understanding-and-generation-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  BLIP Bootstrapping Language Image Pre Training for Unified Vision Language Understanding and Generation 2022&lt;/li&gt;
&lt;li&gt;Author: Junnan Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 15 Feb 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 22, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2201.12086.pdf&#34;&gt;https://arxiv.org/pdf/2201.12086.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BLIP effectively utilises the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Harsh_jhamtani Natural Language Decomposition and Interpretation of Complex Utterances 2023</title>
      <link>https://sino-huang.github.io/posts/harsh_jhamtani-natural-language-decomposition-and-interpretation-of-complex-utterances-2023/</link>
      <pubDate>Mon, 22 May 2023 09:54:04 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/harsh_jhamtani-natural-language-decomposition-and-interpretation-of-complex-utterances-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Natural Language Decomposition and Interpretation of Complex Utterances&lt;/li&gt;
&lt;li&gt;Author: Jacob Andreas&lt;/li&gt;
&lt;li&gt;Publish Year: 15 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 22, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.08677.pdf&#34;&gt;https://arxiv.org/pdf/2305.08677.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230522105001304&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/harsh_jhamtani-natural-language-decomposition-and-interpretation-of-complex-utterances-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;natural language interface often require supervised data to translate user request into structure intent representations&lt;/li&gt;
&lt;li&gt;however, during data collection, it can be difficult to anticipate and formalise the full range of user needs&lt;/li&gt;
&lt;li&gt;we introduce an approach for equipping a simple language to code model to handle complex utterances via a process of hierarchical natural language decomposition.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Experiments show that the proposed approach enables the interpretation of complex utterances with almost no complex training data, while outperforming standard few-shot prompting approaches.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alexander_kirillov Segment Anything 2023</title>
      <link>https://sino-huang.github.io/posts/alexander_kirillov-segment-anything-2023/</link>
      <pubDate>Sun, 21 May 2023 11:56:54 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/alexander_kirillov-segment-anything-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Segment Anything&lt;/li&gt;
&lt;li&gt;Author: Alexander Kirillov et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 5 Apr 2023&lt;/li&gt;
&lt;li&gt;Review Date: Sun, May 21, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2304.02643.pdf&#34;&gt;https://arxiv.org/pdf/2304.02643.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230521115752020&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alexander_kirillov-segment-anything-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we introduce the segment anything project: a new task, model and dataset for image segmentation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using the model in a data collection loop, we built the largest segmentation dataset to date.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the model is designed and trained to be promptable, so it can transfer zero-shot to new images distributions and tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;background&#34;&gt;background&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CLIP and ALIGN use contrastive learning to train text and image encoders that align the two modalities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;goal of the authors&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rohit_gridhar Imagebind One Embedding Space to Bind Them All 2023</title>
      <link>https://sino-huang.github.io/posts/rohit_gridhar-imagebind-one-embedding-space-to-bind-them-all-2023/</link>
      <pubDate>Mon, 15 May 2023 15:06:48 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/rohit_gridhar-imagebind-one-embedding-space-to-bind-them-all-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: ImageBind One Embedding Space to Bind Them All&lt;/li&gt;
&lt;li&gt;Author: Rohit Girdhar et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 9 May 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, May 15, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2305.05665.pdf&#34;&gt;https://arxiv.org/pdf/2305.05665.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we present ImageBind, an approach to learn a joint embedding across six different modalities&lt;/li&gt;
&lt;li&gt;ImageBind can leverage recent large scale vision-language models, and extend their zero shot capabilities to new modalities just using their natural pairing with images.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;multimodality binding&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Qinghao_hitea Hierarchical Temporal Aware Video Language Pre Training 2022</title>
      <link>https://sino-huang.github.io/posts/qinghao_hitea-hierarchical-temporal-aware-video-language-pre-training-2022/</link>
      <pubDate>Thu, 06 Apr 2023 10:02:22 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/qinghao_hitea-hierarchical-temporal-aware-video-language-pre-training-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Hierarchical Temporal Aware Video Language Pre Training&lt;/li&gt;
&lt;li&gt;Author: Qinghao Ye, Fei Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 30 Dec 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Apr 6, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2212.14546.pdf&#34;&gt;https://arxiv.org/pdf/2212.14546.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230406100437893&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/qinghao_hitea-hierarchical-temporal-aware-video-language-pre-training-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;most previous methods directly inherit or adapt typical image-language pre-training paradigms to video-language pretraining, thus not fully exploiting the unique characteristic of video, i.e., temporal.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this paper, the two novel pretraining tasks for modeling cross-modal alignment between moments and texts as well as the temporal relations of video-text pairs.&lt;/li&gt;
&lt;li&gt;specifically, we propose a cross-modal moment exploration task to explore moments in videos, which results in detailed video moment representations&lt;/li&gt;
&lt;li&gt;besides, the inherent temporal relations are capture by alignment video-text pairs as a whole in different time resolutions with multimodal temporal relation exploration tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of previous work&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacob_andreas Guiding Pretraining in Reinforcement Learning With Llms 2023</title>
      <link>https://sino-huang.github.io/posts/jacob_andreas-guiding-pretraining-in-reinforcement-learning-with-llms-2023/</link>
      <pubDate>Wed, 05 Apr 2023 10:02:24 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/jacob_andreas-guiding-pretraining-in-reinforcement-learning-with-llms-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Guiding Pretraining in Reinforcement Learning With Large Language Models&lt;/li&gt;
&lt;li&gt;Author: Yuqing De, Jacob Andreas et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 13 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Apr 5, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.06692.pdf&#34;&gt;https://arxiv.org/pdf/2302.06692.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230405100408131&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jacob_andreas-guiding-pretraining-in-reinforcement-learning-with-llms-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;intrinstically motivated exploration methods address sparse reward problem by rewarding agents for visiting novel states or transitions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we describe a method that uses background knowledge from text corpora to shape exploration.&lt;/li&gt;
&lt;li&gt;This method, call Exploring with LLMs, reward an agent for achieving goals suggested by a language model prompted with a description of agent&amp;rsquo;s current state.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How does ELLM work&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Luke_zettlemoyer Scaling Expert Language Models With Unsupervised Domain Discovery 2023</title>
      <link>https://sino-huang.github.io/posts/luke_zettlemoyer-scaling-expert-language-models-with-unsupervised-domain-discovery-2023/</link>
      <pubDate>Mon, 03 Apr 2023 15:25:01 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/luke_zettlemoyer-scaling-expert-language-models-with-unsupervised-domain-discovery-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Scaling Expert Language Models With Unsupervised Domain Discovery&lt;/li&gt;
&lt;li&gt;Author: Luke Zettlemoyer et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 24 Mar, 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Apr 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2303.14177.pdf&#34;&gt;https://arxiv.org/pdf/2303.14177.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230403152538135&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/luke_zettlemoyer-scaling-expert-language-models-with-unsupervised-domain-discovery-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we introduce a simple but efficient method to asynchronously train large, sparse language models on arbitrary text corpora.&lt;/li&gt;
&lt;li&gt;Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.&lt;/li&gt;
&lt;li&gt;This approach generalise embarrassingly parallel training by automatically discovering the domain for each expert, and eliminates nearly all the communication overhead of existing sparse language models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Cluster-Branch-Train-Merge (C-BTM)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xuanting_chen How Robust Is GPT 3.5 to Predecessors a Comprehensive Study on Language Understanding Tasks</title>
      <link>https://sino-huang.github.io/posts/xuanting_chen-how-robust-is-gpt35-to-predecessors-a-comprehensive-study-on-language-understanding-tasks/</link>
      <pubDate>Mon, 03 Apr 2023 15:00:57 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/xuanting_chen-how-robust-is-gpt35-to-predecessors-a-comprehensive-study-on-language-understanding-tasks/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: How Robust Is GPT 3.5 to Predecessors a Comprehensive Study on Language Understanding Tasks&lt;/li&gt;
&lt;li&gt;Author: Xuanting Chen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Apr 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/ftp/arxiv/papers/2303/2303.00293.pdf&#34;&gt;https://arxiv.org/ftp/arxiv/papers/2303/2303.00293.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT3.5, their robustness, and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our study yielded the following findings by comparing GPT 3.5 with finetuned models&lt;/li&gt;
&lt;li&gt;competitive results on test sets: GPT3.5 achieves SOTA results in some NLU tasks compared to supervised models fine-tuned with task-specific data. In particular GPT-3.5 performs well in reading comprehension and sentiment analysis tasks, but face challenges in sequence tagging and relation extraction tasks.&lt;/li&gt;
&lt;li&gt;Lack of robustness: GPT-3.5 still encounter significant robustness degradation, such as its average performance dropping by up to 35.74% and 43.59% in natural language inference and sentiment analysis tasks, respectively. However, it is worth noting that GPT3.5 achieves remarkable robustness on certain tasks, such as reading comprehension and WSC tasks&lt;/li&gt;
&lt;li&gt;Robustness instability:  In few-shot scenarios, GPT-3.5’s robustness improvement varies greatly across different tasks. For example, GPT-3.5 shows significant improvement in aspect-based sentiment analysis tasks while the robustness actually decreases in natural language inference (Section 4.3.1) and semantic matching (Section 4.3.2) tasks.&lt;/li&gt;
&lt;li&gt;Prompt sensitivity: changes in input prompts have a significant impact on the results, and GPT-3.5&amp;rsquo;s robustness to prompt variations. still requires improvement.&lt;/li&gt;
&lt;li&gt;Number sensitivity: GPT3.5 is more sensitive to numerical inputs than pre-training fine-tuning models. For example, in the NumWord transformation, which involves replacing numerical words in sentences with different numerical values, GPT3.5 exhibits a significantly high level of sensitivity.&lt;/li&gt;
&lt;li&gt;Task labels sensitivity: we speculate that the task construction during the instruction tuning stage may significantly impact the model&amp;rsquo;s performance. In the case of IMDB binary sentiment classification dataset, the model outputs a large number of &amp;ldquo;neutral&amp;rdquo; responses, which are not included in the application label space, resulting in a performance drop&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Significant improvement in zero/few-shot scenarios:&lt;/strong&gt; in zero-shot and few-shot scenario, GPT3.5 outperforms existing LLMs in most NLU tasks, especially in reading comprehension, natural language inference and semantic matching tasks&lt;/li&gt;
&lt;li&gt;Ability for in-context learning: Compared to 0-shot, GPT 3.5 performs better on most tasks in the 1-shot setting. Additionally, performance does no vary significantly between the 1-shot, 3-shot, 6-shot, 9-shot settings for most tasks. However, providing additional examples in the prompts
can be advantageous for sequence tagging tasks&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Anthony_liu a Picture Is Worth a Thousand Words Language Models Plan From Pixels 2023</title>
      <link>https://sino-huang.github.io/posts/anthony_liu-a-picture-is-worth-a-thousand-words-language-models-plan-from-pixels-2023/</link>
      <pubDate>Mon, 03 Apr 2023 11:28:43 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/anthony_liu-a-picture-is-worth-a-thousand-words-language-models-plan-from-pixels-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: A Picture Is Worth a Thousand Words Language Models Plan From Pixels&lt;/li&gt;
&lt;li&gt;Author: Anthony Liu et.al.&lt;/li&gt;
&lt;li&gt;Publish Year: 16 Mar 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Apr 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2303.09031v1.pdf&#34;&gt;https://arxiv.org/pdf/2303.09031v1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230403112936880&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/anthony_liu-a-picture-is-worth-a-thousand-words-language-models-plan-from-pixels-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;planning is a important capability of AI that perform long-horizon tasks in real-world environments.&lt;/li&gt;
&lt;li&gt;prior PLM based approaches for planning either assume observations are available in the form of text, reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;why we need the ability to reason about plans&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wenlong_huang Grounded Decoding Guiding Text Generation With Grounded Models for Robot Control 2023</title>
      <link>https://sino-huang.github.io/posts/wenlong_huang-grounded-decoding-guiding-text-generation-with-grounded-models-for-robot-control-2023/</link>
      <pubDate>Thu, 30 Mar 2023 23:45:18 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/wenlong_huang-grounded-decoding-guiding-text-generation-with-grounded-models-for-robot-control-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Grounded Decoding Guiding Text Generation With Grounded Models for Robot Control&lt;/li&gt;
&lt;li&gt;Author: WenLong Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Mar, 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 30, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2303.00855&#34;&gt;https://arxiv.org/abs/2303.00855&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230331172240565&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/wenlong_huang-grounded-decoding-guiding-text-generation-with-grounded-models-for-robot-control-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unfortunately, applying LLMs to settings with embodied agents, such as robots, is challenging due to their lack of experience with the physical world, inability to parse non-language observations, and ignorance of rewards or safety constraints that robots may require.&lt;/li&gt;
&lt;li&gt;on the other hand, language-conditioned robotic policies that learn from interaction data can provide the necessary grounding that allows the agent to be correctly situated in the real world, but such policies are limited by the lack of high-level semantic understanding due to the limited breadth of the interaction data available for training them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;thus if we want to make use of the semantic knowledge in a language model while still situating it in an embodied setting, we must construct an action sequence that is both likely according to the language model and also realisable according to grounded models of the environment.&lt;/li&gt;
&lt;li&gt;we frame this as a problem similar to probabilistic filtering: decode a sequence that both has high probability under the language model and high probability under a set of grounded model objectives.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;potential-future-work&#34;&gt;Potential future work&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the work is related to using LMs info as a prior bias&lt;/li&gt;
&lt;li&gt;the problem framing is straightforward&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Mariana_learning Generative Models With Goal Conditioned Reinforcement Learning 2023</title>
      <link>https://sino-huang.github.io/posts/mariana_learning-generative-models-with-goal-conditioned-reinforcement-learning-2023/</link>
      <pubDate>Thu, 30 Mar 2023 21:20:31 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/mariana_learning-generative-models-with-goal-conditioned-reinforcement-learning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Learning Generative Models With Goal Conditioned Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Mariana Vargas Vieyra et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 26 Mar 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 30, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2303.14811&#34;&gt;https://arxiv.org/abs/2303.14811&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230330212401597&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/mariana_learning-generative-models-with-goal-conditioned-reinforcement-learning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we present a novel framework for learning generative models with goal-conditioned reinforcement learning&lt;/li&gt;
&lt;li&gt;we define two agents, a goal conditioned agent (GC-agent) and a supervised agent (S-agent)&lt;/li&gt;
&lt;li&gt;Given a user-input initial state, the GC-agent learns to reconstruct the training set. In this context, elements in the training set are the goals.
&lt;ul&gt;
&lt;li&gt;during training, the S-agent learns to imitate the GC-agent while remaining agnostic of the goals&lt;/li&gt;
&lt;li&gt;At inference we generate new samples with S-agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal-Conditioned Reinforcement Learning (GCRL) framework&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Itsugun_cho Deep Rl With Hierarchical Action Exploration for Dialogue Generation 2023</title>
      <link>https://sino-huang.github.io/posts/itsugun_cho-deep-rl-with-hierarchical-action-exploration-for-dialogue-generation-2023/</link>
      <pubDate>Thu, 30 Mar 2023 15:01:16 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/itsugun_cho-deep-rl-with-hierarchical-action-exploration-for-dialogue-generation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Deep RL With Hierarchical Action Exploration for Dialogue Generation&lt;/li&gt;
&lt;li&gt;Author: Itsugun Cho et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Mar 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 30, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2303.13465v1.pdf&#34;&gt;https://arxiv.org/pdf/2303.13465v1.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental.&lt;/li&gt;
&lt;li&gt;we introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene the sampling.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of the maximum likelihood estimation (MLE) objective for the probability distribution of responses&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Theodore_r_sumers How to Talk So Ai Will Learn 2022</title>
      <link>https://sino-huang.github.io/posts/theodore_r_sumers-how-to-talk-so-ai-will-learn-2022/</link>
      <pubDate>Wed, 15 Mar 2023 21:09:32 +0800</pubDate>
      <guid>https://sino-huang.github.io/posts/theodore_r_sumers-how-to-talk-so-ai-will-learn-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: How to talk so AI will learn: Instructions, descriptions, and autonomy&lt;/li&gt;
&lt;li&gt;Author: Theodore R. Sumers et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: NeurIPS 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Mar 15, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2206.07870.pdf&#34;&gt;https://arxiv.org/pdf/2206.07870.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230315211204247&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/theodore_r_sumers-how-to-talk-so-ai-will-learn-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;yet today, we lack computational models explaining such language use&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To address this challenge, we formalise learning from language in a contextual bandit setting and ask how a human might communicate preferences over behaviours.
&lt;ul&gt;
&lt;li&gt;(obtain intent (preference) from the presentation (behaviour))&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we show that instructions are better in low-autonomy settings, but descriptions are better when the agent will need to act independently.&lt;/li&gt;
&lt;li&gt;We then define a pragmatic listener agent that robustly infers the speaker&amp;rsquo;s reward function by reasoning how the speaker expresses themselves. (language reward module?)&lt;/li&gt;
&lt;li&gt;we hope these insights facilitate a shift from developing agents that obey language to agents that learn from it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;two distinct types of language&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cheng_chi Diffusion Policy Visuomotor Policy Learning via Action Diffusion 2023</title>
      <link>https://sino-huang.github.io/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/</link>
      <pubDate>Thu, 09 Mar 2023 19:36:17 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Diffusion Policy Visuomotor Policy Learning via Action Diffusion&lt;/li&gt;
&lt;li&gt;Author: Cheng Chi et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 9, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf&#34;&gt;https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230309193732709&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/cheng_chi-diffusion-policy-visuomotor-policy-learning-via-action-diffusion-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;introducing a new form of robot visuomotor policy that generates behaviour via a &amp;ldquo;conditional denoising diffusion process&amp;rdquo; on robot action space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Explicit policy&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learning this is like imitation learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Implicit policy&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;aiming to minimise the estimation of the energy function&lt;/li&gt;
&lt;li&gt;learning this is like a standard reinforcement learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;diffusion policy&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alan_lindsay Framer Planning Models From Natural Language Action Descriptions 2017</title>
      <link>https://sino-huang.github.io/posts/alan_lindsay-framer-planning-models-from-natural-language-action-descriptions-2017/</link>
      <pubDate>Thu, 09 Mar 2023 19:28:47 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alan_lindsay-framer-planning-models-from-natural-language-action-descriptions-2017/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Framer: Planning Models From Natural Language Action Descriptions&lt;/li&gt;
&lt;li&gt;Author: Alan Lindsay et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2017&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 9, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://core.ac.uk/download/pdf/322329049.pdf&#34;&gt;https://core.ac.uk/download/pdf/322329049.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230309192920549&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alan_lindsay-framer-planning-models-from-natural-language-action-descriptions-2017/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;for modelling assisting and model generation tools, there is a underlying assumption that the user can formulate the problem using some formal language.&lt;/li&gt;
&lt;li&gt;this motivates us to generate planning domain models directly from NL descriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;approach&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;we start from NL descriptions of actions and use NL analysis to construct structured representation, from which we construct formal representations of action sequences
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;? only action sequence? what about the environment&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;the generated action sequence provide the necessary structured input for inducing a PDDL domain, using domain model acquisition technology.&lt;/li&gt;
&lt;li&gt;we use an estimate of functional similarity, so sentences that describe similar behaviour are represented by the same planning operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;problem modelling&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Siddharth_karamcheti Language Driven Representation Learning for Robotics 2023</title>
      <link>https://sino-huang.github.io/posts/siddharth_karamcheti-language-driven-representation-learning-for-robotics-2023/</link>
      <pubDate>Fri, 03 Mar 2023 16:16:19 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/siddharth_karamcheti-language-driven-representation-learning-for-robotics-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language-Driven Representation Learning for Robotics&lt;/li&gt;
&lt;li&gt;Author: Siddharth Karamcheti et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 24 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Mar 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.12766.pdf&#34;&gt;https://arxiv.org/pdf/2302.12766.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks.&lt;/li&gt;
&lt;li&gt;leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control&lt;/li&gt;
&lt;li&gt;but robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration amongst others.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;first, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite (i.e., high-level semantics)&lt;/li&gt;
&lt;li&gt;We then introduce Voltron, a framework for language driven representation learning from human videos and associated captions.
&lt;ul&gt;
&lt;li&gt;Voltron trades off language conditioned visual reconstruction to learn low-level visual patterns (mask auto-encoding) and visually grounded language generation to encode high-level semantics. (hindsight relabelling and contrastive learning)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;How can we learn visual representations that generalise across the diverse spectrum of problems in robot learning?&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tatsuki_kuribayashi Does Vision Accelerate Hierarchical Generalisation of Neural Language Learners 2023</title>
      <link>https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/</link>
      <pubDate>Fri, 03 Mar 2023 15:26:55 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Does Vision Accelerate Hierarchical Generalisation of Neural Language Learners&lt;/li&gt;
&lt;li&gt;Author: Tatsuki Kuribayashi&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Mar 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.00667.pdf&#34;&gt;https://arxiv.org/pdf/2302.00667.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we want to know if the visual information improves hierarchical generalisaiton of the language model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303153510788&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153510788.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303153540288&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153540288.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303153621365&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tatsuki_kuribayashi-does-vision-accelerate-hierarchical-generalisation-of-neural-language-learners-2023/image-assets/image-20230303153621365.png&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;our results have exhibited that vision accelerated a proper linguistic generlisation in the simplified, artificial setting,&lt;/li&gt;
&lt;li&gt;but LMs struggled with the proper generalisation in the noisy, realistic setting. These mixed results have indicated several possibilities; for example, an image can potentially boost language acquisition, but learners&amp;rsquo; additional visual/linguistic **&lt;u&gt;prior knowledge should be needed t&lt;/u&gt;**o robustly make use of raw images for efficient language acquisition.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Jing_cheng_pang Natural Language Conditioned Reinforcement Learning With Inside Out Task Language Development and Translation 2023</title>
      <link>https://sino-huang.github.io/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/</link>
      <pubDate>Fri, 03 Mar 2023 15:19:43 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Jing_cheng_pang Natural Language Conditioned Reinforcement Learning With Inside Out Task Language Development and Translation 2023&lt;/li&gt;
&lt;li&gt;Author: Jing-Cheng Pang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 18 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Mar 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.09368.pdf&#34;&gt;https://arxiv.org/pdf/2302.09368.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303152538759&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;previous approaches generally implemented language-conditioned RL by providing human instructions in natural language and training a following policy
&lt;ul&gt;
&lt;li&gt;this is outside-in approach&lt;/li&gt;
&lt;li&gt;the policy needs to comprehend the NL and manage the task simultaneously.&lt;/li&gt;
&lt;li&gt;However, the unbounded NL examples often bring much extra complexity for solving concrete RL tasks, which can distract policy learning from completing the task&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we investigate an inside-out scheme for natural language-conditioned RL by developing a task language (TL) that is task-related and unique. The TL is used in RL to achieve high effective policy training.&lt;/li&gt;
&lt;li&gt;besides, a translator is trained to translate NL into TL.&lt;/li&gt;
&lt;li&gt;experiments indicate that the new model not only better comprehends NL instructions but also leads to better instruction following policy that improves 13.4% success rate and adapts to unseen expressions of NL instruction.&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20230303161350807&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jing_cheng_pang-natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation-2023/image-assets/image-20230303161350807.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Suvaansh_bhambri Multi Level Compositional Reasoning for Interactive Instruction Following 2023</title>
      <link>https://sino-huang.github.io/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/</link>
      <pubDate>Fri, 03 Mar 2023 11:17:01 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Multi-Level Compositional Reasoning for Interactive Instruction Following&lt;/li&gt;
&lt;li&gt;Author: Suvaansh Bhambri et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Mar 3, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf&#34;&gt;https://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303112210161&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The task given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction.&lt;/li&gt;
&lt;li&gt;at the highest level, we infer a sequence of human-interpreatable subgoals to be executed based on the language instructions by a high-level policy composition controller.&lt;/li&gt;
&lt;li&gt;at the middle level, we discriminatively control the agent&amp;rsquo;s navigation by a master policy by alternating between a navigation policy and various independent interaction policies.&lt;/li&gt;
&lt;li&gt;finally, at the lowest level, we infer manipulation actions with the corresponding object masks using appropriate interaction policy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230303143459781&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/suvaansh_bhambri-multi-level-compositional-reasoning-for-interactive-instruction-following-2023/image-assets/image-20230303143459781.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tianjun_zhang the Wisdom of Hindsight Makes Language Models Better Instruction Followers 2023</title>
      <link>https://sino-huang.github.io/posts/tianjun_zhang-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-2023/</link>
      <pubDate>Thu, 02 Mar 2023 19:06:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tianjun_zhang-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: The Wisdom of Hindsight Makes Language Models Better Instruction Followers&lt;/li&gt;
&lt;li&gt;Author: Tianjun Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.05206.pdf&#34;&gt;https://arxiv.org/pdf/2302.05206.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230302190916037&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tianjun_zhang-the-wisdom-of-hindsight-makes-language-models-better-instruction-followers-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Reinforcement learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the pipeline for reward and value networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner.&lt;/li&gt;
&lt;li&gt;Such an algorithm doesn&amp;rsquo;t require any additional parameters except for the original language model and maximally reuses the pretraining pipeline.&lt;/li&gt;
&lt;li&gt;To achieve this, we formulate instruction alignment problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for alignment language models with instructions.&lt;/li&gt;
&lt;li&gt;The resulting two-stage algorithm shed light to a family of reward-free approaches that utilise the hindsightly relabeled instructions based on feedback.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;fine-tuning language model&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ying_shen Learning by Asking for Embodied Visual Navigation and Task Completion 2023</title>
      <link>https://sino-huang.github.io/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/</link>
      <pubDate>Thu, 02 Mar 2023 17:51:02 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Learning by Asking for Embodied Visual Navigation and Task Completion&lt;/li&gt;
&lt;li&gt;Author: Ying Shen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 9 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.04865.pdf&#34;&gt;https://arxiv.org/pdf/2302.04865.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230302175239223&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;despite recent progress on related vision-language benchmarks, most prior work has focused on building agents that follow instructions rather than endowing agents the ability to ask questions to actively resolve ambiguities arising naturally in embodied environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230302175434615&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ying_shen-learning-by-asking-for-embodied-visual-navigation-and-task-completion-2023/image-assets/image-20230302175434615.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ernest_davis Benchmarks for Automated Commonsense Reasoning a Survey 2023</title>
      <link>https://sino-huang.github.io/posts/ernest_davis-benchmarks-for-automated-commonsense-reasoning-a-survey-2023/</link>
      <pubDate>Thu, 02 Mar 2023 15:22:51 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ernest_davis-benchmarks-for-automated-commonsense-reasoning-a-survey-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Benchmarks for Automated Commonsense Reasoning a Survey&lt;/li&gt;
&lt;li&gt;Author: Ernest Davis&lt;/li&gt;
&lt;li&gt;Publish Year: 9 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Mar 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.04752.pdf&#34;&gt;https://arxiv.org/pdf/2302.04752.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;we mainly focus on the section where the author discusses about features of commonsense reasoning generally.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;terms&#34;&gt;Terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;clarify what we mean by common sense&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what is exactly &amp;ldquo;commonsensical&amp;rdquo;?&lt;/li&gt;
&lt;li&gt;Claims about common sense that seem true to the author
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Commonsense knowledge is common.&lt;/strong&gt; In talking to other person, we do not have to explain common sense reasoning or enumerate common sense facts. We can assume that they know that unsupported things fall down, that outside the tropics, days in temperate regions are generally warmer than winter, and so on.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common sense is largely sensible&lt;/strong&gt;. Any individual person or even an entire society may have various foolish or mistaken beliefs, but for the most part common sense knowledge correponds to the realities of the world as people experience it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Common sense supports reasoning&lt;/strong&gt;. For example a person who knows that Central Park is in New York and the Golden Gate Bridge is in San Francisco and that New York and San Francisco are 3000 miles apart will realize that they cannot walk from one to the other in fifteen minutes.&lt;/li&gt;
&lt;li&gt;commonsense reasoning is integrated with other cognitive abilities&lt;/li&gt;
&lt;li&gt;Common sense extends across tasks and modalities&lt;/li&gt;
&lt;li&gt;Common sense is a broad scope&lt;/li&gt;
&lt;li&gt;Commonsense knowledge can be distinguished from common knowledge, encyclopaedic knowledge and expert knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Half-truths about commonsense knowledge
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Commonsense knowledge is language-independent&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The English-language bias is as pervasive in commmonsense reasoning as in other areas of AI. Impressively, versions of ConceptNet with at least 10,000 concepts exist in 83 different languages, and a few commonsense benchmarks have been translated (table 4) but most
resources and benchmarks only exist in English or in a symbolic form in which the symbols are in fact English words or short phrases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commonsense knowledge is the same for people of different cultures and of different historical periods&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Even if a belief has been commonsense knowledge for everyone at all times up to the present, that does not mean that that will continue in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commonsense reasoning is fast and intuitive; it falls within &amp;ldquo;System 1&amp;rdquo;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Processes in System 1 characteristically are executed quickly, do not require conscious thought, are not open to introspection, in at least in some cases are not controllable (one cannot decide not to interpret what one is seeing), and do not place a cognitive burden on working memory; vision is a paradigmatic example. Processes in System 2 are the reverse: slow, consciously carried out, consciously controllable, instrospectable, and taxing on working memory. System 2 processes can call on system 1 but not vice versa, since a fast process cannot use a slow subroutine.&lt;/li&gt;
&lt;li&gt;encyclopaedic and expert knowledge can also be called on in System 1 activities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commonsense knowledge can be expressed using simple language&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;it seems plausible: basic vocabulary tends to refer to the well-known concepts and relations which are the subject of commonsense knowledge&lt;/li&gt;
&lt;li&gt;however, there is a very large exception here, which is commonsense spatial knowledge. Natural language is notoriously ill-suited to the description of characteristics of shapes and positions that are easily apprehended (bad expressivity of natural language)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;An untrue claim about commonsense knowledge
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;u&gt;commonsense knowledge is not logically complex&lt;/u&gt;&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;However, in physical reasoning, understanding the physical characteristics could be quite complex (e.g., considering angry birds). But humans are good at playing angry birds.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Alexander_nikulin Anti Exploration by Random Network Distillation 2023</title>
      <link>https://sino-huang.github.io/posts/alexander_nikulin-anti-exploration-by-random-network-distillation-2023/</link>
      <pubDate>Wed, 01 Mar 2023 22:14:11 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alexander_nikulin-anti-exploration-by-random-network-distillation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Anti Exploration by Random Network Distillation&lt;/li&gt;
&lt;li&gt;Author: Alexander Nikulin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 31 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Mar 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.13616.pdf&#34;&gt;https://arxiv.org/pdf/2301.13616.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230301221745373&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alexander_nikulin-anti-exploration-by-random-network-distillation-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;?? wait, why we want to penalizing out-of-distribution actions?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;With a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue.&lt;/li&gt;
&lt;li&gt;We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;why we want uncertainty-based penalization&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Edoardo_cetin Learning Pessimism for Reinforcement Learning 2023</title>
      <link>https://sino-huang.github.io/posts/edoardo_cetin-learning-pessimism-for-reinforcement-learning-2023/</link>
      <pubDate>Wed, 01 Mar 2023 21:02:25 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/edoardo_cetin-learning-pessimism-for-reinforcement-learning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Learning Pessimism for Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Edoardo Cetin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Mar 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://kclpure.kcl.ac.uk/portal/files/196848783/10977.CetinE.pdf&#34;&gt;https://kclpure.kcl.ac.uk/portal/files/196848783/10977.CetinE.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230301210301465&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/edoardo_cetin-learning-pessimism-for-reinforcement-learning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Off-policy deep RL algorithms commonly compensate for overestimation bias during temporal difference learning by utilizing pessimistic estimates of the expected target returns&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose Generalised Pessimism Learning (GPL), a strategy employing a novel learnable penalty to enact such pessimism. In particular&lt;/li&gt;
&lt;li&gt;we propose to learn this penalty alongside the critic with dual TD-learning, a new procedure to estimate and minimise the magnitude of the target returns bias with trivial computational cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;We attribute recent improvements on RL algs to two main linked advances:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Timo_schick Toolformer Language Models Can Teach Themselves to Use Tools 2023</title>
      <link>https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/</link>
      <pubDate>Wed, 01 Mar 2023 19:57:49 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Toolformer: Language Models Can Teach Themselves to Use Tools 2023&lt;/li&gt;
&lt;li&gt;Author: Timo Schick et. al. META AI research&lt;/li&gt;
&lt;li&gt;Publish Year: 9 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Mar 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.04761.pdf&#34;&gt;https://arxiv.org/pdf/2302.04761.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230301201424679&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/timo_schick-toolformer-language-models-can-teach-themselves-to-use-tools-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LMs exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale.&lt;/li&gt;
&lt;li&gt;They also struggle with basic functionality, such as arithmetic or factual lookup.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.&lt;/li&gt;
&lt;li&gt;We introduce Toolformer, a model that incorporate a range of tools, including a calculator, a Q&amp;amp;A system, a search engine, a translation system and a calendar.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;limitation of language models&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Almog_gueta Knowledge Is a Region in Weight Space for Fine Tuned Language Model 2023</title>
      <link>https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/</link>
      <pubDate>Wed, 01 Mar 2023 12:45:54 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Knowledge Is a Region in Weight Space for Fine Tuned Language Model&lt;/li&gt;
&lt;li&gt;Author: Almog Gueta et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 12 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Mar 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.04863.pdf&#34;&gt;https://arxiv.org/pdf/2302.04863.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230301124703839&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/almog_gueta-knowledge-is-a-region-in-weight-space-for-fine-tuned-language-model-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;relatively little is known a bout the relationships between different models, especially those trained or tested on different datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa&lt;/li&gt;
&lt;li&gt;language models that have been fine-tuned on the same dataset form a tight cluster in the same weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster.&lt;/li&gt;
&lt;li&gt;traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning&lt;/li&gt;
&lt;li&gt;Our findings demonstrate that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;more findings&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xiwen_liang Contrastive Instruction Trajectory Learning for Vision Language Navigation 2022</title>
      <link>https://sino-huang.github.io/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/</link>
      <pubDate>Fri, 10 Feb 2023 02:51:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Contrastive Instruction Trajectory Learning for Vision Language Navigation&lt;/li&gt;
&lt;li&gt;Author: Xiwen Liang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: AAAI 2022&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Feb 10, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/abs/2112.04138&#34;&gt;https://arxiv.org/abs/2112.04138&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230210025151701&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/xiwen_liang-contrastive-instruction-trajectory-learning-for-vision-language-navigation-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;previous works learn to navigate step-by-step following an instruction. However, these works may fail to discriminate the similarities and discrepancies across instruction-trajectory pairs and ignore the &lt;strong&gt;&lt;u&gt;temporal continuity&lt;/u&gt;&lt;/strong&gt; of sub-instructions. These problems hinder agents from learning distinctive vision-and-language representations,&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose
&lt;ul&gt;
&lt;li&gt;a coarse-grained &lt;strong&gt;contrastive learning&lt;/strong&gt; objective  to enhance vision-and-language representations by &lt;u&gt;contrasting semantics of full trajectory observations&lt;/u&gt; and instructions respectively;&lt;/li&gt;
&lt;li&gt;a fine-grained contrastive learning objective to perceive instructions by leveraging the &lt;u&gt;temporal information&lt;/u&gt; of the sub-instructions.&lt;/li&gt;
&lt;li&gt;a pairwise sample-reweighting mechanism for contrastive learning to sampling bias in contrastive learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Limitation of current VLN model&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacob_andreas Lammp Language Models as Probabilistic Priors for Perception and Action 2023</title>
      <link>https://sino-huang.github.io/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/</link>
      <pubDate>Fri, 10 Feb 2023 00:46:15 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: LAMMP Language Models as Probabilistic Priors for Perception and Action 2023&lt;/li&gt;
&lt;li&gt;Author: Belinda Z. Li, Jacob Andreas et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 3 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Feb 10, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.02801.pdf&#34;&gt;https://arxiv.org/pdf/2302.02801.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230210004929089&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jacob_andreas-lammp-language-models-as-probabilistic-priors-for-perception-and-action-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Language models trained on large text corpora encode rich distributional information about real-world environments and action sequences.
&lt;ul&gt;
&lt;li&gt;this information plays a crucial role&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we describe how to leverage language models for non-linguistic perception and control tasks&lt;/li&gt;
&lt;li&gt;Our approach casts labelling and decision-making as inference in probabilistic graphical models in which language models parameterize prior distributions over labels, decisions and parameters, making it possible to integrate uncertain observations and incomplete background knowledge in a principled way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;common-sense priors&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zhuosheng_zhang Multimodal Chain of Thought Reasoning in Language Models 2023</title>
      <link>https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/</link>
      <pubDate>Wed, 08 Feb 2023 22:23:45 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Multimodal Chain of Thought Reasoning in Language Models&lt;/li&gt;
&lt;li&gt;Author: Zhuosheng Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 8, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2302.00923.pdf&#34;&gt;https://arxiv.org/pdf/2302.00923.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230208222840588&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/zhuosheng_zhang-multimodal-chain-of-thought-reasoning-in-language-models-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LLMs have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer.&lt;/li&gt;
&lt;li&gt;to elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning.&lt;/li&gt;
&lt;li&gt;The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We propose Mutimodal-CoT that incorporates vision features in a decoupled training framework.&lt;/li&gt;
&lt;li&gt;The framework separates the rationale generation and answer inference into two stages, the model is able to generate effective rationales that contribute to answer inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Multimodal-CoT&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Siyuan_wang Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning 2023</title>
      <link>https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/</link>
      <pubDate>Wed, 08 Feb 2023 22:17:31 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Unifying Structure Reasoning and Language Model Pre Training for Complex Reasoning&lt;/li&gt;
&lt;li&gt;Author: Siyuan Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 21 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 8, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.08913.pdf&#34;&gt;https://arxiv.org/pdf/2301.08913.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230209160806080&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/siyuan_wang-unifying-structure-reasoning-and-language-model-pre-training-for-complex-reasoning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;language models still suffer from a heterogeneous information alignment problem and a noisy knowledge injection problem.&lt;/li&gt;
&lt;li&gt;for complex reasoning, the context contains rich knowledge that typically exists in complex and sparse form.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose to unify structure reasoning and language model pre-training&lt;/li&gt;
&lt;li&gt;identifies four types of elementary knowledge structures from contexts to construct structured queries&lt;/li&gt;
&lt;li&gt;utilise box embedding method to conduct explicit structure reasoning along query during language modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What is the problem&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ekin_akyurek Towards Tracing Factual Knowledge in Language Models Back to the Training Data 2022</title>
      <link>https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/</link>
      <pubDate>Wed, 08 Feb 2023 22:16:28 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Towards Tracing Factual Knowledge in Language Models Back to the Training Data&lt;/li&gt;
&lt;li&gt;Author: Ekin Akyurek et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: EMNLP 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 8, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.180.pdf&#34;&gt;https://aclanthology.org/2022.findings-emnlp.180.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230209232944264&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ekin_akyurek-towards-tracing-factual-knowledge-in-language-models-back-to-the-training-data-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LMs have been shown to memorize a great deal of factual knowledge contained in their training data. But when an LM generates an assertion, it is often difficult to determine where it learned this information and whether it is true.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;image-assets/image-20230210000202731.png&#34; alt=&#34;image-20230210000202731&#34; style=&#34;width:70%;&#34; /&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose the problem of fact tracing
&lt;ul&gt;
&lt;li&gt;identifying which training examples taught an LM to generate a particular factual assertion.&lt;/li&gt;
&lt;li&gt;prior work on training data distribution (TDA) may offer effective tools for identifying such examples, known as &amp;ldquo;proponent&amp;rdquo;. We present the first quantitative benchmark to evaluate this&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we compare two popular families of TDA methods
&lt;ul&gt;
&lt;li&gt;gradient based&lt;/li&gt;
&lt;li&gt;embedding based&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Training data distribution method (TDA)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Danijar_hafner Mastering Diverse Domains Through World Models 2023</title>
      <link>https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/</link>
      <pubDate>Tue, 07 Feb 2023 18:18:37 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Mastering Diverse Domains Through World Models&lt;/li&gt;
&lt;li&gt;Author: Danijar Hafner et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Feb 7, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://www.youtube.com/watch?v=vfpZu0R1s1Y&#34;&gt;https://www.youtube.com/watch?v=vfpZu0R1s1Y&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230207182123945&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/danijar_hafner-mastering-diverse-domains-through-world-models-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;general intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but held back by the resources and knowledge required tune them for new task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters.&lt;/li&gt;
&lt;li&gt;we observe favourable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;World Model learning&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yuanhan_zhang What Makes Good Examples for Visual in Context Learning 2023</title>
      <link>https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/</link>
      <pubDate>Mon, 06 Feb 2023 22:38:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: What Makes Good Examples for Visual in Context Learning&lt;/li&gt;
&lt;li&gt;Author: Yuan Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Feb 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Feb 6, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.13670.pdf&#34;&gt;https://arxiv.org/pdf/2301.13670.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230207133939704&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yuanhan_zhang-what-makes-good-examples-for-visual-in-context-learning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, the main focus is on an emergent ability in large vision models, known. as in-context learning&lt;/li&gt;
&lt;li&gt;this concept has been well-known in natural language processing but has only been studied very recently for large vision models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we for the first time provide a comprehensive investigation on the impact of in-context examples in computer vision, and find that the performance is highly sensitive to the choice of in-context examples.&lt;/li&gt;
&lt;li&gt;exposing a critical issue that different in-context examples could lead to drastically different results.
&lt;ul&gt;
&lt;li&gt;Our methods obtain significant improvements over
random selection under various problem settings, showing
the potential of using prompt retrieval in vision applications
with a Model-as-a-Service (MaaS) business structure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we show that a good in-context example should be semantically similar to the query and closer in context.&lt;/li&gt;
&lt;li&gt;A model that can better balance spatial and se-
mantic closedness in feature space would be more ideal for
visual in-context learning.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;yeah, it is because the model is not that smart in a way that it can directly tell the semantic regardless of what the spatial structure looks like&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;existing issue of using LLM&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jing_yu_koh Grounding Language Models to Images for Multimodal Generation 2023</title>
      <link>https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/</link>
      <pubDate>Mon, 06 Feb 2023 22:37:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Grounding Language Models to Images for Multimodal Generation&lt;/li&gt;
&lt;li&gt;Author: Jing Yu Koh et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 31 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Feb 6, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.13823.pdf&#34;&gt;https://arxiv.org/pdf/2301.13823.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230207134638732&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jing_yu_koh-grounding-language-models-to-images-for-multimodal-generation-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose an efficient method to ground pre-trained text-only language models to the visual domain&lt;/li&gt;
&lt;li&gt;How
&lt;ul&gt;
&lt;li&gt;we keep the language model frozen, and &lt;em&gt;&lt;strong&gt;finetune input and output linear layers&lt;/strong&gt;&lt;/em&gt; to enable cross-modality interactions. This allows our model to process arbitrarily interleaved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pre-trained language models in visually grounded settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;related-work&#34;&gt;Related work&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;LLMs for vision-and-language&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zhenfang_chen See Think Confirm Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning 2023</title>
      <link>https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/</link>
      <pubDate>Mon, 06 Feb 2023 22:36:41 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge Based Visual Reasoning&lt;/li&gt;
&lt;li&gt;Author: Zhenfang Chen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 12 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Feb 6, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.05226.pdf&#34;&gt;https://arxiv.org/pdf/2301.05226.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230207113442635&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/zhenfang_chen-see-think-confirm-interactive-prompting-between-vision-and-language-models-for-knowledge-based-visual-reasoning-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect external world knowledge, and perform step-by-step reasoning to answer the questions correctly.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge based visual reasoning.&lt;/li&gt;
&lt;li&gt;IPVR contains three stages, &lt;strong&gt;see, think, and confirm&lt;/strong&gt;. The see stage scans the image and &lt;u&gt;grounds the visual concept candidates&lt;/u&gt; with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;confirm&lt;/strong&gt; stage further uses the LLM to generate the supporting rational to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;human process to handle knowledge-based visual reasoning&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xiaotian_liu a Planning Based Neural Symbolic Approach for Embodied Instruction Following 2022</title>
      <link>https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/</link>
      <pubDate>Thu, 02 Feb 2023 13:28:19 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  A Planning Based Neural Symbolic Approach for Embodied Instruction Following&lt;/li&gt;
&lt;li&gt;Author: Xiaotian Liu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Feb 2, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://embodied-ai.org/papers/2022/15.pdf&#34;&gt;https://embodied-ai.org/papers/2022/15.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230202132958709&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/xiaotian_liu-a-planning-based-neural-symbolic-approach-for-embodied-instruction-following-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;end-to-end deep learning methods struggle at these tasks due to long-horizon and sparse rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our main innovation relies on combining DL models for perception and NLP with a new egocentric planner based on successive planning problems formulated using the PDDL syntax, both for exploration and task accomplishment.&lt;/li&gt;
&lt;li&gt;our planning framework can naturally recover from action failures at any stage of the planned trajectory.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Embodied Instruction Following&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>So_yeon_min Film Following Instructions in Language With Modular Methods 2022</title>
      <link>https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/</link>
      <pubDate>Wed, 01 Feb 2023 18:32:24 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: FILM: Following Instructions in Language With Modular Methods&lt;/li&gt;
&lt;li&gt;Author: So Yeon Min et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 16 Mar 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2110.07342.pdf&#34;&gt;https://arxiv.org/pdf/2110.07342.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230201183341643&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/so_yeon_min-film-following-instructions-in-language-with-modular-methods-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;current approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning.&lt;/li&gt;
&lt;li&gt;in contrast, we propose a modular method with structured representation that
&lt;ol&gt;
&lt;li&gt;build a semantic map of scene and&lt;/li&gt;
&lt;li&gt;perform exploration with a semantic search policy, to achieve natural language goal.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FILM consists of several modular components that each
&lt;ol&gt;
&lt;li&gt;processes language instructions into structured forms (language processing)&lt;/li&gt;
&lt;li&gt;converts egocentric visual input into a semantic metric map (Semantic Mapping)&lt;/li&gt;
&lt;li&gt;predicts a search goal location (Semantic Search Policy) ?
&lt;ol&gt;
&lt;li&gt;subgoal will be plotted as a dot on the semantic top-down map&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;outputs subsequent navigation/interaction actions (Deterministic Policy)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;embodied instruction following&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yuki_inoue Prompter Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following 2022</title>
      <link>https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/</link>
      <pubDate>Wed, 01 Feb 2023 17:22:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following&lt;/li&gt;
&lt;li&gt;Author: Yuki Inoue et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 7 Nov 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Feb 1, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2211.03267.pdf&#34;&gt;https://arxiv.org/pdf/2211.03267.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230201183430332&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yuki_inoue-prompter-utilizing-large-language-model-prompting-for-a-data-efficient-embodied-instruction-following-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose FILM++ which extends the existing work FILM with modifications that do not require extra data.&lt;/li&gt;
&lt;li&gt;furthermore, we propose Prompter, which replace FILM++&amp;rsquo;s semantic search module with language model prompting.&lt;/li&gt;
&lt;li&gt;no training is needed for our prompting based implementation while achieving better or least comparable performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;FILM++ to fill the role of the data efficient baseline.&lt;/li&gt;
&lt;li&gt;we propose Prompter, which replaces the semantic search module of FILM++ with language prompting, making it even more data efficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Difficulty in converting language into robot controls&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kyle_mahowald Dissociating Language and Thought in Large Language Models a Cognitive Perspective 2023</title>
      <link>https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/</link>
      <pubDate>Tue, 31 Jan 2023 18:47:45 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Dissociating Language and Thought in Large Language Models a Cognitive Perspective&lt;/li&gt;
&lt;li&gt;Author: Kyle Mahowald et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 16 Jan 2023&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Jan 31, 2023&lt;/li&gt;
&lt;li&gt;url: &lt;a href=&#34;https://arxiv.org/pdf/2301.06627.pdf&#34;&gt;https://arxiv.org/pdf/2301.06627.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230131184855294&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/kyle_mahowald-dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-2023/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author tried to challenge the &amp;ldquo;good at language $\implies$ good at thought&amp;rdquo; fallacy.&lt;/li&gt;
&lt;li&gt;the second fallacy is &amp;ldquo;bad at thought $\implies$ bad at language&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author argued that LLMs have promise as scientific models of one piece of the human cognitive toolbox &amp;ndash; formal language processing &amp;ndash; but fall short of modelling human thought.&lt;/li&gt;
&lt;li&gt;in section 4, we consider several domains required for functional linguistic competence &amp;ndash; formal reasoning, world knowledge, situation modelling and social cognitive abilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;deep learning models in linguistics&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Michael_janner Planning With Diffusion for Flexible Behaviour Synthesis 2022</title>
      <link>https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/</link>
      <pubDate>Mon, 30 Jan 2023 13:43:20 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Planning With Diffusion for Flexible Behaviour Synthesis&lt;/li&gt;
&lt;li&gt;Author: Michael Janner et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 21 Dec 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Jan 30, 2023&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230130134456046&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/michael_janner-planning-with-diffusion-for-flexible-behaviour-synthesis-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;use the diffusion model to learn the dynamics&lt;/li&gt;
&lt;li&gt;tight coupling of the modelling and planning&lt;/li&gt;
&lt;li&gt;our goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;ideal model-based RL&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022</title>
      <link>https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/</link>
      <pubDate>Fri, 20 Jan 2023 13:59:00 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Shailaja_keyur_sampat Reasoning About Actions Over Visual and Linguistic Modalities a Survey 2022&lt;/li&gt;
&lt;li&gt;Author:&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Jan 20, 2023&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230120140740245&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;reasoning about actions &amp;amp; changes has been widely studies in the knowledge representation community, it has recently piqued the interest of NLP and computer vision researchers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Six most frequent types of commonsense knowledge&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt=&#34;image-20230120162054834&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shailaja_keyur_sampat-reasoning-about-actions-over-visual-and-linguistic-modalities-a-survey-2022/image-assets/image-20230120162054834.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;tasks that involve language-based reasoning about actions&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xin_wang Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019</title>
      <link>https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/</link>
      <pubDate>Wed, 18 Jan 2023 09:48:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforced Cross Modal Matching and Self Supervised Imitation Learning for Vision Language Navigation 2019&lt;/li&gt;
&lt;li&gt;Author: Xin Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Jan 18, 2023&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20230118095333795&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/xin_wang-reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation-2019/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Visual Language Navigation (VLN) presents some unique challenges&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first, reasoning over images and natural language instructions can be difficult.&lt;/li&gt;
&lt;li&gt;secondly, except for strictly following expert demonstrations, the feedback is rather coarse, since the &amp;ldquo;Success&amp;rdquo; feedback is provided only when the agent reaches a target position (sparse reward)&lt;/li&gt;
&lt;li&gt;A good &amp;ldquo;instruction following&amp;rdquo; trajectory may ended up just stop before you reaching the goal state and then receive zero rewards.&lt;/li&gt;
&lt;li&gt;existing work suffer from generalisation problem. (need to retrain the agent in new environment)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;agent can infer which sub-instruction to focus on and where to look at. (automatic splitting long instruction)&lt;/li&gt;
&lt;li&gt;with a matching critic that evaluates an executed path by the probability of reconstructing the original instruction from the executed path. P(original instruction | past trajectory)
&lt;ol&gt;
&lt;li&gt;cycle reconstruction: we have P(target trajectory | the instruction) = 1, and we want to measure P(original instruction | past trajectory)&lt;/li&gt;
&lt;li&gt;this will enhance the interpretability as now you understand how the robot was thinking about&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Alekh_agarwal PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning 2020</title>
      <link>https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/</link>
      <pubDate>Wed, 28 Dec 2022 14:39:25 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: PC-PG Policy Cover Directed Exploration for Provable Policy Gradient Learning&lt;/li&gt;
&lt;li&gt;Author: Alekh Agarwal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year:&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221228144306599&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alekh_agarwal-pcpg-policy-cover-directed-exploration-for-provable-policy-gradient-learning-2020/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The primary drawback of direct policy gradient methods is that, by being local in nature, they fail to adequately explore the environment.&lt;/li&gt;
&lt;li&gt;In contrast, while model-based approach and Q-learning directly handle exploration through the use of optimism.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Policy Cover-Policy Gradient algorithm (PC-PG), a direct, model-free, policy optimisation approach which addresses exploration through the use of a learned ensemble of policies, the latter provides a policy cover over the state space.
&lt;ul&gt;
&lt;li&gt;the use of a learned policy cover address exploration, and also address what is the catastrophic forgetting problem in policy gradient approaches (which use reward bonuses);&lt;/li&gt;
&lt;li&gt;the on-policy algorithm, where approximation errors due to model mispecification amplify (see [Lu et al., 2018] for discussion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;suffering from sparse reward&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alekh_agarwal on the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020</title>
      <link>https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/</link>
      <pubDate>Wed, 28 Dec 2022 14:36:20 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: On the Theory of Policy Gradient Methods Optimality Approximation and Distribution Shift 2020&lt;/li&gt;
&lt;li&gt;Author: Alekh Agarwal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 14 Oct 2020&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221228143829438&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/alekh_agarwal-on-the-theory-of-policy-gradient-methods-optimality-approximation-and-distribution-shift-2020/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution and how they cope with approximation error due to using a restricted class of parametric policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One central contribution of this work is in providing approximation guarantees that are average case - which avoid explicit worst-case dependencies on the size of state space &amp;ndash; by making a formal connection to supervised learning under distribution shift. This characterisation shows an important between estimation error, approximation error and exploration (as characterised through a precisely defined condition number)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;basic theoretical convergence questions&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chloe_ching_yun_hsu Revisiting Design Choices in Proximal Policy Optimisation 2020</title>
      <link>https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/</link>
      <pubDate>Wed, 28 Dec 2022 14:32:15 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Revisiting Design Choices in Proximal Policy Optimisation&lt;/li&gt;
&lt;li&gt;Author: Chloe Ching-Yun Hsu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 23 Sep 2020&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221228143502296&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/chloe_ching_yun_hsu-revisiting-design-choices-in-proximal-policy-optimisation-2020/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;on discrete action space with sparse high rewards, standard PPO often gets stuck at suboptimal actions. Why analyze the reason fort these failure modes and explain why they are not exposed by standard benchmarks&lt;/li&gt;
&lt;li&gt;In summary, our study suggests that Beta policy parameterization and KL-regularized objectives should be reconsidered for PPO, especially when alternatives improves PPO in all settings.&lt;/li&gt;
&lt;li&gt;The author proved the convergence guarantee for PPO-KL penalty version, as it inherits convergence guarantees of mirror descent for policy families that are closed under mixture&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;design choices&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>James_queeney Generalized Proximal Policy Optimisation With Sample Reuse 2021</title>
      <link>https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/</link>
      <pubDate>Wed, 28 Dec 2022 14:00:32 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Generalized Proximal Policy Optimisation With Sample Reuse 2021&lt;/li&gt;
&lt;li&gt;Author: James Queeney et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 29 Oct 2021&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221228140752324&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/james_queeney-generalized-proximal-policy-optimisation-with-sample-reuse-2021/image-assets/cover.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typically generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms.&lt;/li&gt;
&lt;li&gt;We develop policy improvement guarantees that are suitable for off-policy setting, and connect these bounds to the clipping mechanism used in PPO&lt;/li&gt;
&lt;li&gt;this motivate an off-policy version of the popular algorithm that we call GePPO.&lt;/li&gt;
&lt;li&gt;we demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;sample complexity&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lun_wang Backdoorl Backdoor Attack Against Competitive Reinforcement Learning 2021</title>
      <link>https://sino-huang.github.io/posts/lun_wang-backdoorl-backdoor-attack-against-competitive-reinforcement-learning-2021/</link>
      <pubDate>Wed, 28 Dec 2022 03:57:59 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/lun_wang-backdoorl-backdoor-attack-against-competitive-reinforcement-learning-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  BackdooRL Backdoor Attack Against Competitive Reinforcement Learning 2021&lt;/li&gt;
&lt;li&gt;Author: Lun Wang et. al&lt;/li&gt;
&lt;li&gt;Publish Year: 12 Dec 2021&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we propose BACKDOORL, a backdoor attack targeted at two player competitive reinforcement learning systems.&lt;/li&gt;
&lt;li&gt;first the adversary agent has to lead the victim to take a series of wrong actions instead of only one to prevent it from winning.&lt;/li&gt;
&lt;li&gt;Additionally, the adversary wants to exhibit the trigger action in as few steps as possible to avoid detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose backdoorl, the first backdoor attack targeted at competitive reinforcement learning systems. The trigger is the action of another agent in the environment.&lt;/li&gt;
&lt;li&gt;We propose a unified method to design fast-failing agent for different environment&lt;/li&gt;
&lt;li&gt;We prototype BACKDOORL and evaluate it in four environments. The results validate the feasibility of backdoor attacks in competitive environment&lt;/li&gt;
&lt;li&gt;We study the possible defenses for backdoorl. The results show that fine-tuning cannot completely remove the backdoor.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;backdoorl workflow&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sandy_huang Adversarial Attacks on Neural Network Policies 2017</title>
      <link>https://sino-huang.github.io/posts/sandy_huang-adversarial-attacks-on-neural-network-policies-2017/</link>
      <pubDate>Wed, 28 Dec 2022 00:08:22 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/sandy_huang-adversarial-attacks-on-neural-network-policies-2017/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Adversarial Attacks on Neural Network Policies&lt;/li&gt;
&lt;li&gt;Author: Sandy Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 8 Feb 2017&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Dec 28, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we characterise the degree of vulnerability across tasks and training algorithm, for a subclass of adversarial example attacks in white-box and black-box settings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yinglun_xu Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/yinglun_xu-efficient-reward-poisoning-attacks-on-online-deep-reinforcement-learning-2022/</link>
      <pubDate>Tue, 27 Dec 2022 23:14:19 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yinglun_xu-efficient-reward-poisoning-attacks-on-online-deep-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Yinglun Xu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 30 May 2022&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we study data poisoning attacks on online deep reinforcement learning (DRL) where the attacker is oblivious to the learning algorithm used by the agent and does not necessarily have full knowledge of the environment.&lt;/li&gt;
&lt;li&gt;we instantiate our framework to construct several attacks which only corrupts the rewards for a small fraction of the total training timesteps and make the agent learn a low performing policy&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;result show that the reward attack efficiently poison agent learning with a variety of SOTA DRL algorithm such as DQN, PPO&lt;/li&gt;
&lt;li&gt;our attack can work on model-free DRL algorithm for all popular learning paradigms, and only assume the learning algorithm to be efficient.&lt;/li&gt;
&lt;li&gt;large enough reward poisoning attack in the right direction is able to disrupt the DRL algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Young_wu Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/young_wu-reward-poisoning-attacks-on-offline-multi-agent-reinforcement-learning-2022/</link>
      <pubDate>Tue, 27 Dec 2022 22:50:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/young_wu-reward-poisoning-attacks-on-offline-multi-agent-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Poisoning Attacks on Offline Multi Agent Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Young Wu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Dec 2022&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;unlike attacks on single-agent RL, we show that the attacker can install the target poilcy as a Markov Perfect Dominant Strategy Equilibrium (MPDSE), which rational agents are guaranteed to follow.&lt;/li&gt;
&lt;li&gt;This attack can be significantly cheaper than separate single-agent attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xuezhou_zhang Robust Policy Gradient Against Strong Data Corruption 2021</title>
      <link>https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/</link>
      <pubDate>Tue, 27 Dec 2022 20:35:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Robust Policy Gradient Against Strong Data Corruption&lt;/li&gt;
&lt;li&gt;Author: Xuezhou Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221227203806030&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/xuezhou_zhang-robust-policy-gradient-against-strong-data-corruption-2021/image-assets/image-20221227203806030.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author utilised a SVD-denoising technique to identify and remove the possible reward perturbations&lt;/li&gt;
&lt;li&gt;this approach gives a robust RL algorithm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This approach only solve the attack perturbation that is not consistent. (i.e. not stealthy)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Policy gradient methods&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kiarash_banihashem Defense Against Reward Poisoning Attacks in Reinforcement Learning 2021</title>
      <link>https://sino-huang.github.io/posts/kiarash_banihashem-defense-against-reward-poisoning-attacks-in-reinforcement-learning-2021/</link>
      <pubDate>Tue, 27 Dec 2022 18:27:17 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/kiarash_banihashem-defense-against-reward-poisoning-attacks-in-reinforcement-learning-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Defense Against Reward Poisoning Attacks in Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Kiarash Banihashem et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 20 Jun 2021&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;our goal is to design agents that are robust against such attacks in terms of the worst-case utility w.r.t. the true unpoisoned rewards while computing their policies under the poisoned rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we formalise this reasoning and characterize the utility of our novel framework for designing defense policies. In summary, the key contributions include&lt;/p&gt;</description>
    </item>
    <item>
      <title>Amin_rakhsha Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments 2021</title>
      <link>https://sino-huang.github.io/posts/amin_rakhsha-reward-poisoning-in-reinforcement-learning-attacks-against-unknown-learners-in-unknown-environments-2021/</link>
      <pubDate>Tue, 27 Dec 2022 15:50:22 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/amin_rakhsha-reward-poisoning-in-reinforcement-learning-attacks-against-unknown-learners-in-unknown-environments-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Poisoning in Reinforcement Learning Attacks Against Unknown Learners in Unknown Environments&lt;/li&gt;
&lt;li&gt;Author: Amin Rakhsha et. al.&lt;/li&gt;
&lt;li&gt;Publish Year:  16 Feb 2021&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our attack makes minimum assumptions on the prior knowledge of the environment or the learner&amp;rsquo;s learning algorithm.&lt;/li&gt;
&lt;li&gt;most of the prior work makes strong assumptions on the knowledge of adversary &amp;ndash; it often assumed that the adversary has full knowledge of the environment or the agent&amp;rsquo;s learning algorithm or both.&lt;/li&gt;
&lt;li&gt;under such assumptions, attack strategies have been proposed that can mislead the agent to learn a nefarious policy with minimal perturbation to the rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We design a novel black-box attack, U2, that can provably achieve a near-matching performance to the SOTA white-box attack, demonstrating the feasibility of reward poisoning even in the most challenging black-box setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xuezhou_zhang Adaptive Reward Poisoning Attacks Against Reinforcement Learning 2020</title>
      <link>https://sino-huang.github.io/posts/xuezhou_zhang-adaptive-reward-poisoning-attacks-against-reinforcement-learning-2020/</link>
      <pubDate>Tue, 27 Dec 2022 00:21:15 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xuezhou_zhang-adaptive-reward-poisoning-attacks-against-reinforcement-learning-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Adaptive Reward Poisoning Attacks Against Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Xuezhou Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Jun, 2020&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Dec 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$&lt;/li&gt;
&lt;li&gt;whereas non-adaptive attacks require exponential steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe.
&lt;ul&gt;
&lt;li&gt;similar to this &lt;a href=&#34;https://sino-huang.github.io/posts/yunhan_huang-manipulating-reinforcement-learning-stealthy-attacks-on-cost-signals-2020/&#34;&gt;paper&lt;/a&gt;, it shows that reward attack has its limit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we provide a corresponding upper threshold above which the attack is feasible.&lt;/li&gt;
&lt;li&gt;we characterise conditions under which such attacks are guaranteed to fail (thus RL is safe), and vice versa&lt;/li&gt;
&lt;li&gt;in the case where attack is feasible, we provide upper bounds on the attack cost in the processing of achieving bad poliy&lt;/li&gt;
&lt;li&gt;we show that effective attacks can be found empirically using deep RL techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;feasible attack category&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anindya_sarkar Reward Delay Attacks on Deep Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/anindya_sarkar-reward-delay-attacks-on-deep-reinforcement-learning-2022/</link>
      <pubDate>Mon, 26 Dec 2022 21:07:03 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/anindya_sarkar-reward-delay-attacks-on-deep-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward Delay Attacks on Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Anindya Sarkar et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 8 Sep 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Dec 26, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we present novel attacks targeting Q-learning that exploit a vulnerability entailed by this assumption by delaying the reward signal for a limited time period.&lt;/li&gt;
&lt;li&gt;We evaluate the efficacy of the proposed attacks through a series of experiments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;our first observation is that reward-delay attacks are extremely effective when the goal for the adversarial is simply to minimise reward.&lt;/li&gt;
&lt;li&gt;we find that some mitigation method remains insufficient to ensure robustness to attacks that delay, but preserve the order, of rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tom_everitt Reinforcement Learning With a Corrupted Reward Channel 2017</title>
      <link>https://sino-huang.github.io/posts/tom_everitt-reinforcement-learning-with-a-corrupted-reward-channel-2017/</link>
      <pubDate>Mon, 26 Dec 2022 01:11:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tom_everitt-reinforcement-learning-with-a-corrupted-reward-channel-2017/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With a Corrupted Reward Channel&lt;/li&gt;
&lt;li&gt;Author: Tom Everitt&lt;/li&gt;
&lt;li&gt;Publish Year: August 22, 2017&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Dec 26, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP&lt;/li&gt;
&lt;li&gt;Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be &lt;strong&gt;completely managed&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;second, by using randomisation to blunt the agent&amp;rsquo;s optimisation, reward corruption can be partially managed under some assumption&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yunhan_huang Manipulating Reinforcement Learning Stealthy Attacks on Cost Signals 2020</title>
      <link>https://sino-huang.github.io/posts/yunhan_huang-manipulating-reinforcement-learning-stealthy-attacks-on-cost-signals-2020/</link>
      <pubDate>Sun, 25 Dec 2022 19:12:17 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yunhan_huang-manipulating-reinforcement-learning-stealthy-attacks-on-cost-signals-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Manipulating Reinforcement Learning Stealthy Attacks on Cost Signals&lt;/li&gt;
&lt;li&gt;Deceptive Reinforcement Learning Under Adversarial
Manipulations on Cost Signals&lt;/li&gt;
&lt;li&gt;Author: Yunhan Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Dec 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;understand the impact of the falsification of cost signals on the convergence of Q-learning algorithm&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In Q-learning, we show that Q-learning algorithms converge under stealthy attacks and bounded falsifications on cost signals.&lt;/li&gt;
&lt;li&gt;and there is a robust region within which the adversarial attacks cannot achieve its objective. The robust region of the cost can be utilised by both offensive and defensive side.&lt;/li&gt;
&lt;li&gt;An RL agent can leverage the robust region to evaluate the robustness to malicious falsification.&lt;/li&gt;
&lt;li&gt;we provide conditions on the falsified cost which can mislead the agent to learn an adversary&amp;rsquo;s favoured policy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Stealthy Attacks&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Vincent_zhuang No Regret Reinforcement Learning With Heavy Tailed Rewards 2021</title>
      <link>https://sino-huang.github.io/posts/vincent_zhuang-no-regret-reinforcement-learning-with-heavy-tailed-rewards-2021/</link>
      <pubDate>Sun, 25 Dec 2022 18:15:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/vincent_zhuang-no-regret-reinforcement-learning-with-heavy-tailed-rewards-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: No-Regret Reinforcement Learning With Heavy Tailed Rewards&lt;/li&gt;
&lt;li&gt;Author: Vincent Zhuang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Dec 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To the best of our knowledge, no prior work has considered our setting of heavy-tailed rewards in the MDP setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We demonstrate that &lt;strong&gt;robust mean estimation techniques&lt;/strong&gt; can be broadly applied to reinforcement learning algorithms (specifically
confidence-based methods) in order to provably han-
dle the heavy-tailed reward setting&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Robust UCB algorithm&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wenshuai_zhao Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning 2020</title>
      <link>https://sino-huang.github.io/posts/wenshuai_zhao-towards-closing-the-sim-to-real-gap-in-collaborative-multi-robot-deep-reinforcement-learning-2020/</link>
      <pubDate>Sun, 25 Dec 2022 16:54:11 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/wenshuai_zhao-towards-closing-the-sim-to-real-gap-in-collaborative-multi-robot-deep-reinforcement-learning-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Towards Closing the Sim to Real Gap in Collaborative Multi Robot Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Wenshuai Zhao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Dec 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we introduce the effect of sensing, calibration, and accuracy mismatches in distributed reinforcement learning&lt;/li&gt;
&lt;li&gt;we discuss on how both the different types of perturbations and how the number of agents experiencing those perturbations affect the collaborative learning effort&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is, to the best of our knowledge, the first work exploring the limitation of PPO in multi-robot systems when considering that different robots might be exposed to different environment where their sensors or actuators have induced errors&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jan_corazza Reinforcement Learning With Stochastic Reward Machines 2022</title>
      <link>https://sino-huang.github.io/posts/jan_corazza-reinforcement-learning-with-stochastic-reward-machines-2022/</link>
      <pubDate>Sat, 24 Dec 2022 22:36:07 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jan_corazza-reinforcement-learning-with-stochastic-reward-machines-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Stochastic Reward Machines&lt;/li&gt;
&lt;li&gt;Author: Jan Corazza et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: AAAI 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequence of actions. However, existing algorithms for learning reward machines assume an overly idealized   setting where rewards have to be free of noise.&lt;/li&gt;
&lt;li&gt;to overcome this practical limitation, we introduce a novel type of reward machines called stochastic reward machines, and an algorithm for learning them.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Discussing the handling of noisy reward for non-markovian reward function.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;limitation&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;the solution introduces multiple sub value function models, which is different from the standard RL algorithm.&lt;/li&gt;
&lt;li&gt;The work does not emphasise on the sample efficiency of the algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reward machine&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Oguzhan_dogru Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering 2022</title>
      <link>https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/</link>
      <pubDate>Sat, 24 Dec 2022 19:32:25 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Constrained Uncertain Reward Function Through Particle Filtering&lt;/li&gt;
&lt;li&gt;Author: Oguzhan Dogru et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: July 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this study consider a type of uncertainty, which is caused by the sensor that are utilised for reward function. When the noise is Gaussian and the system is linear&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221224214550390&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/oguzhan_dogru-reinforcement-learning-with-constrained-uncertain-reward-function-through-particle-filtering-2022/image-assets/image-20221224214550390.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this work used &amp;ldquo;particle filtering&amp;rdquo; technique to estimate the true reward function from the perturbed discrete reward sampling points.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;h2 id=&#34;good-things-about-the-paper-one-paragraph&#34;&gt;Good things about the paper (one paragraph)&lt;/h2&gt;
&lt;h2 id=&#34;major-comments&#34;&gt;Major comments&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Citation&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inaam_ilahi Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/</link>
      <pubDate>Sat, 24 Dec 2022 17:06:12 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/inaam_ilahi-challenges-and-countermeasures-for-adversarial-attacks-on-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Challenges and Countermeasures for Adversarial Attacks on Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Inaam Ilahi et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 13 Sep 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DRL is susceptible to adversarial attacks, which precludes its use in real-life critical system and applications.&lt;/li&gt;
&lt;li&gt;Therefore, we provide a comprehensive survey that discusses emerging attacks on DRL-based system and the potential countermeasures to defend against these attacks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we provide the DRL fundamentals along with a non-exhaustive taxonomy of advanced DRL algorithms&lt;/li&gt;
&lt;li&gt;we present a comprehensive survey of adversarial attacks on DRL and their potential countermeasures&lt;/li&gt;
&lt;li&gt;we discuss the available benchmarks and metrics for the robustness of DRL&lt;/li&gt;
&lt;li&gt;finally, we highlight the open issues and research challenges in the robustness of DRL and introduce some potential research directions .&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;organisation of this article&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zuxin_liu on the Robustness of Safe Reinforcement Learning Under Observational Perturbations 2022</title>
      <link>https://sino-huang.github.io/posts/zuxin_liu-on-the-robustness-of-safe-reinforcement-learning-under-observational-perturbations-2022/</link>
      <pubDate>Thu, 22 Dec 2022 22:38:13 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/zuxin_liu-on-the-robustness-of-safe-reinforcement-learning-under-observational-perturbations-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: On the Robustness of Safe Reinforcement Learning Under Observational Perturbations&lt;/li&gt;
&lt;li&gt;Author: Zuxin Liu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 3 Oct 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Dec 22, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;While many recent safe RL methods with deep policies can achieve outstanding constraint satisfaction in noise-free simulation environment, such a concern regarding their vulnerability under adversarial perturbation has not been studies in the safe RL setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we are the first to formally analyze the unique vulnerability of the optimal policy in safe RL under observational corruptions. We define the state-adversarial safe RL problem and investigate its fundamental properties. We show that optimal solutions of safe RL problems are theoretically vulnerable under observational adversarial attacks&lt;/li&gt;
&lt;li&gt;we show that existing adversarial attack algorithms focusing on minimizing agent rewards do not always work, and propose two effective attack algorithms with theoretical justifications &amp;ndash; one directly maximise the constraint violation cost, and one maximise the task reward to induce a tempting but risky policy.
&lt;ul&gt;
&lt;li&gt;Surprisingly, the maximum reward attack is very strong in inducing unsafe behaviors, both in theory and practice&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;we propose an adversarial training algorithm with the proposed attackers and show contraction properties of their Bellman operators. Extensive experiments in continuous control tasks show that our method is more robust against adversarial perturbations in terms of constraint satisfaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Safe reinforcement learning definition&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ruben_majadas Disturbing Reinforcement Learning Agents With Corrupted Rewards 2021</title>
      <link>https://sino-huang.github.io/posts/ruben_majadas-disturbing-reinforcement-learning-agents-with-corrupted-rewards-2021/</link>
      <pubDate>Sat, 17 Dec 2022 00:38:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ruben_majadas-disturbing-reinforcement-learning-agents-with-corrupted-rewards-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Disturbing Reinforcement Learning Agents With Corrupted Rewards&lt;/li&gt;
&lt;li&gt;Author: Ruben Majadas et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Feb 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 17, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;recent works have shown how the performance of RL algorithm decreases under the influence of soft changes in the reward function.&lt;/li&gt;
&lt;li&gt;However, little work has been done about how sensitive these disturbances are depending on the aggressiveness of the attack and the learning learning exploration strategy.&lt;/li&gt;
&lt;li&gt;it chooses a subclass of MDPs: episodic, stochastic goal-only rewards MDPs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;it demonstrated that smoothly crafting adversarial rewards are able to mislead the learner&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;the policy that is learned using low exploration probability values is more robust to corrupt rewards.&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;(though this conclusion seems valid only for the proposed experiment setting)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the agent is completely lost with attack probabilities higher that than p=0.4&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;deterministic goal only reward MDP&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jingkang_wang Reinforcement Learning With Perturbed Rewards 2020</title>
      <link>https://sino-huang.github.io/posts/jingkang_wang-reinforcement-learning-with-perturbed-rewards-2020/</link>
      <pubDate>Fri, 16 Dec 2022 20:48:51 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jingkang_wang-reinforcement-learning-with-perturbed-rewards-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reinforcement Learning With Perturbed Rewards&lt;/li&gt;
&lt;li&gt;Author: Jingkang Wang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 1 Feb 2020&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Dec 16, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this paper studies RL with perturbed rewards, where a technical challenge is to revert the perturbation process so that the right policy is learned. Some experiments are used to support the algorithm (i.e., estimate the confusion matrix and revert) using existing techniques from the supervised learning (and crowdsourcing) literature.&lt;/li&gt;
&lt;li&gt;Limitation
&lt;ul&gt;
&lt;li&gt;reviewers had concerns over the scope / significance of this work, mostly about how the confusion matrix is learned. If this matrix is known, correcting reward perturbation is easy, and standard RL can be applied to the corrected rewards.&lt;/li&gt;
&lt;li&gt;Specifically, the work seems to be limited in two substantial ways, both related to how confusion matrix is learned
&lt;ul&gt;
&lt;li&gt;the reward function needs to be deterministic&lt;/li&gt;
&lt;li&gt;majority voting requires the number of states to be finite
&lt;ul&gt;
&lt;li&gt;the significance of this work is therefore limited to finite-state problems with deterministic rewards, which is quite restricted.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;overall, the setting studied here, together with a thorough treatment of an (even restricted) case, could make an interesting paper that inspires future work. However, the exact problem setting is not completely clear in the paper, and the limitation of the technical contribution is somewhat unclear.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The SOTA PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;reward function is often perturbed&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacob_andreas Language Models as Agent Models 2022</title>
      <link>https://sino-huang.github.io/posts/jacob_andreas-language-models-as-agent-models-2022/</link>
      <pubDate>Sat, 10 Dec 2022 00:47:33 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jacob_andreas-language-models-as-agent-models-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language Models as Agent Models&lt;/li&gt;
&lt;li&gt;Author: Jacob Andreas&lt;/li&gt;
&lt;li&gt;Publish Year: 3 Dec 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Dec 10, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2212.01681.pdf&#34;&gt;https://arxiv.org/pdf/2212.01681.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;during training, LMs have access only to the text of the documents, with no direct evidence of the internal states of the human agent that produce them. (kind of hidden MDP thing)&lt;/li&gt;
&lt;li&gt;this is a fact often used to argue that LMs are incapable of modelling goal-directed aspects of human language production and comprehension.&lt;/li&gt;
&lt;li&gt;The author stated that even in today&amp;rsquo;s non-robust and error-prone models &amp;ndash; LM infer and use representations of fine-grained communicative intensions and more abstract beliefs and goals. Despite that limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In other words&lt;/strong&gt;, the author said that language model can be used to communicate intention of human agent, and hence it can be treated as a agent model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the author claimed that
&lt;ul&gt;
&lt;li&gt;in the course of performing next-word prediction in context, current LMs sometimes infer inappropriate, partial representations of beliefs ,desires and intentions possessed by the agent that produced the context, and other agents mentioned within it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Once&lt;/strong&gt; these representations are &lt;strong&gt;inferred&lt;/strong&gt;, they are &lt;strong&gt;causally&lt;/strong&gt; &lt;strong&gt;linked&lt;/strong&gt; to LM prediction, and thus bear the same relation to generated text that an intentional agent&amp;rsquo;s state bears to its communicative actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The high-level goals of this paper are twofold:
&lt;ul&gt;
&lt;li&gt;first, to outline a specific sense in which idealised language models can function as models of agent belief, desires and intentions;&lt;/li&gt;
&lt;li&gt;second, to highlight a few cases in which existing models appear to approach this idealization (and describe the ways in which they still fall short)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Training on text alone produces ready-made models of the map from agent states to text; these models offer a starting point for language processing systems that communicate intentionally.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Current language model is bad&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Charlie_snell Context Aware Language Modeling for Goal Oriented Dialogue Systems 2022</title>
      <link>https://sino-huang.github.io/posts/charlie_snell-context-aware-language-modeling-for-goal-oriented-dialogue-systems-2022/</link>
      <pubDate>Sun, 20 Nov 2022 16:29:59 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/charlie_snell-context-aware-language-modeling-for-goal-oriented-dialogue-systems-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Context Aware Language Modeling for Goal Oriented Dialogue Systems&lt;/li&gt;
&lt;li&gt;Author: Charlie Snell et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Apr 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Nov 20, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;while supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question.&lt;/li&gt;
&lt;li&gt;how can we scalably and effectively introduce the mechanisms of goal-directed decision making into end-to-end language models, to steer language generation toward completing specific dialogue tasks rather than simply generating probable responses.&lt;/li&gt;
&lt;li&gt;they aim to directly finetune language models in a task-aware manner such that they can maximise a give &lt;em&gt;utility function&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;img src=&#34;image-assets/image-20221120180500086.png&#34; alt=&#34;image-20221120180500086&#34; style=&#34;width:50%;&#34; /&gt;
&lt;ul&gt;
&lt;li&gt;it seems like the manipulation of training dataset and also the auxiliary objective are the two main &amp;ldquo;innovations&amp;rdquo; of the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dialogue&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sanchit_agarwal Building Goal Oriented Dialogue Systems With Situated Visual Context 2021</title>
      <link>https://sino-huang.github.io/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/</link>
      <pubDate>Sun, 20 Nov 2022 16:29:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Building Goal Oriented Dialogue Systems With Situated Visual Context 2021&lt;/li&gt;
&lt;li&gt;Author: Sanchit Agarwal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Nov 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Nov 20, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;with the surge of virtual assistants with screen, the next generation of agents are required to also understand screen context in order to provide a proper interactive experience, and better understand users&amp;rsquo; goals.&lt;/li&gt;
&lt;li&gt;So in this paper, they propose a novel multimodal conversational framework, where the agent&amp;rsquo;s next action and their arguments are derived jointly conditioned on the conversational and the visual context.&lt;/li&gt;
&lt;li&gt;The model can recognise visual features such as color and shape as well as the metadata based features such as price or star rating associated with a visual entity.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;propose a novel multimodal conversational system that considers screen context, in addition to dialogue context, while deciding the agent&amp;rsquo;s next action&lt;/li&gt;
&lt;li&gt;The proposed visual grounding model takes both metadata and images as input allowing it to reason over metadata and visual information&lt;/li&gt;
&lt;li&gt;Our solution encodes the user query and each visual entities and then compute the similarity between them. to improve the visual entity encoding, they introduced query guided attention and entity self-attention layers.&lt;/li&gt;
&lt;li&gt;collect the MTurk survey and also create a multimodal dialogue simulator&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221120232504688&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/sanchit_agarwal-building-goal-oriented-dialogue-systems-with-situated-visual-context-2021/image-assets/image-20221120232504688.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yichi_zhang Danli Deliberative Agent for Following Natural Language Instructions 2022</title>
      <link>https://sino-huang.github.io/posts/yichi_zhang-danli-deliberative-agent-for-following-natural-language-instructions-2022/</link>
      <pubDate>Sun, 20 Nov 2022 16:28:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yichi_zhang-danli-deliberative-agent-for-following-natural-language-instructions-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: DANLI: Deliberative Agent for Following Natural Language Instructions&lt;/li&gt;
&lt;li&gt;Author: Yichi Zhang&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Oct, 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sun, Nov 20, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;reactive agent simply learn and imitate behaviours encountered in the training data&lt;/li&gt;
&lt;li&gt;these reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from the past experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Natural language instruction following with embodied AI agents&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xiang_li Diffusion-LM Improves Controllable Text Generation 2022</title>
      <link>https://sino-huang.github.io/posts/xiang_li-diffusion_lm_improves_controllable_text_generation_2022/</link>
      <pubDate>Mon, 14 Nov 2022 16:32:31 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/xiang_li-diffusion_lm_improves_controllable_text_generation_2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Diffusion-LM Improves Controllable Text Generation&lt;/li&gt;
&lt;li&gt;Author: Xiang Lisa Li&lt;/li&gt;
&lt;li&gt;Publish Year: May 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Nov 14, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2205.14217.pdf&#34;&gt;https://arxiv.org/pdf/2205.14217.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;can language tokens be represented as floating number?&lt;/li&gt;
&lt;li&gt;they develop a new non-autoregressive language model based on continuous diffusion&lt;/li&gt;
&lt;li&gt;Diffusion LM iteratively denoises as sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variable.&lt;/li&gt;
&lt;li&gt;how to convert from continuous embeddings back to words
&lt;ul&gt;
&lt;li&gt;they used rounding and many other tricks to stabilise the training process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they tried diffusion model for Language Model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;incomprehension&#34;&gt;Incomprehension&lt;/h2&gt;
&lt;p&gt;Not sure if the model is good at text generation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jie_huang Can Language Models Be Specific How 2022</title>
      <link>https://sino-huang.github.io/posts/jie_huang-can-language-models-be-specific-how-2022/</link>
      <pubDate>Tue, 08 Nov 2022 20:41:04 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jie_huang-can-language-models-be-specific-how-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Can Language Models Be Specific? How?&lt;/li&gt;
&lt;li&gt;Author: Jie Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 11 Oct 2022&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Nov 8, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they propose to measure how specific the language of pre-trained language models (PLM) is, To achieve this, they introduced a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts.&lt;/li&gt;
&lt;li&gt;for instance given &amp;ldquo;J.K. Rowling was born in [MASK]&amp;rdquo;, we want to test whether a more specific answer will be better filled by PLMs. e.g., Yate instead of England&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;image-20221108211541780image-assetsimage-20221108211541780png&#34;&gt;&lt;img alt=&#34;image-20221108211541780&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jie_huang-can-language-models-be-specific-how-2022/image-assets/image-20221108211541780.png&#34;&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;it is known that if the prediction is more specific, we can retrieve more fine-grained information from language models, and further acquire more information.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;viewer&amp;rsquo;s opinion&lt;/em&gt;: we are not saying that summarisation is easy or having less useful information, there are cases that abstract info is more useful&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;although there are works on measuring how much knowledge is stored in PLMs or improving the correctness of the predictions, non attempted to measure or improve the specificity of prediction made by PLMs.&lt;/li&gt;
&lt;li&gt;Understanding how specific the language of PLMs is can help us better understand the behaviour of language models and facilitate downstream applications such as question answering etc.&lt;/li&gt;
&lt;li&gt;setup a dataset benchmark for specificity,  The quality of the benchmark is high, where the judgment on which answer is more specific is ∼ 97% consistent with humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;discovery&#34;&gt;Discovery&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;in general, PLMs prefer less specific answers without subjects given, and they only have a weak ability to differentiate coarse-grained/fine-grained objects by measuring their (cosine) similarities to subjects.&lt;/li&gt;
&lt;li&gt;the results indicate that specificity was neglected by existing research on language models&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;improving-specificity-of-the-prediction&#34;&gt;Improving specificity of the prediction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;few-shot prompting&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yizhou_zhao Semantic Aligned Fusion Transformer for One Shot Object Detection 2022</title>
      <link>https://sino-huang.github.io/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/</link>
      <pubDate>Mon, 24 Oct 2022 19:14:34 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Semantic-Aligned Fusion Transformer for One Shot Object Detection&lt;/li&gt;
&lt;li&gt;Author: Yizhou Zhao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Oct 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.09093v2.pdf&#34;&gt;https://arxiv.org/pdf/2203.09093v2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;with extreme data scarcity, current approaches, explore various feature fusions to obtain directly transferable meta-knowledge&lt;/li&gt;
&lt;li&gt;in this paper, they, attribute the previous limitation to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structure and scale variances.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025164925086&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/image-assets/image-20221025164925086.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025165002340&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yizhou_zhao-semantic-aligned-fusion-transformer-for-one-shot-object-detection-2022/image-assets/image-20221025165002340.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ting_i_hsieh One Shot Object Detection With Co Attention and Co Excitation 2019</title>
      <link>https://sino-huang.github.io/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/</link>
      <pubDate>Mon, 24 Oct 2022 19:13:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: One-Shot Object Detection With Co-Attention and Co-Excitation&lt;/li&gt;
&lt;li&gt;Author: Ting-I Hsieh et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Nov 2019&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Oct 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1911.12529.pdf&#34;&gt;https://arxiv.org/pdf/1911.12529.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025160104142&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/image-assets/image-20221025160104142.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this paper aims to tackle the challenging problem of one-shot object detection, Given a query image patch whose class label is not included in the training data,&lt;/li&gt;
&lt;li&gt;To this end, they developed a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects
&lt;ul&gt;
&lt;li&gt;first, use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation.&lt;/li&gt;
&lt;li&gt;second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasise correlated feature channels to help uncover relevant object proposals and eventually the target objects&lt;/li&gt;
&lt;li&gt;third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen training.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;heading&#34;&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025162638405&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ting_i_hsieh-one-shot-object-detection-with-co-attention-and-co-excitation-2019/image-assets/image-20221025162638405.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ayan_kumar_bhunia a Deep One Shot Network for Query Based Logo Retrieval 2019</title>
      <link>https://sino-huang.github.io/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/</link>
      <pubDate>Mon, 24 Oct 2022 19:12:22 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: A Deep-One Shot Network for Query-Based Logo Retrieval&lt;/li&gt;
&lt;li&gt;Author: Ayan Kumar Bhunia et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Jul 2019&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Oct 24, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1811.01395.pdf&#34;&gt;https://arxiv.org/pdf/1811.01395.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Existing general purpose just cannot handle unseen new logos (not labelled logos)&lt;/li&gt;
&lt;li&gt;in this work, they developed an easy-to-implement query based logo detection and localisation system by employing a one-shot learning technique using off-the-shelf neural network components.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025130007528&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/image-assets/image-20221025130007528.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;limitation-of-current-work&#34;&gt;Limitation of current work&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Deep-learning based framework are largely data-driven, contrary to logo-dataset that have several image classes but &lt;strong&gt;few images&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;need to be robust to new unseen logos, the model should be designed to satisfy the incremental demands for logo classes, contrary to existing methods which are limited to a set of seen logos and are not.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;propose a scalable solution for the logo detection problem, they present a query-based logo search and detection system by employing a simple fully differentiable one-shot learning framework which can be used for new logo classes without further training the whole network.&lt;/li&gt;
&lt;li&gt;to deal with the logos of varying sizes, we propose a novel one-shot framework through multi-scale conditioning that is specially designed to learn the similarity between the query image and target image at multiple scales and resolutions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221025151708843&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/ayan_kumar_bhunia-a-deep-one-shot-network-for-query-based-logo-retrieval-2019/image-assets/image-20221025151708843.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yuetian_weng an Efficient Spatio Temporal Pyramid Transformer for Action Detection 2022</title>
      <link>https://sino-huang.github.io/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/</link>
      <pubDate>Thu, 20 Oct 2022 19:06:41 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: An Efficient Spatio-Temporal Pyramid Transformer for Action Detection&lt;/li&gt;
&lt;li&gt;Author: Yuetian Weng et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Jul 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Oct 20, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the task of action detection aims at deducing both the action category and localisation of the start and end moment for each action instance in a long, untrimmed video.&lt;/li&gt;
&lt;li&gt;it is non-trivial to design an efficient architecture for action detection due to the prohibitively &lt;strong&gt;expensive&lt;/strong&gt; self-attentions over a long sequence of video clips&lt;/li&gt;
&lt;li&gt;To this end, they present an efficient hierarchical spatial temporal transformer for action detection&lt;/li&gt;
&lt;li&gt;Building upon the fact that the early self-attention layer in Transformer still focus on local patterns.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;to date, the majority of action detection methods are driven by 3D convolutional neural networks (CNNs), e.g., C3D, I3D, to encode video segment features from video RGB frames and optical flows
&lt;ul&gt;
&lt;li&gt;however, the limited receptive field hinders the CNN-based models to capture long-term spatio-temporal dependencies. alternatively, vision transformers have shown the advantage of capturing global dependencies via the self-attention mechanism.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hierarchical ViTs divide Transformer blocks into several stages and progressively reduce the spatial size of feature maps when the network goes deeper.
&lt;ul&gt;
&lt;li&gt;but having self-attention over a sequence of images is expensive&lt;/li&gt;
&lt;li&gt;also they found out that the &lt;em&gt;global attention&lt;/em&gt; in the early layers actually only encodes local visual pattens (i.e., &lt;u&gt;it only attends to its nearby tokens in adjacent frames&lt;/u&gt; while rarely interacting with tokens in distance frames)&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20221021185742980&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/image-assets/image-20221021185742980.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;efficient-spatio-temporal-pyramid-transformer&#34;&gt;Efficient Spatio-temporal Pyramid Transformer&lt;/h3&gt;
&lt;p&gt;&lt;img alt=&#34;image-20221022182202983&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yuetian_weng-an-efficient-spatio-temporal-pyramid-transformer-for-action-detection-2022/image-assets/image-20221022182202983.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Steven_kapturowski Human Level Atari 200x Faster 2022</title>
      <link>https://sino-huang.github.io/posts/steven_kapturowski-human-level-atari-200x-faster-2022/</link>
      <pubDate>Wed, 05 Oct 2022 23:22:01 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/steven_kapturowski-human-level-atari-200x-faster-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Human Level Atari 200x Faster&lt;/li&gt;
&lt;li&gt;Author: Steven Kapturowski et. al. DeepMind&lt;/li&gt;
&lt;li&gt;Publish Year: September 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Oct 5, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.07550.pdf&#34;&gt;https://arxiv.org/pdf/2209.07550.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Agent 57 came at the cost of &lt;u&gt;poor data-efficiency&lt;/u&gt; , requiring nearly 80,000 million frames of experience to achieve.&lt;/li&gt;
&lt;li&gt;this one can achieve the same performance in 390 million frames&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;NFNet - Normalisation Free Network&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee&#34;&gt;https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Batch normalisation &amp;ndash; the bad
&lt;ul&gt;
&lt;li&gt;it is expensive&lt;/li&gt;
&lt;li&gt;batch normalisation breaks the assumption of data independence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NFNet applies 3 different techniques:
&lt;ul&gt;
&lt;li&gt;Modified residual branches and convolutions with Scaled Weight standardisation&lt;/li&gt;
&lt;li&gt;Adaptive Gradient Clipping&lt;/li&gt;
&lt;li&gt;Architecture optimisation for improved accuracy and training speed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/vballoli/nfnets-pytorch&#34;&gt;https://github.com/vballoli/nfnets-pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Previous Non-Image features&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Andrea_banino Coberl Contrastive Bert for Reinforcement Learning 2022</title>
      <link>https://sino-huang.github.io/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/</link>
      <pubDate>Wed, 05 Oct 2022 23:04:49 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/andrea_banino-coberl-contrastive-bert-for-reinforcement-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: CoBERL Contrastive BERT for Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Andrea Banino et. al. DeepMind&lt;/li&gt;
&lt;li&gt;Publish Year: Feb 2022&lt;/li&gt;
&lt;li&gt;Review Date: Wed, Oct 5, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2107.05431.pdf&#34;&gt;https://arxiv.org/pdf/2107.05431.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Representation learning in reinforcement learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;motivation:
&lt;ul&gt;
&lt;li&gt;if state information could be effectively extracted from raw observations it may then be possible to learn from there as fast as from states.&lt;/li&gt;
&lt;li&gt;however, given the often sparse reward signal coming from the environment, learning representations in RL has to be achieved with little to no supervision.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;approach types
&lt;ul&gt;
&lt;li&gt;class 1: auxiliary self-supervised losses to accelerate the learning speed in model-free RL algorithm&lt;/li&gt;
&lt;li&gt;class 2: learn a world model and use this to collect imagined rollouts, which then act as extra data to train the RL algorithm reducing the samples required from the environment&lt;/li&gt;
&lt;li&gt;CoBERL is in class 1
&lt;ul&gt;
&lt;li&gt;​	it uses both masked language modelling and contrastive learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;RL using BERT architecture&lt;/strong&gt; &amp;ndash; RELIC&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jonathan_ho Video Diffusion Models 2022</title>
      <link>https://sino-huang.github.io/posts/jonathan_ho-video-diffusion-models-2022/</link>
      <pubDate>Thu, 22 Sep 2022 20:40:21 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jonathan_ho-video-diffusion-models-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Google Video Diffusion Models&lt;/li&gt;
&lt;li&gt;Author: Jonathan Ho et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 22 Jun 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Sep 22, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;proposing a diffusion model for video generation that shows very promising initial results&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;this is the extension of image diffusion model&lt;/li&gt;
&lt;li&gt;they introduce a new conditional sampling technique for spatial and temporal video extension that performs better.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Diffusion model&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A diffusion model specified in continuous time is a generative model with latents&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20220923173758941&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jonathan_ho-video-diffusion-models-2022/image-assets/image-20220923173758941.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Training diffusion model&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dongwon Fire Burns Sword Cuts Commonsense Inductive Bias for Exploration in Text Based Games 2022</title>
      <link>https://sino-huang.github.io/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/</link>
      <pubDate>Thu, 22 Sep 2022 19:38:56 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/dongwon-fire-burns-sword-cuts-commonsense-inductive-bias-for-exploration-in-text-based-games-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text Based Games&lt;/li&gt;
&lt;li&gt;Author: Dongwon Kelvin Ryu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: ACL 2022&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Sep 22, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action space.&lt;/li&gt;
&lt;li&gt;A fundamental challenges in TGs is the &lt;u&gt;efficient exploration of the large action space&lt;/u&gt; when the agent has not yet acquired &lt;u&gt;enough knowledge&lt;/u&gt; about the environment.&lt;/li&gt;
&lt;li&gt;So, we want to inject external commonsense knowledge into the agent during training when the agent is most uncertain about its next action.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In addition to performance increase, the produced trajectory of actions exhibit lower perplexity, when tested with a pre-trained LM, indicating better closeness to human language.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Exploration efficiency&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wenlong_huang Language Models as Zero Shot Planners Extracting Actionable Knowledge for Embodied Agents 2022</title>
      <link>https://sino-huang.github.io/posts/wenlong_huang-language-models-as-zero-shot-planners-extracting-actionable-knowledge-for-embodied-agents-2022/</link>
      <pubDate>Mon, 19 Sep 2022 21:55:13 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/wenlong_huang-language-models-as-zero-shot-planners-extracting-actionable-knowledge-for-embodied-agents-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language Models as Zero Shot Planners: Extracting Actionable Knowledge for Embodied Agents&lt;/li&gt;
&lt;li&gt;Author: Wenlong Huang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Mar 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mon, Sep 19, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Large language models are learning general commonsense world knowledge.&lt;/li&gt;
&lt;li&gt;so this paper, the author investigate the possibility of grounding high-level tasks, expressed as natural language (e.g., &amp;ldquo;make breakfast&amp;rdquo;) to a chosen set of action steps (&amp;ldquo;open fridge&amp;rdquo;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they found out that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training.&lt;/li&gt;
&lt;li&gt;they proposed several tools to improve executability of the model generation without invasive probing or modifications to the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;What is prompt learning&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pengchuan_zhang Vinvl Revisiting Visual Representations in Vision Language Models 2021</title>
      <link>https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/</link>
      <pubDate>Sat, 03 Sep 2022 17:17:47 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/pengchuan_zhang-vinvl-revisiting-visual-representations-in-vision-language-models-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: VinVL: Revisiting Visual Representations in Vision Language Models&lt;/li&gt;
&lt;li&gt;Author: Pengchuan Zhang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 10 Mar 2021&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Sep 3, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In our experiments we feed the visual features generated by &lt;strong&gt;the new object detection model&lt;/strong&gt; into a Transformer-based VL fusion model Oscar.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And utilise an improved approach OSCAR + to pretrain the VL model&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;has a bigger Object Detection model with larger amount of training data, called &amp;ldquo;ResNeXt-152 C4&amp;rdquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Vision Language Pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Xiujun_li Oscar Object Semantic Aligned Pro Training for Vision Language Tasks 2020</title>
      <link>https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/</link>
      <pubDate>Sat, 03 Sep 2022 17:12:54 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/xiujun_li-oscar-object-semantic-aligned-pro-training-for-vision-language-tasks-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Oscar: Object Semantic Aligned Pro Training for Vision Language Tasks&lt;/li&gt;
&lt;li&gt;Author: Xiujun Li et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 26 Jul 2020&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Sep 3, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Existing method simply concatenates image region features (patch features) and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the lack of explicit alignment information between the image regions and the text poses alignment modelling a weakly-supervised learning task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yung_sung_chuang Diffcse Difference Based Contrastive Learning for Sentence Embeddings 2022</title>
      <link>https://sino-huang.github.io/posts/yung_sung_chuang-diffcse-difference-based-contrastive-learning-for-sentence-embeddings-2022/</link>
      <pubDate>Sat, 27 Aug 2022 16:03:42 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/yung_sung_chuang-diffcse-difference-based-contrastive-learning-for-sentence-embeddings-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: DiffCSE: Difference Based Contrastive Learning for Sentence Embeddings&lt;/li&gt;
&lt;li&gt;Author: Yung-Sung Chuang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 21 Apr 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Aug 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;DiffCSE learns sentences that are sensitive to the difference between the original sentence and and &lt;strong&gt;edited sentence&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;we propose DiffCSE, an unsupervised contrastive learning framework for learning &lt;strong&gt;sentence embeddings&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;DiffCSE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;this is an unsupervsied contrastive learning framework rather than model architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Contrastive learning in single modality data&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gregor_geigle Retrieve Fast Rerank Smart Cooperative and Joint Approaches for Improved Cross Modal Retrieval 2022</title>
      <link>https://sino-huang.github.io/posts/gregor_geigle-retrieve-fast-rerank-smart-coorperative-and-joint-approaches-for-improved-cross-modal-retrieval-2022/</link>
      <pubDate>Sat, 27 Aug 2022 00:31:38 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/gregor_geigle-retrieve-fast-rerank-smart-coorperative-and-joint-approaches-for-improved-cross-modal-retrieval-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval&lt;/li&gt;
&lt;li&gt;Author: Gregor Geigle et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 19 Feb, 2022&lt;/li&gt;
&lt;li&gt;Review Date: Sat, Aug 27, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;they want to combine the cross encoder and the bi encoder advantages and have a more efficient cross-modal search and retrieval&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;efficiency and simplicity of BE approach based on twin network&lt;/li&gt;
&lt;li&gt;expressiveness and cutting-edge performance of CE methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;p&gt;We propose a novel joint Cross Encoding and Binary Encoding model (Joint-Coop), which is trained to simultaneously cross-encode and embed multi-modal input; it achieves the highest scores overall while maintaining retrieval efficiency&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kaitao_song Mpnet Masked and Permuted Retrain for Language Understanding 2020</title>
      <link>https://sino-huang.github.io/posts/kaitao_song-mpnet-masked-and-permuted-retrain-for-language-understanding-2020/</link>
      <pubDate>Thu, 25 Aug 2022 12:24:55 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/kaitao_song-mpnet-masked-and-permuted-retrain-for-language-understanding-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: MPNet: Masked and Permuted Pre-training for Language Understanding&lt;/li&gt;
&lt;li&gt;Author: Kaitao Song et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Aug 25, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;BERT adopts masked language modelling (MLM) for pre-training and is one of the most successful pre-training models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Since BERT is all attention block and the positional embedding is the only info that care about the ordering, BERT neglects dependency among predicted tokens&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiali_duan Multimodal Alignment Using Representation Codebook 2022</title>
      <link>https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/</link>
      <pubDate>Tue, 09 Aug 2022 07:26:46 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/jiali_duan-multimodal-alignment-using-representation-codebook-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Multi-modal Alignment Using Representation Codebook&lt;/li&gt;
&lt;li&gt;Author: Jiali Duan, Liqun Chen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022 CVPR&lt;/li&gt;
&lt;li&gt;Review Date: Tue, Aug 9, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;aligning signals from different modalities is an important step as it affects the performance of later stage such as cross-modality fusion.&lt;/li&gt;
&lt;li&gt;since image and text often reside in different regions of the feature space, directly aligning them at instance level is challenging especially when features are still evolving during training.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;in this paper, we treat image and text as two &amp;ldquo;views&amp;rdquo; of the same entity, and encode them into a joint vision-language coding space spanned by a dictionary of cluster centres (codebook).&lt;/li&gt;
&lt;li&gt;to further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Types of Vision language pre-training tasks&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Younggyo_seo Masked World Models for Visual Control 2022</title>
      <link>https://sino-huang.github.io/posts/younggyo_seo-masked-world-models-for-visual-control-2022/</link>
      <pubDate>Fri, 01 Jul 2022 12:03:57 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/younggyo_seo-masked-world-models-for-visual-control-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title:  Masked World Models for Visual Control 2022&lt;/li&gt;
&lt;li&gt;Author: Younggyo Seo et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2022&lt;/li&gt;
&lt;li&gt;Review Date: Fri, Jul 1, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.14244?context=cs.AI&#34;&gt;https://arxiv.org/abs/2206.14244?context=cs.AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/mwm-rl&#34;&gt;https://sites.google.com/view/mwm-rl&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://lh3.googleusercontent.com/owoi-mhzvc1Jb8T_jENqF3jzsCYlizdoTPCJQYp0cNmv6AM5nZWqPUi2juwMuDYJrZ4Z6Pnsi5TF7J56GvL6CEyJTZF5AQBqSw-1njMf4Jy9El-Uck_iscK1PU1Y5gC_1w=w1280&#34;&gt;&lt;/p&gt;
&lt;p&gt;TL:DR: &lt;a href=&#34;https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2111.06377&amp;amp;sa=D&amp;amp;sntz=1&amp;amp;usg=AOvVaw1IDxCsGFfXTiNsfRJw8iat&#34;&gt;Masked autoencoders (MAE)&lt;/a&gt; has emerged as a scalable and effective self-supervised learning technique. Can MAE be also effective for visual model-based RL? Yes! with the recipe of convolutional feature masking and reward prediction to capture fine-grained and task-relevant information.&lt;/p&gt;
&lt;h3 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Decouple visual representation learning and dynamics learning&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Brief Overview of Rank Based Prioritized Experience Replay 2016</title>
      <link>https://sino-huang.github.io/posts/a-brief-overview-of-rank-based-prioritized-experience-replay-2022/</link>
      <pubDate>Thu, 02 Jun 2022 11:47:17 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/a-brief-overview-of-rank-based-prioritized-experience-replay-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Prioritised Experience Replay&lt;/li&gt;
&lt;li&gt;Author: Neuralnet.ai&lt;/li&gt;
&lt;li&gt;Publish Year:  25 Feb, 2016&lt;/li&gt;
&lt;li&gt;Review Date: Thu, Jun 2, 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://www.neuralnet.ai/a-brief-overview-of-rank-based-prioritized-experience-replay/&#34;&gt;https://www.neuralnet.ai/a-brief-overview-of-rank-based-prioritized-experience-replay/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;replay-memory-is-essential-in-rl&#34;&gt;Replay memory is essential in RL&lt;/h2&gt;
&lt;p&gt;Replay memory has been successfully deployed in both value based and  policy gradient based reinforcement learning algorithms, to great success. The reasons for this success cut right to the heart of  reinforcement learning. In particular, replay memory simultaneously solves two outstanding problems with the field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deepmind Flamingo a Visual Language Model for Few Shot Learning 2022</title>
      <link>https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/</link>
      <pubDate>Wed, 11 May 2022 16:35:03 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/deepmind-flamingo-a-visual-language-model-for-few-shot-learning-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Flamingo: a Visual Language Model for Few-Shot Learning&lt;/li&gt;
&lt;li&gt;Author: Jean-Baptiste Alayrac et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Apr 2022&lt;/li&gt;
&lt;li&gt;Review Date: May 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;flamingo-architecture&#34;&gt;Flamingo architecture&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Pretrained&lt;/strong&gt; &lt;strong&gt;vision encoder: from pixels to features&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the model&amp;rsquo;s vision encoder is a pretrained Normalizer-Free ResNet (NFNet)&lt;/p&gt;
&lt;p&gt;they pretrain the vision encoder using a contrastive objective on their datasets of image and text pairs, using the two term contrastive loss from paper &amp;ldquo;Learning Transferable Visual Models From Natural Language Supervision&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Angela_fan Augmenting Transformer With Knn Composite Memory for Dialog 2021</title>
      <link>https://sino-huang.github.io/posts/angela_fan-augmenting-transformer-with-knn-composite-memory-for-dialog-2021/</link>
      <pubDate>Thu, 21 Apr 2022 11:01:14 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/angela_fan-augmenting-transformer-with-knn-composite-memory-for-dialog-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Augmenting Transformers with KNN-based composite memory for dialog&lt;/li&gt;
&lt;li&gt;Author: Angela Fan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Apr 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The author proposed augmenting generative Transformer neural network with KNN based Information Fetching module&lt;/p&gt;
&lt;p&gt;Each KIF module learns a read operation to access fix external knowledge (e.g., WIKI)&lt;/p&gt;
&lt;p&gt;The author demonstrated the effectiveness of this approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images and human-written dialog utterances.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hao_hu Generalisable Episodic Memory for Drl 2021</title>
      <link>https://sino-huang.github.io/posts/hao_hu-generalisable-episodic-memory-for-drl-2021/</link>
      <pubDate>Thu, 07 Apr 2022 12:12:20 +1000</pubDate>
      <guid>https://sino-huang.github.io/posts/hao_hu-generalisable-episodic-memory-for-drl-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Generalisable episodic memory for Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Hao Hu et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Jun 2021&lt;/li&gt;
&lt;li&gt;Review Date: April 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The author proposed Generalisable Episodic Memory (GEM), which effectively organises the state-action values of episodic memory in a generalisable manner and supports implicit planning on memorised trajectories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;so compared to traditional memory table&lt;/strong&gt;, GEM learns a virtual memory table memorized by deep neural networks to aggregate similar state-action pairs that essentially have the same nature.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ilya_kostrikov Offline Rl With Implicit Q Learning 2021</title>
      <link>https://sino-huang.github.io/posts/ilya_kostrikov-offline-rl-with-implicit-q-learning-2021/</link>
      <pubDate>Tue, 22 Mar 2022 19:01:49 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/ilya_kostrikov-offline-rl-with-implicit-q-learning-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Offline Reinforcement Learning with Implicit Q-learning&lt;/li&gt;
&lt;li&gt;Author:Ilya Kostrikov et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;conflict in offline reinforcement learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;offline reinforcement learning requires reconciling two conflicting aims:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;learning a policy that improves over the behaviour policy (old policy) that collected the dataset&lt;/li&gt;
&lt;li&gt;while at the same time minimizing the deviation from the behaviour policy so as to avoid errors due to distributional shift (e.g., obtain out of distribution actions) -&amp;gt; the challenge is how to constrain those unseen actions to be in-distribution. (meaning there is no explicit Q-function for actions, and thus the issue of unseen action is gone)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;all the previous solutions like 1. limit how far the new policy deviates from the behaviour policy and 2. assign low value to out of distribution actions impose a trade-off between how much the policy improve and how vulnerable it is to misestimation due to distributional shift.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Qinqing_zheng Online Decision Transformer 2022</title>
      <link>https://sino-huang.github.io/posts/qinqing_zheng-online-decision-transformer-2022/</link>
      <pubDate>Mon, 21 Mar 2022 21:56:45 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/qinqing_zheng-online-decision-transformer-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Online Decision Transformer&lt;/li&gt;
&lt;li&gt;Author: Qinqing Zheng&lt;/li&gt;
&lt;li&gt;Publish Year: Feb 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;the author proposed online Decision transformer (ODT), an RL algorithm based on sequence modelling that blends offline pretraining with online fine-tuning in a unified framework.&lt;/p&gt;
&lt;p&gt;ODT builds on the decision transformer architecture previously introduced for offline RL&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;quantify exploration&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;compared to DT, they shifted from deterministic to stochastic policies for defining exploration objectives during the online phase. They quantify exploration via the &lt;strong&gt;entropy&lt;/strong&gt; of the policy similar to max-ent RL frameworks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sebastian_borgeaud Improving Language Models by Retrieving From Trillions of Tokens 2022</title>
      <link>https://sino-huang.github.io/posts/sebastian_borgeaud-improving-language-models-by-retrieving-from-trillions-of-tokens-2022/</link>
      <pubDate>Mon, 21 Mar 2022 19:07:36 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/sebastian_borgeaud-improving-language-models-by-retrieving-from-trillions-of-tokens-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Improving language models by retrieving from trillions of tokens&lt;/li&gt;
&lt;li&gt;Author: Sebastian Borgeaud et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Feb 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;in order to decrease the size of language model, this work suggested retrieval from a large text database as a complementary path to scaling language models.&lt;/p&gt;
&lt;p&gt;they equip models with the ability to directly access a large dataset to perform prediction &amp;ndash; a &lt;strong&gt;semi-parametric&lt;/strong&gt; approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machel_reid Can Wikipedia Help Offline Rl 2022</title>
      <link>https://sino-huang.github.io/posts/machel_reid-can-wikipedia-help-offline-rl-2022/</link>
      <pubDate>Wed, 16 Mar 2022 21:18:24 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/machel_reid-can-wikipedia-help-offline-rl-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Can Wikipedia Help Offline Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Machel Reid et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Mar 2022&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments.&lt;/p&gt;
&lt;p&gt;Moreover, when the model is trained from scratch, it suffers from slow convergence speeds&lt;/p&gt;
&lt;p&gt;In this paper, they look to take advantage of this formulation of reinforcement learning as &lt;strong&gt;sequence modelling&lt;/strong&gt; and investigate the transferability of pre-trained sequence models on other domains (vision, language) when fine tuned on offline RL tasks (control,  games).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stephen_cresswell Generalised Domain Model Acquisition From Action Traces 2013</title>
      <link>https://sino-huang.github.io/posts/stephen_cresswell-generalised-domain-model-acquisition-from-action-traces-2013/</link>
      <pubDate>Tue, 15 Mar 2022 16:34:45 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/stephen_cresswell-generalised-domain-model-acquisition-from-action-traces-2013/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Generalised Domain Model Acquisition from Action Traces (LOCM2)&lt;/li&gt;
&lt;li&gt;Author: Stephen Cresswell et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2013&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;One approach to the problem of formulating domain models for planning is to learn the models from example action sequences.&lt;/p&gt;
&lt;p&gt;This work extended LOCM by allowing multiple parameterised state machine to represent a single object.&lt;/p&gt;
&lt;p&gt;In other words, it is possible to automatically infer the underlying transition system from &lt;em&gt;sample action sequences of the domain&lt;/em&gt;. Using such an approach removes the necessity for the domain expert to also be an expert at modelling transition systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Wenfeng_feng Extracting Action Sequences From Texts by Rl</title>
      <link>https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/</link>
      <pubDate>Tue, 15 Mar 2022 14:40:38 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Extracting Action Sequences from Texts Based on Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Wenfeng Feng et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Mar 2018&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;the author want to build a model that learns to directly extract action sequences without external tools like POS tagging and dependency parsing results&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Annotation dataset structure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220315161910319&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/image-assets/image-20220315161910319.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;example&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220315162057150&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/wenfeng_feng-extracting-action-sequences-from-texts-by-rl/image-assets/image-20220315162057150.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;they exploit the framework to learn two models to predict action names and arguments respectively.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shivam_miglani Nltopddl Learning From Nlp Manuals 2020</title>
      <link>https://sino-huang.github.io/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/</link>
      <pubDate>Mon, 14 Mar 2022 15:08:45 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: NLtoPDDL: One-Shot Learning of PDDL Models from Natural Language Process Manuals&lt;/li&gt;
&lt;li&gt;Author: Shivam Miglani et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;pipeline&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220314153211927&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/image-assets/image-20220314153211927.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pipeline architecture&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220314165337096&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/shivam_miglani-nltopddl-learning-from-nlp-manuals-2020/image-assets/image-20220314165337096.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 1&lt;/strong&gt; we have a DQN that learns to extract words that represent action name, action arguments, and the sequence of actions present in annotated NL process manuals. (why only action name, do we need to extract other information???) Again, why this is called DQN RL? is it just normal supervised learning&amp;hellip;  (Check EASDRL paper to understand Phase 1)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Giuseppe_de_giacomo Foundations for Retraining Bolts Rl With Ltl 2019</title>
      <link>https://sino-huang.github.io/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/</link>
      <pubDate>Fri, 04 Mar 2022 12:12:57 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf Restraining Specification&lt;/li&gt;
&lt;li&gt;Author: Giuseppe De Giacomo et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2019&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The author investigated the concept of &amp;ldquo;restraining bolt&amp;rdquo; that can control the behaviour of learning agents.&lt;/p&gt;
&lt;p&gt;Essentially, the way to control a RL agent is that the bolt provides additional rewards to the agent&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220309181427110&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/giuseppe_de_giacomo-foundations-for-retraining-bolts-rl-with-ltl-2019/image-assets/image-20220309181427110.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Although this method is essentially the same as reward shaping (providing additional rewards to the agent), the contribution of this paper is&lt;/p&gt;</description>
    </item>
    <item>
      <title>Joseph_kim Collaborative Planning With Encoding of High Level Strategies 2017</title>
      <link>https://sino-huang.github.io/posts/joseph_kim-collaborative-planning-with-encoding-of-high-level-strategies-2017/</link>
      <pubDate>Fri, 04 Mar 2022 12:12:27 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/joseph_kim-collaborative-planning-with-encoding-of-high-level-strategies-2017/</guid>
      <description>&lt;p&gt;please modify the following&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Collaborative Planning with Encoding of Users&amp;rsquo; High-level Strategies&lt;/li&gt;
&lt;li&gt;Author: Joseph Kim et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2017&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Automatic planning is computationally expensive. Greedy search heuristics often yield low-quality plans that can result in wasted resources; also, even in the event that an adequate plan is generated, users may have difficulty interpreting the reason why the plan performs well and trusting it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mikayel_samvelyan Minihack the Planet a Sandbox for Open Ended Rl Research 2021</title>
      <link>https://sino-huang.github.io/posts/mikayel_samvelyan-minihack-the-planet-a-sandbox-for-open-ended-rl-research-2021/</link>
      <pubDate>Fri, 04 Mar 2022 12:11:55 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/mikayel_samvelyan-minihack-the-planet-a-sandbox-for-open-ended-rl-research-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research&lt;/li&gt;
&lt;li&gt;Author: Mikayel Samvelyan et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Nov 2021&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;They presented MiniHack, an easy-to-use framework for creating rich and varied RL environments, as well as a suite of tasks developed using this framework. Built upon NLE and the &lt;code&gt;des-file&lt;/code&gt; format, MiniHack enables the use of rich entities and dynamics from the game of NetHack to create a large variety of RL environments for targeted experimentation, while also allowing painless scaling-up of the difficulty of existing environments.  MiniHack’s environments are procedurally generated by default, ensuring the evaluation of systematic generalization of RL agents.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Richard_shin Constrained Language Models Yield Few Shot Semantic Parsers 2021</title>
      <link>https://sino-huang.github.io/posts/richard_shin-constrained-language-models-yield-few-shot-semantic-parsers-2021/</link>
      <pubDate>Wed, 02 Mar 2022 00:19:18 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/richard_shin-constrained-language-models-yield-few-shot-semantic-parsers-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Constrained Language models yield few-shot semantic parsers&lt;/li&gt;
&lt;li&gt;Author: Richard Shin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Nov 2021&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The author wanted to explore the use of large pretrained language models as few-shot semantic parsers&lt;/p&gt;
&lt;p&gt;However, language models are trained to generate natural language. To bridge the gap, they used language models to paraphrase inputs into a &lt;em&gt;&lt;strong&gt;controlled&lt;/strong&gt;&lt;/em&gt; sublanguage resembling English that can be automatically mapped to a target meaning representation. (using synchronous context-free grammar SCFG)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Heinrich_kuttler the Nethack Learning Environment 2020</title>
      <link>https://sino-huang.github.io/posts/heinrich_kuttler-the-nethack-learning-environment-2020/</link>
      <pubDate>Wed, 02 Mar 2022 00:18:35 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/heinrich_kuttler-the-nethack-learning-environment-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: The NetHack Learning Environment&lt;/li&gt;
&lt;li&gt;Author: Heinrich Kuttler et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Dec 2020&lt;/li&gt;
&lt;li&gt;Review Date: Mar 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The NetHack Learning Environment (NLE), a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack.&lt;/p&gt;
&lt;p&gt;NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pashootan_vaezipoor Ltl2action Generalising Ltl Instructions for Multi Task Rl 2021</title>
      <link>https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/</link>
      <pubDate>Tue, 01 Mar 2022 20:53:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/pashootan_vaezipoor-ltl2action-generalising-ltl-instructions-for-multi-task-rl-2021/</guid>
      <description>&lt;p&gt;please modify the following&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: LTL2Action: Generalizing LTL Instructions for Multi-Task RL&lt;/li&gt;
&lt;li&gt;Author: Pashootan Vaezipoor et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: March 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;they addressed the problem of teaching a deep reinforcement learning agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language &amp;ndash; linear temporal logic (LTL)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limitation of the vanilla MDP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;temporal constraints cannot be expressed as rewards in MDP setting and thus modular policy and other stuffs are not able to obtain maximum rewards.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Roma_patel Learning to Ground Language Temporal Logical Form 2019</title>
      <link>https://sino-huang.github.io/posts/roma_patel-learning-to-ground-language-temporal-logical-form-2019/</link>
      <pubDate>Mon, 28 Feb 2022 21:40:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/roma_patel-learning-to-ground-language-temporal-logical-form-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Learning to Ground Language to Temporal Logical Form&lt;/li&gt;
&lt;li&gt;Author: Roma Patel et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2019&lt;/li&gt;
&lt;li&gt;Review Date: Feb 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;natural language commands often exhibits sequential (temporal) constraints e.g., &amp;ldquo;go through the kitchen and then into the living room&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;But this constraints cannot be expressed in the reward of Markov Decision Process setting. (see &lt;a href=&#34;https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/&#34;&gt;this paper&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Therefore, they proposed to ground language to Linear Temporal logic (LTL) and after that continue to map from LTL expressions to action sequences.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Thang_m_pham Out of Order How Important Is the Sequential Order of Words in a Sentence in Natural Language Understanding Tasks 2021</title>
      <link>https://sino-huang.github.io/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/</link>
      <pubDate>Mon, 28 Feb 2022 18:58:52 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?&lt;/li&gt;
&lt;li&gt;Author: Thang M. Pham&lt;/li&gt;
&lt;li&gt;Publish Year: Jul 2021&lt;/li&gt;
&lt;li&gt;Review Date: Feb 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The author found out that BERT-based models trained on GLUE have low sensitivity to word orders.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220228205000220&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/image-assets/image-20220228205000220.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The research questions are the following&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Do BERT-based models trained on GLUE care about the order of words in a sentence?
&lt;ul&gt;
&lt;li&gt;ANS: NO, except one task named CoLA, which is to detecting grammatically incorrect sentences. Surprisingly, for the rest of the 5 out of 6 binary-classification tasks (i.e.  except CoLA), between75% and 90% of the originally correct predictions remain constant after  1-grams are randomly re-ordered&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are SOTA BERT-based models using word order information when solving NLU tasks? If not, what cues do they rely on?
&lt;ul&gt;
&lt;li&gt;ANS: they heavily rely on the word itself rather than the ordering. The results showed that if the top - 1 most important word measured by LIME has a positive meaning, then there is 100% probability that the sentence&amp;rsquo;s label is &amp;ldquo;positive&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;img alt=&#34;image-20220228212916959&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/thang_m_pham-out-of-order-how-important-is-the-sequential-order-of-words-in-a-sentence-in-natural-language-understanding-tasks-2021/image-assets/image-20220228212916959.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Anton_belyy Guided K Best Selection for Semantic Parsing Annotation 2021</title>
      <link>https://sino-huang.github.io/posts/anton_belyy-guided-k-best-selection-for-semantic-parsing-annotation-2021/</link>
      <pubDate>Wed, 23 Feb 2022 19:42:39 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/anton_belyy-guided-k-best-selection-for-semantic-parsing-annotation-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Guided K-best Selection for Semantic Parsing Annotation&lt;/li&gt;
&lt;li&gt;Author: Anton Belyy et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Feb 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;They wanted to tackle the challenge of efficient data collection (data annotation) for the conversational semantic parsing task.&lt;/p&gt;
&lt;p&gt;In the presence of little available training data, they proposed human-in-the-loop interfaces for guided K-best selection, using a prototype model trained on limited data.&lt;/p&gt;
&lt;h3 id=&#34;result&#34;&gt;Result&lt;/h3&gt;
&lt;p&gt;Their user studies showed that the keyword searching function combined with a keyword suggestion method strikes the balance between annotation accuracy and speed&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacob_andreas Compositionality as Lexical Symmetry 2022</title>
      <link>https://sino-huang.github.io/posts/jacob_andreas-compositionality-as-lexical-symmetry-2022/</link>
      <pubDate>Tue, 08 Feb 2022 14:20:19 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jacob_andreas-compositionality-as-lexical-symmetry-2022/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Compositionality as Lexical Symmetry&lt;/li&gt;
&lt;li&gt;Author: Ekin Akyurek; Jacob Andreas&lt;/li&gt;
&lt;li&gt;Publish Year: Jan 2022&lt;/li&gt;
&lt;li&gt;Review Date: Feb 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;Standard deep network models lack the inductive bias needed to generalize compositionally in tasks like semantic parsing, translation, and question answering.&lt;/p&gt;
&lt;p&gt;So, a large body of work in NLP seeks to overcome this limitation with new model architectures that enforce a compositional process of sentence interpretation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tao_lei When Attention Meets Fast Recurrence Training Language Models With Reduced Compute 2021</title>
      <link>https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/</link>
      <pubDate>Fri, 14 Jan 2022 00:26:37 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: When Attention Meets Fast Recurrence: Training Language Models with Reduce Compute&lt;/li&gt;
&lt;li&gt;Author: Tao Lei&lt;/li&gt;
&lt;li&gt;Publish Year: Sep 2021&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220114003204904&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114003204904.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As the author mentioned, the inspiration of SRU++ comes from two lines of research:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paralleization / speed problem of Original RNN
&lt;ul&gt;
&lt;li&gt;&lt;img alt=&#34;image-20220114004410134&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114004410134.png&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;leveraging recurrence in conjunction with self-attention&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Structure of SRU++&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220114005227231&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tao_lei-when-attention-meets-fast-recurrence-training-language-models-with-reduced-compute-2021/image-assets/image-20220114005227231.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New discovery :little attention is needed given recurrence.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Similar to the observation of &lt;a href=&#34;https://arxiv.org/abs/1911.11423&#34;&gt;Merity (2019)&lt;/a&gt;, they found using a couple of attention layers sufficient to obtain SOTA results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alex_nichol Glide Towards Photorealistic Image Generation and Editing With Text Guided Diffusion Models 2021</title>
      <link>https://sino-huang.github.io/posts/alex_nichol-glide-towards-photorealistic-image-generation-and-editing-with-text-guided-diffusion-models-2021/</link>
      <pubDate>Wed, 12 Jan 2022 16:54:01 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/alex_nichol-glide-towards-photorealistic-image-generation-and-editing-with-text-guided-diffusion-models-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/li&gt;
&lt;li&gt;Author: Alex Nichol et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Dec 2021&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;In author&amp;rsquo;s &lt;a href=&#34;https://arxiv.org/pdf/2105.05233.pdf&#34;&gt;previous work&lt;/a&gt;, the diffusion model can achieve photorealism in the class-conditional setting by augmenting with &lt;em&gt;classifier guidance&lt;/em&gt;, a technique which allows diffusion models to condition on a classifier&amp;rsquo;s labels.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The classifier is first trained on noised images, and during the diffusion sampling process, gradients from the classifier are used to guide the output sample towards the label.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;classifier details&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Junyang_lin M6 a Chinese Multimodal Pretrainer 2021</title>
      <link>https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/</link>
      <pubDate>Wed, 12 Jan 2022 13:38:14 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/junyang_lin-m6-a-chinese-multimodal-pretrainer-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: M6: A Chinese Multimodal Pretrainer&lt;/li&gt;
&lt;li&gt;Author: Junyang Lin et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: May 2021&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;This paper re-emphasises that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;large model trained on big data have extremely large capacity and it can outperform the SOTA in downstream tasks especially in the zero-shot setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the author trained a big multi-modal model&lt;/p&gt;
&lt;p&gt;Also, they proposed a innovative way to tackle downstream tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;they use masks to block cross attention between tokens so as to fit different types of downstream task&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Key idea: mask tokens during cross attention so as to solve certain tasks&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tianshi_cao Babyai Plus Plus Towards Grounded Language Learning Beyond Memorization 2020</title>
      <link>https://sino-huang.github.io/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/</link>
      <pubDate>Mon, 03 Jan 2022 22:38:40 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: BABYAI++: Towards Grounded-Language Learning Beyond Memorization&lt;/li&gt;
&lt;li&gt;Author: Tianshi Cao et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020 ICLR&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper introduced a new RL environment BabyAI++ that can investigate whether RL agents can extract knowledge from descriptive text and eventually increase generalisation performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BabyAI++ environment example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20220104181838965&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tianshi_cao-babyai-plus-plus-towards-grounded-language-learning-beyond-memorization-2020/image-assets/image-20220104181838965.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the descriptive text describe the feature of the object.
&lt;ul&gt;
&lt;li&gt;notice that the feature of object can easily change as we change the descriptive text.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Federico_bianchi Language in a Search Box Grounding Language Learning in Real World Human Machine Interaction 2021</title>
      <link>https://sino-huang.github.io/posts/federico_bianchi-language-in-a-search-box-grounding-language-learning-in-real-world-human-machine-interaction-2021/</link>
      <pubDate>Mon, 03 Jan 2022 16:51:39 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/federico_bianchi-language-in-a-search-box-grounding-language-learning-in-real-world-human-machine-interaction-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction&lt;/li&gt;
&lt;li&gt;Author: Federico Bianchi&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Jan 2022&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;the author investigated grounded language learning through the natural interaction between users and the shopping website search engine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How they do it&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;convert the shopping object dataset into a Latent Grounded Domain&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt=&#34;image-20220103212726450&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/federico_bianchi-language-in-a-search-box-grounding-language-learning-in-real-world-human-machine-interaction-2021/image-assets/image-20220103212726450.png&#34;&gt;&lt;/li&gt;
&lt;li&gt;related products end up closer in the embedding space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;train the mapping model (mapping from text query to a portion of product space) based on the user click behaviour (In the training dataset, the users queries about &amp;ldquo;Nike&amp;rdquo; and the they would click relevant Nike Product)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lili_chen Decision Transformer Reinforcement Learning via Sequence Modeling 2021</title>
      <link>https://sino-huang.github.io/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/</link>
      <pubDate>Fri, 24 Dec 2021 23:29:49 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Decision Transformer: Reinforcement Learning via Sequence Modeling&lt;/li&gt;
&lt;li&gt;Author: Lili Chen et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: Jun 2021&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The Architecture of Decision Transformer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211225170954043&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/lili_chen-decision-transformer-reinforcement-learning-via-sequence-modeling-2021/image-assets/image-20211225170954043.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inputs are reward, observation and action&lt;/p&gt;
&lt;p&gt;Outputs are action, in training time, the future action will be masked out.&lt;/p&gt;
&lt;p&gt;I believe this model is able to generate a very good long sequence of actions due to transformer architecture.&lt;/p&gt;
&lt;p&gt;But somehow this is not RL anymore because the transformer is not trained by reward signal &amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiayuan_mao Grammar Based Grounded Lexicon Learning 2021</title>
      <link>https://sino-huang.github.io/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/</link>
      <pubDate>Wed, 22 Dec 2021 17:22:15 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Grammar-Based Grounded Lexicon Learning&lt;/li&gt;
&lt;li&gt;Author: Jiayuan Mao&lt;/li&gt;
&lt;li&gt;Publish Year: 2021 NeurIPS&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The paper extend the previous work &amp;ldquo;&lt;a href=&#34;https://arxiv.org/pdf/1904.12584.pdf&#34;&gt;Neuro-Symbolic Concept Learner&lt;/a&gt;&amp;rdquo; by parsing the natural language questions using symbolic manner.&lt;/p&gt;
&lt;p&gt;The core semantic parsing technique is Combinatory Categorical Grammar with CKY algorithm to prune unlikely expressions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The full picture looks like this&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211224225350012&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/image-assets/image-20211224225350012.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The detailed algorithm process looks like this&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211224225456819&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/jiayuan_mao-grammar-based-grounded-lexicon-learning-2021/image-assets/image-20211224225456819.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to derive concept embedding&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Julia_kiseleva Interactive Grounded Language Understanding in a Collaborative Environment 2021</title>
      <link>https://sino-huang.github.io/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/</link>
      <pubDate>Wed, 22 Dec 2021 15:10:56 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Interactive Grounded Language Understanding in a Collaborative Environment&lt;/li&gt;
&lt;li&gt;Author: Julia Kiseleva et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211222154135180&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/image-assets/image-20211222154135180.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211222154153920&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/julia_kiseleva-interactive-grounded-language-understanding-in-a-collaborative-environment-2021/image-assets/image-20211222154153920.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The split the problem into following concrete research questions, which correspond to separate tasks that can be used to study each component individually before joining all of them into one system&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dominik_drexler Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches 2021</title>
      <link>https://sino-huang.github.io/posts/dominik_drexler-expressing-and-exploiting-the-common-subgoal-structure-of-classical-planning-domains-using-sketches-2021/</link>
      <pubDate>Fri, 17 Dec 2021 13:07:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/dominik_drexler-expressing-and-exploiting-the-common-subgoal-structure-of-classical-planning-domains-using-sketches-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches&lt;/li&gt;
&lt;li&gt;Author: Dominik Drexler et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;Algorithms like SIW often fail when the goal is not easily serialisable or when some of the subproblems have a high width. In this work, the author address these limitations by using a simple but powerful language for expressing finer problem decompositions called policy sketches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yiding_jiang Language as Abstraction for Hierarchical Deep Reinforcement Learning</title>
      <link>https://sino-huang.github.io/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/</link>
      <pubDate>Wed, 15 Dec 2021 19:49:28 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Language as an Abstraction for Hierarchical Deep Reinforcement Learning&lt;/li&gt;
&lt;li&gt;Author: Yiding Jiang et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2019 NeurIPS&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;Solving complex, temporally-extended tasks is a long-standing problem in RL.&lt;/p&gt;
&lt;p&gt;Acquiring effective yet general abstractions for hierarchical RL is remarkably challenging.&lt;/p&gt;
&lt;p&gt;Therefore, they propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalisation&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211218205222284&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/yiding_jiang-language-as-abstraction-for-hierarchical-deep-reinforcement-learning/image-assets/image-20211218205222284.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hengyuan_hu Hierarchical Decision Making by Generating and Following Natural Language Instructions 2019</title>
      <link>https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/</link>
      <pubDate>Wed, 15 Dec 2021 13:11:05 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Hierarchical Decision Making by Generating and Following Natural Language Instructions&lt;/li&gt;
&lt;li&gt;Author: Hengyuan Hu et. al. FAIR&lt;/li&gt;
&lt;li&gt;Publish Year: 2019&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;One line summary: they build a Architect Builder model to clone human behaviour for playing RTS game&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211215191322769&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191322769.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211215191341893&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191341893.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211215191350360&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/hengyuan_hu-hierarchical-decision-making-by-generating-and-following-natural-language-instructions-2019/image-assets/image-20211215191350360.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Their task environment is very similar to IGLU competition setting, but their model is too task-specific&lt;/p&gt;
&lt;p&gt;The author mentioned some properties about natural language instructions&lt;/p&gt;</description>
    </item>
    <item>
      <title>David_ding Attention Over Learned Object Embeddings Enables Complex Visual Reasoning 2021</title>
      <link>https://sino-huang.github.io/posts/david_ding-attention-over-learned-object-embeddings-enables-complex-visual-reasoning-2021/</link>
      <pubDate>Wed, 15 Dec 2021 12:59:07 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/david_ding-attention-over-learned-object-embeddings-enables-complex-visual-reasoning-2021/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;Title: Attention Over Learned Object Embeddings Enables Complex Visual Reasoning&lt;/li&gt;
&lt;li&gt;Author: David Ding et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021 NeurIPS&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Background info for this paper:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Their paper propose a all-in-one transformer model that is able to answer CLEVRER counterfactual questions with higher accuracy (75.6% vs 46.5%) and less training data (- 40%)&lt;/p&gt;
&lt;p&gt;They believe that their model relies on three key aspects:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;self-attention&lt;/li&gt;
&lt;li&gt;soft-discretization&lt;/li&gt;
&lt;li&gt;self-supervised learning&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211214201703442&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/david_ding-attention-over-learned-object-embeddings-enables-complex-visual-reasoning-2021/image-assets/image-20211214201703442.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jacob_andreas Modular Multitask Reinforcement Learning With Policy Sketches 2017</title>
      <link>https://sino-huang.github.io/posts/jacob_andreas-modular-multitask-reinforcement-learning-with-policy-sketches-2017/</link>
      <pubDate>Mon, 13 Dec 2021 17:23:12 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/jacob_andreas-modular-multitask-reinforcement-learning-with-policy-sketches-2017/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;Title: Modular Multitask Reinforcement Learning with Policy Sketches&lt;/li&gt;
&lt;li&gt;Author: Jacob Andreas et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2017&lt;/li&gt;
&lt;li&gt;Review Date: Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Background info for this paper:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Their paper describe a framework that is inspired by on &lt;a href=&#34;https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf&#34;&gt;&lt;em&gt;options MDP&lt;/em&gt;&lt;/a&gt;, for which a reinforcement learning task is handled by several sub-MDP modules. (that is why they call it Modular RL)&lt;/p&gt;
&lt;p&gt;They consider a multitask RL problem in a shared environment. (See the figure below). The IGLU Minecraft challenge as well as Angry Birds also belongs to this category.&lt;/p&gt;</description>
    </item>
    <item>
      <title>David_abel on the Expressivity of Markov Reward 2021</title>
      <link>https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/</link>
      <pubDate>Sun, 05 Dec 2021 12:02:23 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/david_abel-on-the-expressivity-of-markov-reward-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: On the Expressivity of Markov Reward&lt;/li&gt;
&lt;li&gt;Author: David Abel et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: NuerIPS 2021&lt;/li&gt;
&lt;li&gt;Review Date: 6 Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The author found out that in the &lt;em&gt;Markov Decision Process&lt;/em&gt; scenario, (i.e., we do not look at the &lt;em&gt;history&lt;/em&gt; of the trajectory to provide rewards), some tasks cannot be &lt;em&gt;realised&lt;/em&gt; perfectly by reward functions. i.e.,&lt;/p&gt;</description>
    </item>
    <item>
      <title>Rishabh_agarwal Deep Reinforcement Learning at the Edge of the Stats Precipice 2021</title>
      <link>https://sino-huang.github.io/posts/rishabh_agarwal-deep-reinforcement-learning-at-the-edge-of-the-stats-precipice-2021/</link>
      <pubDate>Fri, 03 Dec 2021 19:50:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/rishabh_agarwal-deep-reinforcement-learning-at-the-edge-of-the-stats-precipice-2021/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Deep Reinforcement Learning at the Edge of the Statistical Precipice&lt;/li&gt;
&lt;li&gt;Author: Rishabh Agarwal et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: NeurIPS 2021&lt;/li&gt;
&lt;li&gt;Review Date: 3 Dec 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most current published results on deep RL benchmarks uses &lt;em&gt;point estimate&lt;/em&gt; of aggregate performance such as mean and median score across the task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Borja_ibarz Reward Learning From Human Preferences and Demonstrations in Atari 2018</title>
      <link>https://sino-huang.github.io/posts/borja_ibarz-reward-learning-from-human-preferences-and-demonstrations-in-atari-2018/</link>
      <pubDate>Sat, 27 Nov 2021 19:14:04 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/borja_ibarz-reward-learning-from-human-preferences-and-demonstrations-in-atari-2018/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Reward learning from human preferences and demonstractions in Atari&lt;/li&gt;
&lt;li&gt;Author: Borja Ibarz et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2018&lt;/li&gt;
&lt;li&gt;Review Date: Nov 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The author proposed a method that uses &lt;strong&gt;&lt;u&gt;human expert&amp;rsquo;s annotation&lt;/u&gt;&lt;/strong&gt; rather than extrinsic reward from the environment to guide the reinforcement learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adrien_ecoffet Go Explore a New Approach for Hard Exploration Problems 2021 Paper Review</title>
      <link>https://sino-huang.github.io/posts/adrien_ecoffet-go-explore-a-new-approach-for-hard-exploration-problems-2021-paper-review/</link>
      <pubDate>Sat, 27 Nov 2021 18:58:32 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/adrien_ecoffet-go-explore-a-new-approach-for-hard-exploration-problems-2021-paper-review/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Go-Explore: a New Approach for Hard-Exploration Problems&lt;/li&gt;
&lt;li&gt;Author: Adrien Ecoffet et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Nov 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The author hypothesised that there are two main issues that prevent DRL agents from achieving high score in exploration-hard game (e.g., Montezuma&amp;rsquo;s Revenge)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tuomas_haarnoja Soft Actor Critic Off Policy Maximum Entropy Deep Reinforcement Learning With a Stochastic Actor 2018 Paper Review</title>
      <link>https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/</link>
      <pubDate>Thu, 18 Nov 2021 12:08:53 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;论文简析sac-soft-actor-critic-part-1180101290httpswwwbilibilicomvideobv1yk4y1t7b6fromsearchseid13778340264869221460&#34;&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1YK4y1T7b6?from=search&amp;amp;seid=13778340264869221460&#34;&gt;[论文简析]SAC: Soft Actor-Critic Part 1[1801.01290]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211123221032880&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221032880.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211123221054067&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221054067.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211123221439305&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221439305.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211123221905958&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221905958.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211123221928661&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211123221928661.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124234149933&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234149933.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124234615632&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234615632.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124234654185&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234654185.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124234853137&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124234853137.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235018547&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235018547.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235141106&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235141106.png&#34;&gt;&lt;img alt=&#34;image-20211124235309125&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235309125.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235518333&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235518333.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235636919&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235636919.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;hat&lt;/strong&gt; means estimation&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235727839&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235727839.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211124235947697&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211124235947697.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000155557&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000155557.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000319745&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000319745.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000439109&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000439109.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000645586&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000645586.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000751016&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000751016.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125000932147&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125000932147.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002108776&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002108776.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002223831&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002223831.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002441206&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002441206.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002504865&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002504865.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002514461&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002514461.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125002922725&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125002922725.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125003125841&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003125841.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125003148666&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003148666.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20211125003208320&#34; loading=&#34;lazy&#34; src=&#34;https://sino-huang.github.io/posts/tuomas_haarnoja-soft-actor-critic-off-policy-maximum-entropy-deep-reinforcement-learning-with-a-stochastic-actor-2018-paper-review/image-assets/image-20211125003208320.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adria Badia Agent57 Outperforming the Atari Human Benchmark 2020 Paper Review</title>
      <link>https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/</link>
      <pubDate>Thu, 18 Nov 2021 12:05:47 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/adria-badia-agent57-outperforming-the-atari-human-benchmark-2020-paper-review/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Agent57: Outperforming the Atari Human Benchmark 2020&lt;/li&gt;
&lt;li&gt;Author: Adria Badia et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2020&lt;/li&gt;
&lt;li&gt;Review Date: Nov 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Agent57 is the SOTA Atari RL agent in 2020 that can play difficult Atari games like &amp;ldquo;Montezuma&amp;rsquo;s Revenge, &amp;ldquo;Pitfall&amp;rdquo;, &amp;ldquo;Solaris&amp;rdquo; and &amp;ldquo;Skiing&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stefan O Toole Width Based Lookaheads With Learnt Base Policies and Heuristics Over the Atari 2600 Benchmark 2021 Paper Reivew</title>
      <link>https://sino-huang.github.io/posts/stefan-o-toole-width-based-lookaheads-with-learnt-base-policies-and-heuristics-over-the-atari-2600-benchmark-2021-paper-reivew/</link>
      <pubDate>Tue, 16 Nov 2021 17:40:10 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/stefan-o-toole-width-based-lookaheads-with-learnt-base-policies-and-heuristics-over-the-atari-2600-benchmark-2021-paper-reivew/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: Width-based Lookaheads with Learnt Base Policies and Heuristics Over the Atari-2600 Benchmark&lt;/li&gt;
&lt;li&gt;Author: Stefan O&amp;rsquo;Toole et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021&lt;/li&gt;
&lt;li&gt;Review Date: Tue 16 Nov 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This paper proposed a new width-based planning and learning agent that can play Atari-2600 games (though it cannot play Montezuma&amp;rsquo;s Revenge). The author claimed that width-based planning &lt;em&gt;exploration&lt;/em&gt; plus (greedy) optimal MDP policy exploitation is able to achieve better performance than Monte-Carlo Tree Search.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cristian Paul Bara Mindcraft Theory of Mind Modelling 2021 Paper Review</title>
      <link>https://sino-huang.github.io/posts/cristian-paul-bara-mindcraft-theory-of-mind-modelling-2021-paper-review/</link>
      <pubDate>Fri, 12 Nov 2021 12:56:24 +1100</pubDate>
      <guid>https://sino-huang.github.io/posts/cristian-paul-bara-mindcraft-theory-of-mind-modelling-2021-paper-review/</guid>
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Title: MINDCRAFT: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks&lt;/li&gt;
&lt;li&gt;Author: Cristian-Paul Bara et. al.&lt;/li&gt;
&lt;li&gt;Publish Year: 2021 EMNLP&lt;/li&gt;
&lt;li&gt;Review Date: 12 Nov 2021&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-of-paper&#34;&gt;Summary of paper&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This needs to be only 1-3 sentences, but it demonstrates that you understand the paper and, moreover, can summarize it more concisely than the author in his abstract.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The contribution of this paper is &lt;strong&gt;the mind modelling dataset&lt;/strong&gt; (Using Minecraft environment).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
